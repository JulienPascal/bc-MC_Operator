{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccbae77f",
   "metadata": {},
   "source": [
    "# Functions for bc-MC-consumption-savings\n",
    "\n",
    "## Description\n",
    "\n",
    "store functions for the file bc-MC-consumption-savings.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "034146bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-10 13:05:22.699612: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-10 13:05:22.699630: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import quantecon as qe\n",
    "from interpolation import interp\n",
    "from quantecon.optimize import brentq\n",
    "from numba import njit, float64\n",
    "from numba.experimental import jitclass\n",
    "import Tasmanian # sparse grids\n",
    "\n",
    "import random\n",
    "import scipy.stats\n",
    "import chaospy  ## for quadrature\n",
    "from itertools import product\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from math import sqrt\n",
    "import seaborn as sns; sns.set()\n",
    "from tqdm import tqdm as tqdm         # tqdm is a nice library to visualize ongoing loops\n",
    "import datetime\n",
    "# followint lines are used for indicative typing\n",
    "from typing import Tuple\n",
    "class Vector: pass\n",
    "from scipy.stats import norm\n",
    "import Tasmanian\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "# To create copies of NN\n",
    "import copy\n",
    "import matplotlib.ticker as mtick\n",
    "# To use sparse kronecker product\n",
    "from scipy import sparse\n",
    "from torchcontrib.optim import SWA\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4173ead7",
   "metadata": {},
   "source": [
    "## I. Time iteration\n",
    "\n",
    "Iterate over the Euler equation of the model:\n",
    "\n",
    "$$\n",
    "\\begin{gather*}\n",
    "u^{\\prime }(c) = \\beta \\overline{r}E_{\\epsilon }\\left[ u^{\\prime }\\left(\n",
    "c^{\\prime }\\right) \\exp \\left(   \\delta ^{\\prime }-\\delta\n",
    "+r^{\\prime }\\right)  \\right] \n",
    "\\end{gather*}\n",
    "$$\n",
    "\n",
    "$$ c(w_t, y_t, p_t, r_t, \\delta_t) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddf35cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time iteration using Monte Carlo integration\n",
    "def euler_diff(c, σ_func, r, p, q, δ, w, e_r, e_δ, e_p, e_q, params):\n",
    "    \"\"\"\n",
    "    Set up a function such that the root with respect to c,\n",
    "    Uses Monte Carlo integration for the shocks next period\n",
    "    \"\"\"\n",
    "    # transitions of the exogenous processes\n",
    "    rnext = r*params.ρ_r + e_r\n",
    "    δnext = δ*params.ρ_δ + e_δ\n",
    "    pnext = p*params.ρ_p + e_p\n",
    "    qnext = q*params.ρ_q + e_q\n",
    "    # (epsilon = (rnext, δnext, pnext, qnext))\n",
    "    \n",
    "    # transition of endogenous states (next denotes variables at t+1)\n",
    "    wnext = np.exp(pnext+qnext) + (w-c)*params.rbar*np.exp(rnext)\n",
    "    # column 1: r\n",
    "    # column 2: p\n",
    "    # column 3: q\n",
    "    # column 4: δ\n",
    "    # column 5: w\n",
    "    state_next = np.column_stack((rnext, pnext, qnext, δnext, wnext))\n",
    "    cnext = σ_func(state_next)\n",
    "    \n",
    "    vals = np.exp(δnext-δ+rnext)*(cnext)**(-params.γ)\n",
    "    \n",
    "    return c**(-params.γ) - np.maximum(params.β*params.rbar*np.mean(vals), (w+params.bc)**(-params.γ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b85b147c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time iteration using Monte Carlo integration\n",
    "# σ current guess on grid\n",
    "def K(σ, params):\n",
    "    \"\"\"\n",
    "    The Coleman-Reffett operator\n",
    "    Uses Monte Carlo integration for the shocks next period\n",
    "    \"\"\"\n",
    "    # First turn σ into a function via interpolation\n",
    "    σ_func = lambda x: interp(params.r_grid, params.p_grid, params.q_grid, params.δ_grid, params.w_grid, σ, x)\n",
    "    # Initialization\n",
    "    σ_new = np.zeros((params.n_points_grid, params.n_points_grid, params.n_points_grid, params.n_points_grid, params.n_points_w))\n",
    "    for (r_index, r_value) in enumerate(params.r_grid):\n",
    "        for (p_index, p_value) in enumerate(params.p_grid):\n",
    "            for (q_index, q_value) in enumerate(params.q_grid):\n",
    "                for (δ_index, δ_value) in enumerate(params.δ_grid):\n",
    "                    for (w_index, w_value) in enumerate(params.w_grid):\n",
    "                        # shocks\n",
    "                        e_r = np.random.normal(loc=0, scale=params.σ_r, size=(20,)) \n",
    "                        e_δ = np.random.normal(loc=0, scale=params.σ_δ, size=(20,)) \n",
    "                        e_p = np.random.normal(loc=0, scale=params.σ_p, size=(20,)) \n",
    "                        e_q = np.random.normal(loc=0, scale=params.σ_q, size=(20,)) \n",
    "                        c_star = scipy.optimize.brentq(euler_diff, 1e-10, w_value + params.bc, args=(σ_func, r_value, p_value, q_value, δ_value, w_value, e_r, e_δ, e_p, e_q, params))\n",
    "                        σ_new[r_index, p_index, q_index, δ_index, w_index] = c_star\n",
    "    return σ_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "077c2986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time iteration using Gaussian quadrature\n",
    "def euler_diff_quadrature_old(c, σ_func, r, p, q, δ, w, e_r, e_δ, e_p, e_q, params):\n",
    "    \"\"\"\n",
    "    Set up a function such that the root with respect to c,\n",
    "    Uses Gaussian quadrature for the shocks next period\n",
    "    \"\"\"\n",
    "    # transitions of the exogenous processes\n",
    "    rnext = r*params.ρ_r + e_r\n",
    "    δnext = δ*params.ρ_δ + e_δ\n",
    "    pnext = p*params.ρ_p + e_p\n",
    "    qnext = q*params.ρ_q + e_q\n",
    "    # (epsilon = (rnext, δnext, pnext, qnext))\n",
    "    \n",
    "    # transition of endogenous states (next denotes variables at t+1)\n",
    "    wnext = np.exp(pnext+qnext) + (w-c)*params.rbar*np.exp(rnext)\n",
    "    # column 1: r\n",
    "    # column 2: p\n",
    "    # column 3: q\n",
    "    # column 4: δ\n",
    "    # column 5: w\n",
    "    state_next = np.column_stack((rnext, pnext, qnext, δnext, wnext))\n",
    "    cnext = σ_func(state_next)\n",
    "    \n",
    "    vals = np.exp(δnext - δ + rnext)*(cnext)**(-params.γ)\n",
    "    Exp_val = params.β*params.rbar*np.dot(params.weights, vals)\n",
    "    \n",
    "    return c**(-params.γ) - np.maximum(Exp_val, (w+params.bc)**(-params.γ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "670f48d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time iteration using Gaussian quadrature\n",
    "def euler_diff_quadrature(c, σ_func, w, r, δ, ps, e_r, e_δ, e_ps, params):\n",
    "    \"\"\"\n",
    "    Set up a function such that the root with respect to c,\n",
    "    Uses Gaussian quadrature for the shocks next period\n",
    "    \n",
    "    w, r, delta: vectors\n",
    "    ps: matrix (row: observations, column: dimension (p1, p2, ..., pl))\n",
    "    e_r, e_δ: vectors\n",
    "    e_ps: matrix (row: observations, column: dimension (e_p1, e_p2, ..., e_pl))\n",
    "    \"\"\"\n",
    "    # transitions of the exogenous processes\n",
    "    rnext = r*params.ρ_r + e_r\n",
    "    δnext = δ*params.ρ_δ + e_δ\n",
    "    # transition for the ps:\n",
    "    # repeat the value ps_value len(e_ps) times\n",
    "    # so each column is p1, p2, ..., pl\n",
    "    ps_next = np.expand_dims(ps, axis=0).repeat(len(e_ps), axis=0)*params.ρ_p + e_ps\n",
    "    # Take the sum rowise (only the sum matters for y_t)\n",
    "    sum_ps_next = np.sum(ps_next, axis=1)\n",
    "    \n",
    "    # transition of endogenous states (next denotes variables at t+1)\n",
    "    wnext = np.exp(sum_ps_next) + (w-c)*params.rbar*np.exp(rnext)\n",
    "    # State next period\n",
    "    # order: w, r, delta, p1, ..., pl\n",
    "    wrδ_next = np.column_stack((wnext, rnext, δnext)) #w, r, delta\n",
    "    state_next = np.hstack((wrδ_next, ps_next)) #add p1, ..., pl\n",
    "    \n",
    "    cnext = σ_func(state_next)\n",
    "    \n",
    "    vals = np.exp(δnext-δ+rnext)*(cnext)**(-params.γ)\n",
    "    # DEBUG.\n",
    "    #print(vals.shape)\n",
    "    #raise(\"error\")\n",
    "    Exp_val = params.β*params.rbar*np.dot(params.weights, vals)\n",
    "    \n",
    "    return c**(-params.γ) - np.maximum(Exp_val, (w+params.bc)**(-params.γ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ea9ea2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time iteration using Gaussian quadrature\n",
    "# σ current guess on grid\n",
    "def K_quadrature_old(σ, params):\n",
    "    \"\"\"\n",
    "    The Coleman-Reffett operator\n",
    "    Uses Gaussian quadrature for the shocks next period\n",
    "    \"\"\"\n",
    "    # First turn σ into a function via interpolation\n",
    "    σ_func = lambda x: interp(params.r_grid, params.p_grid, params.q_grid, params.δ_grid, params.w_grid, σ, x)\n",
    "    # Initialization\n",
    "    σ_new = np.zeros((params.n_points_grid, params.n_points_grid, params.n_points_grid, params.n_points_grid, params.n_points_w))\n",
    "    for (r_index, r_value) in enumerate(params.r_grid):\n",
    "        for (p_index, p_value) in enumerate(params.p_grid):\n",
    "            for (q_index, q_value) in enumerate(params.q_grid):\n",
    "                for (δ_index, δ_value) in enumerate(params.δ_grid):\n",
    "                    for (w_index, w_value) in enumerate(params.w_grid):\n",
    "                        # shocks\n",
    "                        # Gaussian quadrature\n",
    "                        e_r = np.transpose(params.nodes)[:,0]\n",
    "                        e_δ = np.transpose(params.nodes)[:,1]\n",
    "                        e_p = np.transpose(params.nodes)[:,2]\n",
    "                        e_q = np.transpose(params.nodes)[:,3]\n",
    "\n",
    "                        c_star = scipy.optimize.brentq(euler_diff_quadrature, 1e-10, w_value + params.bc, args=(σ_func, r_value, p_value, q_value, δ_value, w_value, e_r, e_δ, e_p, e_q, params))\n",
    "                        σ_new[r_index, p_index, q_index, δ_index, w_index] = c_star\n",
    "    return σ_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ffe7b2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time iteration using Gaussian quadrature\n",
    "# σ current guess on grid\n",
    "def K_quadrature(σ, σ_fun, params):\n",
    "    \"\"\"\n",
    "    The Coleman-Reffett operator\n",
    "    Uses Gaussian quadrature for the shocks next period\n",
    "    \"\"\"\n",
    "    σ_new = np.zeros(σ.shape)\n",
    "    \n",
    "    # Loop over states\n",
    "    # Order:  w, r, delta, p1, p2, ..., pn\n",
    "    for (counter, (indices, vals)) in enumerate(zip(params.wrδ_ps_indices, params.wrδ_ps_grid)):\n",
    "        # States\n",
    "        w_value = vals[0]\n",
    "        r_value = vals[1]\n",
    "        δ_value = vals[2]\n",
    "        ps_value = vals[3:] #p1, p2, ..., pl\n",
    "\n",
    "        # shocks\n",
    "        # Gaussian quadrature\n",
    "        # ORder e_r, e_δ, e_p1, e_p2, ..., e_pl\n",
    "        e_r = np.transpose(params.nodes)[:,0]\n",
    "        e_δ = np.transpose(params.nodes)[:,1]\n",
    "        e_ps = np.transpose(params.nodes)[:,2:] #e_p1, ..., e_p2\n",
    "\n",
    "        c_star = scipy.optimize.brentq(euler_diff_quadrature, 1e-10, w_value + params.bc, args=(σ_fun, w_value, r_value, δ_value, ps_value, e_r, e_δ, e_ps, params))\n",
    "        #print(c_star)\n",
    "        σ_new[indices] = c_star\n",
    "        \n",
    "    return σ_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea64d44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_model_time_iter_quadrature(params, σ_local, σ_fun, \n",
    "                                     tol=1e-4, max_iter=100, \n",
    "                                     verbose=True, print_skip=1):\n",
    "    \n",
    "    \"\"\"\n",
    "    Solve the model by time iteration using Gaussian quadrature for expectations.\n",
    "    Update params.c_grid_TI and params.c_function_TI\n",
    "    \n",
    "    REMARK: does not work. It must be an issue with scopes and eval\n",
    "    \"\"\"\n",
    "\n",
    "    # Set up loop\n",
    "    i = 0\n",
    "    error = tol + 1\n",
    "    \n",
    "    while i < max_iter:\n",
    "        \n",
    "        σ_new = K_quadrature(σ_local, σ_fun, params)\n",
    "        error = np.max(np.abs(σ_local - σ_new))\n",
    "        i += 1\n",
    "        if verbose and i % print_skip == 0:\n",
    "            print(f\"Error at iteration {i} is {error}.\")\n",
    "        σ_local = σ_new\n",
    "\n",
    "        # Turn σ into a function via interpolation\n",
    "        #σ_func = lambda x: interp(params.r_grid, params.p_grid, params.q_grid, params.δ_grid, params.w_grid, σ, x)\n",
    "        #exec('σ_func = lambda x: interp({})'.format(params.str_c_grid_TI_local))\n",
    "        σ_fun = eval('lambda x: interp({})'.format(params.str_c_grid_TI_local)) #interpolate\n",
    "        #exec('σ_func = lambda x: interp({})'.format(params.str_c_grid_TI_local))\n",
    "        \n",
    "        if (error < tol) or (i == max_iter):\n",
    "            if (error < tol):\n",
    "                print(\"Convergence reached after {} iterations\".format(i))\n",
    "            if (i == max_iter):\n",
    "                print(\"Convergence NOT reached after {} iterations\".format(i))\n",
    "            params.c_grid_TI = σ_local\n",
    "            params.c_function_TI = σ_fun\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f29a271",
   "metadata": {},
   "source": [
    "### Accuracy of the time iteration solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "650a4036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy_TI_MC(n, n_Monte_Carlo, params):\n",
    "    \"\"\"\n",
    "    # Function to evaluate the accuracy using Monte Carlo for the expectations\n",
    "    # n: number of draws for current state\n",
    "    # n_Monte_Carlo: number of draws for next state\n",
    "    \"\"\"\n",
    "    # To repeat vectors\n",
    "    repeat_vector = np.ones(n_Monte_Carlo)\n",
    "\n",
    "    # Sparse version\n",
    "    A = sparse.eye(n)\n",
    "    B = sparse.csr_matrix(np.ones(n_Monte_Carlo)/n_Monte_Carlo)\n",
    "    # Sparse kronecker product. Then convert to pytorch sparse\n",
    "    W = sparse.kron(A, B)\n",
    "\n",
    "    # randomly drawing current states    \n",
    "    r = np.random.normal(loc=0, scale=params.σ_e_r, size=(n,)) \n",
    "    δ = np.random.normal(loc=0, scale=params.σ_e_δ, size=(n,)) \n",
    "    #p = np.random.normal(loc=0, scale=params.σ_e_p, size=(n,)) \n",
    "    # matrix: row: obs, column: dimension (p1, p2, ... , pl)\n",
    "    ps = np.random.normal(loc=0, scale=params.σ_e_p, size=(n, params.dim_p)) \n",
    "    \n",
    "    #w = (params.wmin - params.wmax) * torch.rand(n) + params.wmax #uniform\n",
    "    if params.use_Sobol == False:\n",
    "        w = (params.wmin - params.wmax) * torch.rand(n) + params.wmax #uniform\n",
    "    else:\n",
    "    #Very slow if T is large\n",
    "        w = (params.wmin - params.wmax) * params.soboleng.draw(n) + params.wmax #uniform\n",
    "        w = w.squeeze(1)\n",
    "    \n",
    "    w = w.detach().numpy()\n",
    "    \n",
    "    # n_Monte_Carlo for each value today\n",
    "    e_r = np.random.normal(loc=0, scale=params.σ_r, size=(n*n_Monte_Carlo,)) \n",
    "    e_δ = np.random.normal(loc=0, scale=params.σ_δ, size=(n*n_Monte_Carlo,)) \n",
    "    # matrix: row: obs, column: dimension (p1, p2, ... , pl)\n",
    "    e_ps = np.random.normal(loc=0, scale=params.σ_p, size=(n*n_Monte_Carlo,  params.dim_p)) \n",
    "\n",
    "    #-------------------\n",
    "    # Evaluate the model\n",
    "    #-------------------\n",
    "    # state: w, r, delta, p1, ..., pl\n",
    "    wrδ = np.column_stack((w, r, δ)) #w, r, delta\n",
    "    state = np.hstack((wrδ, ps)) #add p1, ..., pl\n",
    "    \n",
    "    c = params.c_function_TI(state)\n",
    "    \n",
    "    r_repeated = np.kron(r, repeat_vector)\n",
    "    δ_repeated = np.kron(δ, repeat_vector)\n",
    "    w_repeated = np.kron(w, repeat_vector)\n",
    "    c_repeated = np.kron(c, repeat_vector)\n",
    "    # repeat in matrix form:\n",
    "    ps_repeated = ps.repeat(n_Monte_Carlo, axis=0) #repeat n_Monte_Carlo times\n",
    "\n",
    "    # transitions of the exogenous processes\n",
    "    #r = torch.normal(mean=0, std=σ_e_r, size=(n,)) \n",
    "    #torch.kron(r, repeat_vector).shape\n",
    "    rnext = r_repeated*params.ρ_r + e_r\n",
    "    δnext = δ_repeated*params.ρ_δ + e_δ\n",
    "\n",
    "    # Matrix\n",
    "    ps_next = ps_repeated*params.ρ_p + e_ps\n",
    "    # Take the sum rowise (only the sum matters for y_t)\n",
    "    sum_ps_next = np.sum(ps_next, axis=1)\n",
    "    \n",
    "    # transition of endogenous states (next denotes variables at t+1)\n",
    "    #wnext = torch.exp(pnext+qnext) + (w[i]-c[i])*rbar*torch.exp(rnext)\n",
    "    wnext = np.exp(sum_ps_next) + (w_repeated-c_repeated)*params.rbar*np.exp(rnext)\n",
    "\n",
    "    # state: w, r, delta, p1, ..., pl\n",
    "    wrδ_next = np.column_stack((wnext, rnext, δnext)) #w, r, delta\n",
    "    state_next = np.hstack((wrδ_next, ps_next)) #add p1, ..., pl\n",
    "    \n",
    "    cnext = params.c_function_TI(state_next)\n",
    "\n",
    "    # Sparse\n",
    "    # torch.exp(δnext-δ[i]+rnext)*(cnext**(-γ))\n",
    "    vals = np.exp(δnext-δ_repeated+rnext)*(cnext**(-params.γ))\n",
    "    # value implied by the RHS of the Euler equation when non binding\n",
    "    c_RHS = params.β*params.rbar*(W*vals)\n",
    "\n",
    "    #-----------------\n",
    "    # Evaluate errors\n",
    "    #-----------------\n",
    "    # Euler error: \n",
    "    #print(c_RHS.shape)\n",
    "\n",
    "    f_nodes_euler =  np.abs((np.maximum(c_RHS, (w + params.bc)**(-params.γ))/(c**(-params.γ))) - 1)\n",
    "    f_nodes_euler_bis = np.abs((((np.maximum(c_RHS, (w + params.bc)**(-params.γ)))**(-1/params.γ))/c) - 1)\n",
    "\n",
    "    return f_nodes_euler, f_nodes_euler_bis, c, c_RHS, w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b887e278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the accuracy using Gaussian quadrature for the expectations\n",
    "def evaluate_accuracy_TI_Gaussian(n, params):\n",
    "    # n: number of draws for current state\n",
    "    \n",
    "    # To repeat vectors\n",
    "    n_Monte_Carlo = len(params.weights) #length of nodes\n",
    "    repeat_vector = np.ones(n_Monte_Carlo)\n",
    "\n",
    "    # Create sparse matrix for fast evaluation of expectations\n",
    "    B_gaussian = sparse.csr_matrix(params.weights)\n",
    "    A_gaussian = sparse.eye(n) \n",
    "    W_gaussian = sparse.kron(A_gaussian, B_gaussian)\n",
    "\n",
    "    # randomly drawing current states    \n",
    "    r = np.random.normal(loc=0, scale=params.σ_e_r, size=(n,)) \n",
    "    δ = np.random.normal(loc=0, scale=params.σ_e_δ, size=(n,))  \n",
    "    # matrix: row: obs, column: dimension (p1, p2, ... , pl)\n",
    "    ps = np.random.normal(loc=0, scale=params.σ_e_p, size=(n, params.dim_p))                \n",
    "\n",
    "    #w = (params.wmin - params.wmax) * torch.rand(n) + params.wmax #uniform\n",
    "    if params.use_Sobol == False:\n",
    "        w = (params.wmin - params.wmax) * torch.rand(n) + params.wmax #uniform\n",
    "    else:\n",
    "    #Very slow if T is large\n",
    "        w = (params.wmin - params.wmax) * params.soboleng.draw(n) + params.wmax #uniform\n",
    "        w = w.squeeze(1)\n",
    "\n",
    "    w = w.detach().numpy()\n",
    "\n",
    "    e_r = np.tile(params.nodes[0,:], n)\n",
    "    e_δ = np.tile(params.nodes[1,:], n)\n",
    "    e_ps = np.transpose(np.tile(params.nodes[2:,:], n)) #e_p1, ..., e_p2\n",
    "\n",
    "    #-------------------\n",
    "    # Evaluate the model\n",
    "    #-------------------\n",
    "    # state: w, r, delta, p1, ..., pl\n",
    "    wrδ = np.column_stack((w, r, δ)) #w, r, delta\n",
    "    state = np.hstack((wrδ, ps)) #add p1, ..., pl\n",
    "\n",
    "    c = params.c_function_TI(state)\n",
    "\n",
    "    # Similar to repeat_interleave\n",
    "    #x = torch.tensor([1, 2, 3])\n",
    "    #>>> x.repeat_interleave(2)\n",
    "    #tensor([1, 1, 2, 2, 3, 3])\n",
    "    r_repeated = np.repeat(r, n_Monte_Carlo)\n",
    "    δ_repeated = np.repeat(δ, n_Monte_Carlo)\n",
    "    w_repeated = np.repeat(w, n_Monte_Carlo)\n",
    "    c_repeated = np.repeat(c, n_Monte_Carlo)\n",
    "    # repeat in matrix form:\n",
    "    ps_repeated = ps.repeat(n_Monte_Carlo, axis=0) #repeat n_Monte_Carlo times\n",
    "\n",
    "    # transitions of the exogenous processes\n",
    "    # Vectors\n",
    "    rnext = r_repeated*params.ρ_r + e_r\n",
    "    δnext = δ_repeated*params.ρ_δ + e_δ\n",
    "    # Matrix\n",
    "    ps_next = ps_repeated*params.ρ_p + e_ps\n",
    "    # Take the sum rowise (only the sum matters for y_t)\n",
    "    sum_ps_next = np.sum(ps_next, axis=1)\n",
    "\n",
    "    # transition of endogenous states (next denotes variables at t+1)\n",
    "    wnext = np.exp(sum_ps_next) + (w_repeated-c_repeated)*params.rbar*np.exp(rnext)\n",
    "\n",
    "    # state: w, r, delta, p1, ..., pl\n",
    "    wrδ_next = np.column_stack((wnext, rnext, δnext)) #w, r, delta\n",
    "    state_next = np.hstack((wrδ_next, ps_next)) #add p1, ..., pl\n",
    "\n",
    "    # consumption next period\n",
    "    cnext = params.c_function_TI(state_next)\n",
    "\n",
    "    vals = np.exp(δnext-δ_repeated+rnext)*(cnext**(-params.γ))\n",
    "    # value implied by the RHS of the Euler equation when non binding\n",
    "    c_RHS = params.β*params.rbar*(W_gaussian*vals)\n",
    "\n",
    "    #-----------------\n",
    "    # Evaluate errors\n",
    "    #-----------------\n",
    "    # Euler error: \n",
    "    #print(c_RHS.shape)\n",
    "    f_nodes_euler =  np.abs((np.maximum(c_RHS, (w + params.bc)**(-params.γ))/(c**(-params.γ))) - 1)\n",
    "    f_nodes_euler_bis = np.abs((((np.maximum(c_RHS, (w + params.bc)**(-params.γ)))**(-1/params.γ))/c) - 1)\n",
    "    \n",
    "    return f_nodes_euler, f_nodes_euler_bis, c, c_RHS, w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9611c49f",
   "metadata": {},
   "source": [
    "## II. All-in-One\n",
    "\n",
    "Special case with N=2 (two shocks for each value of the state variable):\n",
    "\n",
    "$$\\mathcal{L}_2(\\theta) = \\frac{1}{T} \\sum_{t=1}^{T} \\Big( f(s_t,\\epsilon_{1,t}|\\theta) f(s_t,\\epsilon_{2,t}|\\theta) \\Big) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c60cba15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9912/634114853.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Get cpu or gpu device for training.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Using {device} device\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Define model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "# Ouputs:\n",
    "# 1. the share of cash-in-hand consumed\n",
    "# 2. the lagrange multiplier h \n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(5, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ba1e288",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_params(params, limited=True):\n",
    "    \"\"\"\n",
    "    Function to display parameter values\n",
    "    \"\"\"\n",
    "    print(\"learning rate: {}\".format(params.lr))\n",
    "    print(\"nb epochs: {}\".format(params.nb_epochs))\n",
    "    print(\"W.shape: {}\".format(params.W.shape))\n",
    "    print(\"W_expanded.shape: {}\".format(params.W_expanded.shape))\n",
    "    print(\"M: {}\".format(params.M))\n",
    "    print(\"N: {}\".format(params.N))\n",
    "    print(\"MN: {}\".format(params.MN))\n",
    "    print(\"T: {}\".format(params.T))\n",
    "    print(\"T: {}\".format(params.use_Sobol))\n",
    "    if limited == False:\n",
    "        print(\"Number nodes Gaussian Q: {}\".format(params.nodes.shape))\n",
    "        print(\"W Gaussian shape: {}\".format(params.W_gaussian.shape))\n",
    "        print(\"N Gaussian: {}\".format(params.N_gaussian))\n",
    "        print(\"M Gaussian: {}\".format(params.M_gaussian))\n",
    "        print(\"MN Gaussian: {}\".format(params.MN_gaussian))\n",
    "    print(\"Budget constraint: {}\".format(params.bc))\n",
    "    print(\"σ_shocks: {}\".format(params.σ_shocks))\n",
    "    print(\"use_Sobol: {}\".format(params.use_Sobol))\n",
    "    print(\"optimizer_chosen: {}\".format(params.optimizer))\n",
    "    print(\"use_scheduler: {}\".format(params.use_scheduler))\n",
    "    print(\"grid_depth_chosen: {}\".format(params.grid_depth))\n",
    "    print(\"grid_depth_chosen: {}\".format(params. grid_depth))\n",
    "    print(\"surplus_threshold_chosen: {}\".format(params.surplus_threshold))\n",
    "    print(\"w1: {}\".format(params.w1))\n",
    "    print(\"w2: {}\".format(params.w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18c8b947",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape) \n",
    "\n",
    "def convert_sparse_matrix_to_sparse_tensor(X):\n",
    "    \"\"\"Convert a scipy sparse matrix to a tf sparse tensor.\"\"\"\n",
    "    coo = X.tocoo()\n",
    "    indices = np.mat([coo.row, coo.col]).transpose()\n",
    "    return tf.SparseTensor(indices, coo.data, coo.shape)\n",
    "\n",
    "# create a nn class (just-for-fun choice :-) \n",
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        \n",
    "    def forward(self,yhat,y):\n",
    "        return torch.sqrt(self.mse(yhat,y))\n",
    "    \n",
    "# Gaussian quadrature rule\n",
    "# See: https://chaospy.readthedocs.io/en/master/api/chaospy.generate_quadrature.html\n",
    "def dist(order, distribution, rule = \"gaussian\", sp=True):\n",
    "    #order=int(n**(1/d))-1\n",
    "    x, w = chaospy.generate_quadrature(order, distribution, rule=(rule), sparse=sp)\n",
    "    return x, w\n",
    "\n",
    "def create_W_expanded_matrix(M, N):\n",
    "    \"\"\"\n",
    "    create a sparse matrix W_expanded with U repeate M times on the diagonal elements\n",
    "    where U is an upper triangular matrix with 0 on the diagonal and 1 on the other upper elements\n",
    "    W_expanded is a sparse torch matrix\n",
    "    \"\"\"\n",
    "    A_expanded = np.ones((N, N))\n",
    "    U = np.triu(A_expanded) # upper trianguler matrix of ones\n",
    "    np.fill_diagonal(U, 0) #fill diagonal with 0\n",
    "    U = sparse.csr_matrix(U) # convert to sparse\n",
    "    # Unity matrix of size (M*M)\n",
    "    B = sparse.csr_matrix(np.eye(M, M))\n",
    "    W_expanded = sparse_mx_to_torch_sparse_tensor(sparse.kron(B, U))\n",
    "    return W_expanded\n",
    "\n",
    "min_FB = lambda a,b: a+b-tf.sqrt(a**2+b**2)\n",
    "min_FB_torch = lambda a,b: a+b-torch.sqrt(a**2+b**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc3d80ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Vector' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6311/2458334069.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mResiduals_torch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me_r\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mVector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me_δ\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mVector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me_p\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mVector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me_q\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mVector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mVector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mδ\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mVector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mVector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mVector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mVector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# all inputs are expected to have the same size n\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# n = r.size()[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Vector' is not defined"
     ]
    }
   ],
   "source": [
    "def Residuals_torch(model, e_r, e_δ, e_ps, w, r, δ, ps, params):\n",
    "    \"\"\"\n",
    "    Calculate the residuals for the ANN\n",
    "    # order shocks: e_r: Vector, e_δ: Vector, e_ps\n",
    "        * e_r and e_δ are vectors\n",
    "        * e_ps is a matrix. Row: obs, Col: dimenson (e_p1, e_p2, ..., e_pl)\n",
    "    # order state variable: w, r, delta, p1, ..., pl\n",
    "        * w, r and δ are vectors\n",
    "        * ps is a matrix: Row: obs, Col: dimenson (p1, p2, ..., pl)\n",
    "    \n",
    "    \"\"\"\n",
    "    # all inputs are expected to have the same size n\n",
    "    # n = r.size()[0]\n",
    "    \n",
    "    # state\n",
    "    # order: w, r, delta, p1, ..., pl\n",
    "    # \n",
    "    c, h = model_normalized(model, w, r, δ, ps, params)\n",
    "\n",
    "    # When the BC is binding\n",
    "    # bc_binding = ((w + params.bc_torch)/c)**(-params.γ)\n",
    "\n",
    "    # transitions of the exogenous processes\n",
    "    rnext = r*params.ρ_r + e_r\n",
    "    δnext = δ*params.ρ_δ + e_δ\n",
    "    # transition for matrix\n",
    "    ps_next = ps*params.ρ_p + e_ps\n",
    "    # the sum matters for tomorrow\n",
    "    sum_ps_next = torch.sum(ps_next, axis=1)\n",
    "    \n",
    "    # transition of endogenous states (next denotes variables at t+1)\n",
    "    wnext = torch.exp(sum_ps_next) + (w-c)*params.rbar*torch.exp(rnext)\n",
    "    \n",
    "    # state next\n",
    "    # order: w, r, delta, p1, ..., pl\n",
    "    cnext, hnext = model_normalized(model, wnext, rnext, δnext, ps_next, params)\n",
    "\n",
    "    R1 = params.β*params.rbar*torch.exp(δnext-δ+rnext)*(cnext/c)**(-params.γ) - h\n",
    "    R2 = min_FB_torch(1 - ((c - params.bc_torch)/w), 1 - h)\n",
    "    \n",
    "    return R1, R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53492fb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Ξ_torch(model, params): # objective function for DL training\n",
    "\n",
    "    # randomly drawing current states    \n",
    "    r = torch.normal(mean=0, std=params.σ_e_r, size=(params.T,)) \n",
    "    δ = torch.normal(mean=0, std=params.σ_e_δ, size=(params.T,)) \n",
    "    # Matrix for p1, p2, ..., pl\n",
    "    # row: obs, col: dim (p1, p2, ..., pl)\n",
    "    ps = torch.normal(mean=0, std=params.σ_e_p, size=(params.T, params.dim_p)) \n",
    "\n",
    "    if params.use_Sobol_T == False:\n",
    "        w = (params.wmin - params.wmax) * torch.rand(params.T) + params.wmax #uniform\n",
    "    else:\n",
    "    #Very slow if T is large\n",
    "        w = (params.wmin - params.wmax) * params.soboleng.draw(params.T) + params.wmax #uniform\n",
    "        w = w.squeeze(1)\n",
    "        \n",
    "    # randomly drawing 1st realization for shocks    \n",
    "    e1_r = torch.normal(mean=0, std=params.σ_r, size=(params.T,)) \n",
    "    e1_δ = torch.normal(mean=0, std=params.σ_δ, size=(params.T,)) \n",
    "    # Matrix for e_p1, e_p2, ..., e_pl\n",
    "    e1_ps = torch.normal(mean=0, std=params.σ_p, size=(params.T, params.dim_p)) \n",
    "\n",
    "    # randomly drawing 2nd realization for shocks\n",
    "    e2_r = torch.normal(mean=0, std=params.σ_r, size=(params.T,)) \n",
    "    e2_δ = torch.normal(mean=0, std=params.σ_δ, size=(params.T,)) \n",
    "    # Matrix for e_p1, e_p2, ..., e_pl\n",
    "    e2_ps = torch.normal(mean=0, std=params.σ_p, size=(params.T, params.dim_p)) \n",
    " \n",
    "    # residuals for n random grid points under 2 realizations of shocks\n",
    "    # e_r, e_δ, e_ps, w, r, δ, ps\n",
    "    R1_e1, R2_e1 = Residuals_torch(model, e1_r, e1_δ, e1_ps, w, r, δ, ps, params)\n",
    "    R1_e2, R2_e2 = Residuals_torch(model, e2_r, e2_δ, e2_ps, w, r, δ, ps, params)\n",
    "\n",
    "    # construct all-in-one expectation operator\n",
    "    # Modification: take absolute value. Sometimes negative values can happen\n",
    "    R_squared = params.w1*R1_e1*R1_e2 + params.w2*R2_e1*R2_e2\n",
    "    \n",
    "    # V1. give a summary of all the draws:\n",
    "    return torch.mean(R_squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2e3b4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate prediction using an ANN\n",
    "def model_normalized(model, w_grid, r_grid, δ_grid, ps_matrix, params, bound=1e-6):\n",
    "    \"\"\"\n",
    "    Predict consumption using an ANN\n",
    "    Inputs of the ANN are normalized\n",
    "    \"\"\"\n",
    "    # Create the state space \n",
    "    # order: #w, r, delta,p1, ..., pl\n",
    "    # normalize vectors\n",
    "    w_normalized = (w_grid - params.wmin)/(params.wmax - params.wmin)*2.0-1.0\n",
    "    r_normalized = r_grid/(params.σ_e_r*2)\n",
    "    δ_normalized = δ_grid/(params.σ_e_δ*2)\n",
    "    # normalize matrix\n",
    "    ps_normalized = ps_matrix/(params.σ_e_p*2) #works because same distribution for all shocks\n",
    "    wrδ_normalized = torch.column_stack((w_normalized, r_normalized, δ_normalized)) #w, r, delta\n",
    "    state = torch.hstack((wrδ_normalized, ps_normalized)) #add p1, ..., pl\n",
    "    \n",
    "    x = model(state)\n",
    "    \n",
    "    # consumption share is always in [0,1]\n",
    "    ζ0 = torch.sigmoid( x[:,0] )\n",
    "    # Truncate corner solutions\n",
    "    ζ1 = torch.minimum(torch.maximum(ζ0, torch.tensor([bound])), torch.tensor([1.0 - bound]))\n",
    "    # Consume a fraction of wealth + borrowing constraint\n",
    "    if len(w_grid.shape) == 2:\n",
    "        ζ2 = torch.sqrt(ζ1*(w_grid.squeeze(1) + params.bc_torch))\n",
    "    else:\n",
    "        ζ2 = torch.sqrt(ζ1*(w_grid + params.bc_torch))\n",
    "    \n",
    "    # expectation of marginal consumption is always positive\n",
    "    h = torch.exp( x[:, 1] )\n",
    "    \n",
    "    return ζ2, h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0a6c7f",
   "metadata": {},
   "source": [
    "### To measure the accuracy of the AIO operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a30ce9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy_pytorch_MC(model, n, n_Monte_Carlo, params):\n",
    "    \"\"\"\n",
    "    Function to evaluate the accuracy using Monte Carlo for the expectations\n",
    "    Use new draws at each call\n",
    "    \"\"\"\n",
    "    # n: number of draws for current state\n",
    "    # n_Monte_Carlo: number of draws for next state\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # To repeat vectors\n",
    "        repeat_vector = torch.ones(n_Monte_Carlo)\n",
    "\n",
    "        # To calculate means quickly, vectorize\n",
    "        # Sparse version\n",
    "        A = sparse.eye(n)\n",
    "        B = sparse.csr_matrix(np.ones(n_Monte_Carlo)/n_Monte_Carlo)\n",
    "        # Sparse kronecker product. Then convert to pytorch sparse\n",
    "        W = sparse_mx_to_torch_sparse_tensor(sparse.kron(A, B))\n",
    "    \n",
    "        # randomly drawing current states\n",
    "        r = torch.normal(mean=0, std=params.σ_e_r, size=(n,)) \n",
    "        δ = torch.normal(mean=0, std=params.σ_e_δ, size=(n,)) \n",
    "        # Matrix\n",
    "        ps = torch.normal(mean=0, std=params.σ_e_p, size=(n, params.dim_p)) \n",
    "\n",
    "        #w = (params.wmin - params.wmax) * torch.rand(n) + params.wmax #uniform\n",
    "        if params.use_Sobol == False:\n",
    "            w = (params.wmin - params.wmax) * torch.rand(n) + params.wmax #uniform\n",
    "        else:\n",
    "        #Very slow if T is large\n",
    "            w = (params.wmin - params.wmax) * params.soboleng.draw(n) + params.wmax #uniform\n",
    "            w = w.squeeze(1)\n",
    "            \n",
    "        # n_Monte_Carlo for each value today\n",
    "        e_r = torch.normal(mean=0, std=params.σ_r, size=(n*n_Monte_Carlo,)) \n",
    "        e_δ = torch.normal(mean=0, std=params.σ_δ, size=(n*n_Monte_Carlo,)) \n",
    "        # Matrix\n",
    "        e_ps = torch.normal(mean=0, std=params.σ_p, size=(n*n_Monte_Carlo, params.dim_p)) \n",
    "\n",
    "        #-------------------\n",
    "        # Evaluate the model\n",
    "        #-------------------\n",
    "        # order: w, r, delta, p1, ..., pl\n",
    "        c, h = model_normalized(model, w, r, δ, ps, params)\n",
    "    \n",
    "        # repeat vectors\n",
    "        r_repeated = torch.kron(r, repeat_vector)\n",
    "        δ_repeated = torch.kron(δ, repeat_vector)\n",
    "        w_repeated = torch.kron(w, repeat_vector)\n",
    "        c_repeated = torch.kron(c, repeat_vector)\n",
    "        # repeat matrix\n",
    "        ps_repeated = ps.repeat_interleave(n_Monte_Carlo, dim=0) #repeat N times matrix\n",
    "\n",
    "        # transitions of the exogenous processes\n",
    "        rnext = r_repeated*params.ρ_r + e_r\n",
    "        δnext = δ_repeated*params.ρ_δ + e_δ\n",
    "        # matrix operation\n",
    "        ps_next = ps_repeated*params.ρ_p + e_ps\n",
    "        # Take the sum rowise (only the sum matters for y_t)\n",
    "        sum_ps_next = torch.sum(ps_next, axis=1)\n",
    "        \n",
    "        # transition of endogenous states (next denotes variables at t+1)\n",
    "        #wnext = torch.exp(pnext+qnext) + (w[i]-c[i])*rbar*torch.exp(rnext)\n",
    "        wnext = torch.exp(sum_ps_next) + (w_repeated-c_repeated)*params.rbar*torch.exp(rnext)\n",
    "        # state next\n",
    "        # order: w, r, delta, p1, ..., pl\n",
    "        cnext, hnext = model_normalized(model, wnext, rnext, δnext, ps_next, params)\n",
    "        \n",
    "        # Sparse\n",
    "        # torch.exp(δnext-δ[i]+rnext)*(cnext**(-γ))\n",
    "        vals = (torch.exp(δnext-δ_repeated+rnext)*(cnext**(-params.γ))).unsqueeze(1)\n",
    "        # value implied by the RHS of the Euler equation when non binding\n",
    "        c_RHS = params.β*params.rbar*torch.sparse.mm(W, vals).squeeze(1)\n",
    "\n",
    "        #-----------------\n",
    "        # Evaluate errors\n",
    "        #-----------------\n",
    "        # Euler error: \n",
    "        f_nodes_euler =  torch.abs((torch.maximum(c_RHS, (w + params.bc_torch)**(-params.γ))/(c**(-params.γ))) - 1)\n",
    "        f_nodes_euler_bis = torch.abs((((torch.maximum(c_RHS, (w+ params.bc_torch)**(-params.γ)))**(-1/params.γ))/c) - 1)\n",
    "        \n",
    "    return f_nodes_euler.numpy(), f_nodes_euler_bis.numpy(), c.numpy(), c_RHS.numpy(), w.numpy()\n",
    "\n",
    "\n",
    "def evaluate_accuracy_pytorch_MC_frozen(model, params):\n",
    "    \"\"\"\n",
    "    Function to evaluate the accuracy using Monte Carlo for the expectations\n",
    "    Use a pre-determined series of shocks to approximate the expectations\n",
    "    \"\"\"\n",
    "    # n: number of draws for current state\n",
    "    # n_Monte_Carlo: number of draws for next state\n",
    "    \n",
    "    with torch.no_grad(): \n",
    "        #-------------------\n",
    "        # Evaluate the model\n",
    "        #-------------------\n",
    "        # order: w, r, delta, p1, ..., pl\n",
    "        # params.p_accuracy is a matrix with (p1, ..., pl)\n",
    "        c, h = model_normalized(model, params.w_accuracy, params.r_accuracy, params.δ_accuracy, params.p_accuracy, params)\n",
    "        #c = ζ*params.w_accuracy\n",
    "\n",
    "        # repeat consumption\n",
    "        c_repeated = torch.kron(c, params.repeat_vector_accuracy)\n",
    "\n",
    "        # transition of endogenous states (next denotes variables at t+1)\n",
    "        #wnext = torch.exp(pnext+qnext) + (w[i]-c[i])*rbar*torch.exp(rnext)\n",
    "        wnext = torch.exp(params.pnext_sum_accuracy) + (params.w_repeated_accuracy - c_repeated)*params.rbar*torch.exp(params.rnext_accuracy)\n",
    "       \n",
    "        # order: w, r, delta, p1, ..., pl\n",
    "        # params.p_accuracy is a matrix with (p1, ..., pl)\n",
    "        cnext, hnext = model_normalized(model, wnext, params.rnext_accuracy, params.δnext_accuracy, params.pnext_accuracy, params)\n",
    "        #cnext = ζnext*wnext\n",
    "\n",
    "        # Sparse\n",
    "        # torch.exp(δnext-δ[i]+rnext)*(cnext**(-γ))\n",
    "        vals = (torch.exp(params.δnext_accuracy - params.δ_repeated_accuracy + params.rnext_accuracy)*(cnext**(-params.γ))).unsqueeze(1)\n",
    "        # value implied by the RHS of the Euler equation when non binding\n",
    "        c_RHS = params.β*params.rbar*torch.sparse.mm(params.W_accuracy, vals).squeeze(1)\n",
    "\n",
    "        #-----------------\n",
    "        # Evaluate errors\n",
    "        #-----------------\n",
    "        f_nodes_euler = torch.abs((torch.maximum(c_RHS, (params.w_accuracy + params.bc_torch)**(-params.γ))/(c**(-params.γ))) - 1)\n",
    "        f_nodes_euler_bis = torch.abs((((torch.maximum(c_RHS, (params.w_accuracy + params.bc_torch)**(-params.γ)))**(-1/params.γ))/c) - 1)\n",
    "    \n",
    "    return f_nodes_euler.numpy(), f_nodes_euler_bis.numpy(), c.numpy(), c_RHS.numpy(), params.w_accuracy.numpy()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7e6532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy_pytorch_Gaussian(model, n, params):\n",
    "    \"\"\"\n",
    "    Function to evaluate the accuracy using Gaussian quadrature for the expectations\n",
    "    Use new draws at each call\n",
    "    \"\"\"\n",
    "    # n: number of draws for current state\n",
    "    # n_Monte_Carlo: number of draws for next state\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # To repeat vectors\n",
    "        n_Monte_Carlo = len(params.weights) #length of nodes\n",
    "        repeat_vector = torch.ones(n_Monte_Carlo)\n",
    "        \n",
    "        # To calculate means quickly, vectorize\n",
    "        # Sparse version\n",
    "        A = sparse.eye(n)\n",
    "        B = sparse.csr_matrix(params.weights)\n",
    "        # Sparse kronecker product. Then convert to pytorch sparse\n",
    "        W = sparse_mx_to_torch_sparse_tensor(sparse.kron(A, B))\n",
    "    \n",
    "        # randomly drawing current states\n",
    "        r = torch.normal(mean=0, std=params.σ_e_r, size=(n,)) \n",
    "        δ = torch.normal(mean=0, std=params.σ_e_δ, size=(n,)) \n",
    "        # Matrix\n",
    "        ps = torch.normal(mean=0, std=params.σ_e_p, size=(n, params.dim_p)) \n",
    "\n",
    "        #w = (params.wmin - params.wmax) * torch.rand(n) + params.wmax #uniform\n",
    "        if params.use_Sobol == False:\n",
    "            w = (params.wmin - params.wmax) * torch.rand(n) + params.wmax #uniform\n",
    "        else:\n",
    "        #Very slow if T is large\n",
    "            w = (params.wmin - params.wmax) * params.soboleng.draw(n) + params.wmax #uniform\n",
    "            w = w.squeeze(1)\n",
    "            \n",
    "        # Shocks\n",
    "        e_r = params.nodes_torch[:,0].float().repeat(n)\n",
    "        e_δ = params.nodes_torch[:,1].float().repeat(n)\n",
    "        e_ps = params.nodes_torch[:,2:].float().repeat(n, 1) #e_p1, ..., e_p2\n",
    "\n",
    "        #-------------------\n",
    "        # Evaluate the model\n",
    "        #-------------------\n",
    "        # order: w, r, delta, p1, ..., pl\n",
    "        c, h = model_normalized(model, w, r, δ, ps, params)\n",
    "    \n",
    "        # repeat vectors\n",
    "        #r_repeated = torch.kron(r, repeat_vector)\n",
    "        #δ_repeated = torch.kron(δ, repeat_vector)\n",
    "        #w_repeated = torch.kron(w, repeat_vector)\n",
    "        #c_repeated = torch.kron(c, repeat_vector)\n",
    "        r_repeated = r.repeat_interleave(n_Monte_Carlo)\n",
    "        δ_repeated = δ.repeat_interleave(n_Monte_Carlo)\n",
    "        w_repeated = w.repeat_interleave(n_Monte_Carlo)\n",
    "        c_repeated = c.repeat_interleave(n_Monte_Carlo)  \n",
    "        # repeat matrix\n",
    "        ps_repeated = ps.repeat_interleave(n_Monte_Carlo, dim=0) #repeat N times matrix\n",
    "\n",
    "        # transitions of the exogenous processes\n",
    "        rnext = r_repeated*params.ρ_r + e_r\n",
    "        δnext = δ_repeated*params.ρ_δ + e_δ\n",
    "        # matrix operation\n",
    "        ps_next = ps_repeated*params.ρ_p + e_ps\n",
    "        # Take the sum rowise (only the sum matters for y_t)\n",
    "        sum_ps_next = torch.sum(ps_next, axis=1)\n",
    "        \n",
    "        # transition of endogenous states (next denotes variables at t+1)\n",
    "        #wnext = torch.exp(pnext+qnext) + (w[i]-c[i])*rbar*torch.exp(rnext)\n",
    "        wnext = torch.exp(sum_ps_next) + (w_repeated-c_repeated)*params.rbar*torch.exp(rnext)\n",
    "        # state next\n",
    "        # order: w, r, delta, p1, ..., pl\n",
    "        cnext, hnext = model_normalized(model, wnext, rnext, δnext, ps_next, params)\n",
    "        \n",
    "        # Sparse\n",
    "        # torch.exp(δnext-δ[i]+rnext)*(cnext**(-γ))\n",
    "        vals = (torch.exp(δnext-δ_repeated+rnext)*(cnext**(-params.γ))).unsqueeze(1)\n",
    "        # value implied by the RHS of the Euler equation when non binding\n",
    "        c_RHS = params.β*params.rbar*torch.sparse.mm(W, vals).squeeze(1)\n",
    "\n",
    "        #-----------------\n",
    "        # Evaluate errors\n",
    "        #-----------------repeat_vector = np.ones(n_Monte_Carlo)\n",
    "        f_nodes_euler =  torch.abs((torch.maximum(c_RHS, (w + params.bc_torch)**(-params.γ))/(c**(-params.γ))) - 1)\n",
    "        f_nodes_euler_bis = torch.abs((((torch.maximum(c_RHS, (w+ params.bc_torch)**(-params.γ)))**(-1/params.γ))/c) - 1)\n",
    "        \n",
    "    return f_nodes_euler.numpy(), f_nodes_euler_bis.numpy(), c.numpy(), c_RHS.numpy(), w.numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84919be1",
   "metadata": {},
   "source": [
    "## III. Monte Carlo\n",
    "\n",
    "* Implement the developed formula\n",
    "\n",
    "$$ \\frac{1}{M} \\frac{2}{(N)(N-1)} \\sum_{m=1}^{M} \\sum_{1\\leq i < j}^{n} f(s_m, \\epsilon_{m}^{(i)})f(s_m, \\epsilon_{m}^{(j)})  $$\n",
    "\n",
    "* This formula can be vectorized as follows:\n",
    "\n",
    "\n",
    "$$ f' \\Big(I_N \\otimes U\\Big). f $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "950d4e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ξ_torch_MC(model, params): # objective function for DL training\n",
    "\n",
    "    # randomly drawing current states\n",
    "    r = torch.normal(mean=0, std=params.σ_e_r, size=(params.M,)) \n",
    "    δ = torch.normal(mean=0, std=params.σ_e_δ, size=(params.M,)) \n",
    "    # matrix: row: obs, column: dim (p1, p2, ..., pl)\n",
    "    ps = torch.normal(mean=0, std=params.σ_e_p, size=(params.M, params.dim_p)) \n",
    "    \n",
    "    #w = (params.wmin - params.wmax) * torch.rand(params.M) + params.wmax #uniform\n",
    "    if params.use_Sobol == False:\n",
    "        w = (params.wmin - params.wmax) * torch.rand(params.M) + params.wmax #uniform\n",
    "    else:\n",
    "    #Very slow if T is large\n",
    "        w = (params.wmin - params.wmax) * params.soboleng.draw(params.M) + params.wmax #uniform\n",
    "        w = w.squeeze(1)\n",
    "        \n",
    "    # repeat elements N times\n",
    "    r_repeated = r.repeat_interleave(params.N) #MN*1 matrix\n",
    "    δ_repeated = δ.repeat_interleave(params.N)\n",
    "    w_repeated = w.repeat_interleave(params.N)\n",
    "    ps_repeated = ps.repeat_interleave(params.N, dim=0) #repeat N times matrix\n",
    "    \n",
    "    # n_Monte_Carlo for each value today\n",
    "    e1_r = torch.normal(mean=0, std=params.σ_r, size=(params.MN,)) \n",
    "    e1_δ = torch.normal(mean=0, std=params.σ_δ, size=(params.MN,)) \n",
    "    # matrix: row: obs, column: dim (p1, p2, ..., pl)\n",
    "    e1_ps = torch.normal(mean=0, std=params.σ_p, size=(params.MN, params.dim_p)) \n",
    "\n",
    "    # residuals for n random grid points under 2 realizations of shocks\n",
    "    # e_r: Vector, e_δ: Vector, e_ps, w: Vector, r: Vector, δ: Vector, ps, params\n",
    "    R1, R2 = Residuals_torch(model, e1_r, e1_δ, e1_ps, w_repeated, r_repeated, δ_repeated, ps_repeated, params)\n",
    "        \n",
    "    # Lagrange multiplier and Euler equation:\n",
    "    # Remark: number of elements in the sum is M(N)(N-1)/2\n",
    "    output = params.w1*(2/((params.M)*(params.N)*(params.N - 1)))*torch.matmul(R1.unsqueeze(1).t(), torch.matmul(params.W_expanded, R1.unsqueeze(1))) + params.w2*torch.mean(R2**2)\n",
    "    # Squeeze to a scalar\n",
    "    return torch.mean(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62305d2b",
   "metadata": {},
   "source": [
    "## Optimal choice of training parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb084d7",
   "metadata": {},
   "source": [
    "### Monte Carlo estimator: optimal choice of M and N\n",
    "\n",
    "In a Monte Carlo estimation, the estimation error (difference between true integral and approximation) is bounded above by:\n",
    "\n",
    "$$ E(|e(f,.)|) \\leq \\sqrt(\\frac{var(MC)}{M})$$\n",
    "\n",
    "The variance of the monte Carlo estimator depends on the choice of M and N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "743fc8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grad_norm(parameters, norm_type=2.0):\n",
    "    \"\"\"\n",
    "    Compute norm over gradients of model parameters.\n",
    "\n",
    "    :param parameters:\n",
    "        the model parameters for gradient norm calculation. Iterable of\n",
    "        Tensors or single Tensor\n",
    "    :param norm_type:\n",
    "        type of p-norm to use\n",
    "\n",
    "    :returns:\n",
    "        the computed gradient norm\n",
    "    \"\"\"\n",
    "    if isinstance(parameters, torch.Tensor):\n",
    "        parameters = [parameters]\n",
    "    parameters = [p for p in parameters if p is not None and p.grad is not None]\n",
    "    total_norm = 0\n",
    "    for p in parameters:\n",
    "        param_norm = p.grad.data.norm(norm_type)\n",
    "        total_norm += param_norm.item() ** norm_type\n",
    "    return total_norm ** (1.0 / norm_type) \n",
    "\n",
    "# Calculate the variance of the loss for several choices of M and N\n",
    "# Start with a fresh model. Then train it for some time. \n",
    "# Then calculate variance of loss\n",
    "def calculate_variance_loss(params, nb_epoch_opt_M_N, nb_rep, \n",
    "                            nb_draws_loss, grid_M, grid_N,\n",
    "                            norm_chosen = 2.0, freq_accuracy=1000, \n",
    "                            calculate_variance_gradient = False,\n",
    "                            initial_model = NeuralNetwork().to(device)):\n",
    "    # To store results\n",
    "    df_variance_loss = pd.DataFrame()\n",
    "    print(\"Norm chosen: {}\".format(norm_chosen))\n",
    "    \n",
    "    for k in range(0, nb_rep):\n",
    "        #-----------------------------------------\n",
    "        # A. Train the model for nb_epoch_opt_M_N\n",
    "        #-----------------------------------------\n",
    "        # Initialize a network\n",
    "        print(\"Rep {} / {}. Training a model for {} Iterations\".format(int(k), nb_rep, nb_epoch_opt_M_N))\n",
    "        \n",
    "        # Initialize a network using a fixed model (copy parameters)\n",
    "        model_opt_M_N = copy.deepcopy(initial_model) \n",
    "        # Training mode\n",
    "        model_opt_M_N.train()\n",
    "\n",
    "        if params.optimizer == \"Adam\":\n",
    "            optimizer = torch.optim.Adam(model_opt_M_N.parameters(), lr=params.lr, eps=1e-07, betas=(0.9, 0.999)) \n",
    "        elif params.optimizer == \"SGD\":\n",
    "            optimizer = torch.optim.SGD(model_opt_M_N.parameters(), params.lr)\n",
    "        elif params.optimizer == \"SWA\":\n",
    "            base_opt = torch.optim.Adam(model_opt_M_N.parameters(), lr=params.lr, eps=1e-07, betas=(0.9, 0.999)) \n",
    "            optimizer = SWA(base_opt, swa_start=params.swa_start, swa_freq=params.swa_freq, swa_lr=params.lr)\n",
    "        else:\n",
    "            raise(\"optimizer unknown\")\n",
    "\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=params.freq_gamma)\n",
    "        list_perc_abs_error_opt_M_N = [] #store abs value percentage error\n",
    "        list_perc_abs_error_opt_M_N_i = []\n",
    "        loss_epochs_opt_M_N = torch.zeros(nb_epoch_opt_M_N)\n",
    "\n",
    "        for i in range(0, nb_epoch_opt_M_N):\n",
    "            \n",
    "            optimizer.zero_grad() #clear gradient\n",
    "            \n",
    "            loss = Ξ_torch_MC(model_opt_M_N, params) #loss \n",
    "            loss_epochs_opt_M_N[[i]] = float(loss.item())\n",
    "\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "            if i % freq_accuracy == 0: #Monitor accuracy\n",
    "                model_opt_M_N.eval() #eval mode\n",
    "                # Evaluate accuracy \n",
    "                with torch.no_grad():\n",
    "                    euler, euler_bis, c, c_RHS, w = evaluate_accuracy_pytorch_MC_frozen(model_opt_M_N, params)\n",
    "                list_perc_abs_error_opt_M_N.append(np.median(euler_bis)) #euler error\n",
    "                list_perc_abs_error_opt_M_N_i.append(i) #append index\n",
    "                model_opt_M_N.train() #back to train mode\n",
    "            if i % 1000 == 0:\n",
    "                loss, current = float(loss.item()), i\n",
    "                print(f\"loss: {loss:>7f}, median percentage euler error {list_perc_abs_error_opt_M_N[-1]:>7f}, [{current:>5d}/{nb_epoch_opt_M_N:>5d}]\")\n",
    "            if (i % params.freq_scheduler == 0) & (i != 0) & (params.use_scheduler == True):\n",
    "                scheduler.step()\n",
    "                print(\"i : {}. Decreasing learning rate: {}\".format(i, scheduler.get_last_lr()))\n",
    "\n",
    "        if params.optimizer == \"SWA\":\n",
    "            optimizer.swap_swa_sgd()\n",
    "        #--------------------------------------\n",
    "        # B. Calculate the variance of the loss\n",
    "        #--------------------------------------\n",
    "        print(\"Rep {} / {}. Calculting variance loss for {} Iterations\".format(int(k), nb_rep, nb_draws_loss))\n",
    "    \n",
    "        model_opt_M_N.eval()\n",
    "        var_loss = torch.zeros(len(grid_N))\n",
    "        std_loss = torch.zeros(len(grid_N))\n",
    "        mean_loss = torch.zeros(len(grid_N))\n",
    "\n",
    "        # Loop over choice of N and M\n",
    "        for (ind, (N_chosen, M_chosen)) in enumerate(zip(grid_N.astype('int'), grid_M.astype('int'))):\n",
    "            # Change M and N\n",
    "            # N_chosen, M_chosen, lr_chosen, pre_train_model_chosen, nb_epochs_chosen, bc_chosen, order_gauss\n",
    "            params_local = MyParams(N_chosen, M_chosen, params.lr, \n",
    "                                    params.pre_train_model, params.nb_epochs,\n",
    "                                    params.bc, params.order_gauss,\n",
    "                                    params.σ_shocks,  params.use_Sobol,  \n",
    "                                    params.optimizer, params.dim_p)\n",
    "\n",
    "            with torch.no_grad():        \n",
    "                Xms = torch.zeros(nb_draws_loss)\n",
    "                # Loop over realizations of loss function\n",
    "                for (j_index, j) in enumerate(range(0, nb_draws_loss)):\n",
    "                    Xms[j] = Ξ_torch_MC(model_opt_M_N, params_local)\n",
    "                # Calculate mean and variance:\n",
    "                var_loss[ind] = torch.var(Xms)\n",
    "                std_loss[ind] = torch.sqrt(var_loss[ind])\n",
    "                mean_loss[ind] = torch.mean(Xms)\n",
    "        #-----------------------------------------------\n",
    "        # C. Calculate variance of the norm the gradient\n",
    "        #-----------------------------------------------\n",
    "        print(\"Rep {} / {}. Calculting variance norm gradient for {} Iterations\".format(int(k), nb_rep, nb_draws_loss))\n",
    "        var_gradient_loss = torch.zeros(len(grid_N )) #variance \n",
    "        std_gradient_loss = torch.zeros(len(grid_N )) #standard deviation\n",
    "        mean_gradient_loss = torch.zeros(len(grid_N ))\n",
    "        \n",
    "        if calculate_variance_gradient == True:\n",
    "            for (ind, (N_chosen, M_chosen)) in enumerate(zip(grid_N.astype('int'), grid_M.astype('int'))):\n",
    "                \n",
    "                # N_chosen, M_chosen, lr_chosen, pre_train_model_chosen, nb_epochs_chosen, bc_chosen, order_gauss\n",
    "                params_local = MyParams(N_chosen, M_chosen, params.lr, \n",
    "                                    params.pre_train_model, params.nb_epochs,\n",
    "                                    params.bc, params.order_gauss,\n",
    "                                    params.σ_shocks,  params.use_Sobol,  \n",
    "                                    params.optimizer, params.dim_p)\n",
    "                \n",
    "    \n",
    "                Xms = torch.zeros(nb_draws_loss)\n",
    "                # Loop over draws \n",
    "                for (j_index, j) in enumerate(range(0, nb_draws_loss)):\n",
    "                    optimizer.zero_grad() # clear gradient\n",
    "                    loss = Ξ_torch_MC(model_opt_M_N, params_local)\n",
    "                    loss.backward() #calculate gradient\n",
    "                    # Store the norm of the gradient\n",
    "                    total_norm = compute_grad_norm(model_opt_M_N.parameters(), norm_type=norm_chosen)\n",
    "                    Xms[j] = total_norm\n",
    "                var_gradient_loss[ind] = torch.var(Xms)\n",
    "                std_gradient_loss[ind] = torch.sqrt(var_gradient_loss[ind])\n",
    "                mean_gradient_loss[ind] = torch.mean(Xms)\n",
    "\n",
    "        # Save to dataframe\n",
    "        if df_variance_loss.empty == True:\n",
    "            df_variance_loss = pd.DataFrame({'N': grid_N,\n",
    "                                  'M': grid_M,\n",
    "                                  'var_loss': var_loss,\n",
    "                                  'std_loss': std_loss,\n",
    "                                  'mean_loss': mean_loss,\n",
    "                                  'var_gradient_loss': var_gradient_loss,\n",
    "                                  'std_gradient_loss': std_gradient_loss,\n",
    "                                  'mean_gradient_loss': mean_gradient_loss,\n",
    "                                  'nb_rep': k})\n",
    "        else:\n",
    "            df_variance_loss_bis = pd.DataFrame({'N': grid_N,\n",
    "                                  'M': grid_M,\n",
    "                                  'var_loss': var_loss,\n",
    "                                  'std_loss': std_loss,\n",
    "                                  'mean_loss': mean_loss,\n",
    "                                  'var_gradient_loss': var_gradient_loss,\n",
    "                                  'std_gradient_loss': std_gradient_loss,\n",
    "                                  'mean_gradient_loss': mean_gradient_loss,\n",
    "                                  'nb_rep': k})\n",
    "            df_variance_loss = pd.concat([df_variance_loss, df_variance_loss_bis])\n",
    "        \n",
    "    # Replace NAN by large values\n",
    "    list_cols = [\"var_loss\", \"std_loss\", \"mean_loss\", \n",
    "                \"std_gradient_loss\", \"var_gradient_loss\", \"mean_gradient_loss\"]\n",
    "    \n",
    "    for col in list_cols:\n",
    "        df_variance_loss[col] = df_variance_loss[col].fillna(np.nanmax(df_variance_loss[col]))\n",
    "    \n",
    "    # Stats on df_var\n",
    "    # Median values\n",
    "    df_variance_loss_median = df_variance_loss.groupby('M').median().reset_index()\n",
    "\n",
    "    for col in list_cols:\n",
    "        df_variance_loss_median[\"min_\" + col] = df_variance_loss.groupby('M')[col].min().reset_index()[col]                     \n",
    "        df_variance_loss_median[\"max_\" + col] = df_variance_loss.groupby('M')[col].max().reset_index()[col]\n",
    "        df_variance_loss_median[\"std_\" + col] = df_variance_loss.groupby('M')[col].std().reset_index()[col]\n",
    "        for qq in [10, 25, 50, 75, 90]:\n",
    "            df_variance_loss_median[\"P\" + str(qq) + \"_\" + col] = df_variance_loss.groupby('M')[col].quantile(qq/100).reset_index()[col]\n",
    "    \n",
    "\n",
    "    return df_variance_loss, df_variance_loss_median\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4fe858",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimizer(model, optimizer_name, lr, momentum):\n",
    "    \"\"\"\n",
    "    Function to create an optimizer\n",
    "    \"\"\"\n",
    "    if optimizer_name == \"Adam\":\n",
    "        #optimizer = torch.optim.Adam(model.parameters(), lr=lr, eps=1e-07, betas=(0.9, 0.999)) \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr) \n",
    "    elif optimizer_name == \"SGD\":\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr)\n",
    "    elif optimizer_name == \"SGD-momentum\":\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr, momentum)\n",
    "    elif optimizer_name == \"Adadelta\":\n",
    "        optimizer = torch.optim.Adadelta(model.parameters(), lr)\n",
    "    elif optimizer_name == \"RMSprop\":\n",
    "        optimizer = torch.optim.RMSprop(model.parameters(), lr)\n",
    "    #elif optimizer_name == \"LBFGS\":\n",
    "    #    torch.optim.LBFGS(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        raise NameError(f\"optimizer {optimizer_name} unknown\")\n",
    "    return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05821e59",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NeuralNetwork' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6204/1722005461.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Solve a model several times, holding parameters constant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# TODO: deal with nan when averaging.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mcalculate_several_runs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_rep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeuralNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;31m#initialization of lists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlist_elapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'NeuralNetwork' is not defined"
     ]
    }
   ],
   "source": [
    "# Solve a model several times, holding parameters constant\n",
    "# TODO: deal with nan when averaging.\n",
    "def calculate_several_runs(params, nb_rep, freq_loss = 1, initial_model = NeuralNetwork().to(device)):\n",
    "    #initialization of lists\n",
    "    list_elapsed_time = []\n",
    "    list_M = []\n",
    "    list_N = []\n",
    "    list_list_losses = [] \n",
    "    list_list_perc_abs_error_MC = [] \n",
    "    list_list_perc_abs_error_MC_i = [] \n",
    "    list_list_perc_abs_error_MC_loss = [] \n",
    "    list_lr = []\n",
    "    list_index_rep = []\n",
    "      \n",
    "    # A.\n",
    "    # Run seral times\n",
    "    for k in range(0, nb_rep):\n",
    "            print(\"Training model : {} / {}\".format(int(k), nb_rep))\n",
    "            list_index_rep.append(k)\n",
    "            list_M.append(params.M)\n",
    "            list_N.append(params.N)\n",
    "            list_lr.append(params.lr)\n",
    "\n",
    "            #-----------------------------------------------------\n",
    "            # Train a model for some periods\n",
    "            # Then look at the expected variance of the gradient\n",
    "            #-----------------------------------------------------\n",
    "            # Initialize a network using a fixed model (copy parameters)\n",
    "            model_MC = copy.deepcopy(initial_model) \n",
    "\n",
    "            if params.optimizer == \"Adam\":\n",
    "                optimizer_MC = torch.optim.Adam(model_MC.parameters(), lr=params.lr, eps=1e-07, betas=(0.9, 0.999)) \n",
    "            elif params.optimizer == \"SGD\":\n",
    "                optimizer_MC = torch.optim.SGD(model_MC.parameters(), params.lr)\n",
    "            elif params.optimizer == \"SWA\":\n",
    "                base_opt_MC = torch.optim.Adam(model_MC.parameters(), lr=params.lr, eps=1e-07, betas=(0.9, 0.999)) \n",
    "                optimizer_MC = SWA(base_opt_MC, swa_start=params.swa_start, swa_freq=params.swa_freq, swa_lr=params.lr)\n",
    "            else:\n",
    "                raise(\"optimizer unknown\")\n",
    "\n",
    "            scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer_MC, gamma=params.freq_gamma)\n",
    "            loss_epochs_MC = torch.zeros(params.nb_epochs)\n",
    "            list_perc_abs_error_MC = [] #store abs value percentage error\n",
    "            list_perc_abs_error_MC_i = [] #store index i\n",
    "            list_perc_abs_error_MC_loss = [] #store loss\n",
    "\n",
    "            for i in range(0, params.nb_epochs):\n",
    "                \n",
    "                optimizer_MC.zero_grad() #clear gradient\n",
    "                \n",
    "                loss = Ξ_torch_MC(model_MC, params) #calculate loss\n",
    "                loss_epochs_MC[[i]] = float(loss.item())\n",
    "                \n",
    "                # Backpropagation\n",
    "                loss.backward()\n",
    "                \n",
    "                # Gradient descent step\n",
    "                optimizer_MC.step()\n",
    "\n",
    "                if i % freq_loss == 0: #Monitor the predictive power\n",
    "                    model_MC.eval() #eval mode\n",
    "                    # Evaluate accuracy \n",
    "                    with torch.no_grad():\n",
    "                        euler, euler_bis, c, c_RHS, w = evaluate_accuracy_pytorch_MC_frozen(model_MC, params)\n",
    "                    list_perc_abs_error_MC.append(np.median(euler_bis)) #euler error\n",
    "                    list_perc_abs_error_MC_i.append(i) #append index\n",
    "                    list_perc_abs_error_MC_loss.append(float(loss.item()))\n",
    "                    model_MC.train() #back to train mode\n",
    "                if i % 1000 == 0:\n",
    "                    loss, current = float(loss.item()), i\n",
    "                    print(f\"loss: {loss:>7f} [{current:>5d}/{params.nb_epochs:>5d}]\")\n",
    "                if (i % params.freq_scheduler == 0) & (i != 0) & (params.use_scheduler == True):\n",
    "                    scheduler.step()\n",
    "                    print(\"i : {}. Decreasing learning rate: {}\".format(i, scheduler.get_last_lr()))\n",
    "                    print(f\"loss: {loss:>7f}, median percentage error {list_perc_abs_error_MC[-1]:>7f}, [{current:>5d}/{params.nb_epochs:>5d}]\")\n",
    "\n",
    "            list_list_losses.append(loss_epochs_MC)\n",
    "            list_list_perc_abs_error_MC.append(list_perc_abs_error_MC)\n",
    "            list_list_perc_abs_error_MC_i.append(list_perc_abs_error_MC_i)\n",
    "            list_list_perc_abs_error_MC_loss.append(list_perc_abs_error_MC_loss)\n",
    "\n",
    "            if params.optimizer == \"SWA\":\n",
    "                optimizer_MC.swap_swa_sgd()\n",
    "                \n",
    "    # B. Compile results\n",
    "    #-------------------\n",
    "    df_MC = pd.DataFrame({'M': list_M[0],\n",
    "                        'N': list_N[0],\n",
    "                        'repetition': list_index_rep[0],\n",
    "                        'iter': list_list_perc_abs_error_MC_i[0],\n",
    "                        'loss': np.sqrt(np.abs(list_list_perc_abs_error_MC_loss[0])),\n",
    "                        'med_percentage_error': list_list_perc_abs_error_MC[0]\n",
    "       })\n",
    "\n",
    "    for k in range(0, len(list_list_perc_abs_error_MC)):\n",
    "        df_MC_bis = pd.DataFrame({'M': list_M[k],\n",
    "                                'N': list_N[k],\n",
    "                                'repetition': list_index_rep[k],\n",
    "                                'iter': list_list_perc_abs_error_MC_i[k],\n",
    "                                'loss': np.sqrt(np.abs(list_list_perc_abs_error_MC_loss[k])),\n",
    "                                'med_percentage_error': list_list_perc_abs_error_MC[k]})\n",
    "        df_MC = pd.concat([df_MC, df_MC_bis])\n",
    "    \n",
    "    # Replace NAN by large values\n",
    "    for col in [\"loss\", \"med_percentage_error\"]:\n",
    "        df_MC[col] = df_MC[col].fillna(np.nanmax(df_MC[col]))\n",
    "    \n",
    "    # C. Statistics on results\n",
    "    df_MC_average = df_MC.groupby('iter').mean().reset_index() #mean value by itera\n",
    "\n",
    "    list_cols = [\"loss\", \"med_percentage_error\"]\n",
    "    \n",
    "    for col in list_cols:\n",
    "        df_MC_average[\"min_\" + col] = df_MC.groupby('iter')[col].min().reset_index()[col]                     \n",
    "        df_MC_average[\"max_\" + col] = df_MC.groupby('iter')[col].max().reset_index()[col]\n",
    "        df_MC_average[\"std_\" + col] = df_MC.groupby('iter')[col].std().reset_index()[col]\n",
    "        for qq in [1, 5, 10, 25, 50, 75, 90, 95, 99]:\n",
    "            df_MC_average[\"P\" + str(qq) + \"_\" + col] = df_MC.groupby('iter')[col].quantile(qq/100).reset_index()[col]\n",
    "\n",
    "    # Add extra info\n",
    "    df_MC_average['optimizer'] = params.optimizer\n",
    "    df_MC_average['sigma_e'] = params.σ_e\n",
    "    df_MC_average['Sobol'] = params.use_Sobol\n",
    "    df_MC_average['user_scheduler'] = params.use_scheduler\n",
    "    df_MC_average['gamma_scheduler'] = params.freq_gamma\n",
    "    df_MC_average['lr'] = params.lr\n",
    "\n",
    "    return df_MC, df_MC_average"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d73ea6",
   "metadata": {},
   "source": [
    "# III. Sparse grids and adaptive sparse grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a3f7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time iteration using Gaussian quadrature\n",
    "# σ current guess on grid\n",
    "def K_quadrature_sparse(σ, σ_fun, params, grid_points):\n",
    "    \"\"\"\n",
    "    The Coleman-Reffett operator\n",
    "    Uses Gaussian quadrature for the shocks next period\n",
    "    \"\"\"\n",
    "    # Initialization\n",
    "    σ_new = np.zeros((grid_points.shape[0], 1))\n",
    "    \n",
    "    # Loop over states\n",
    "    # Order:  w, r, delta, p1, p2, ..., pn\n",
    "    for (indices, vals) in enumerate(grid_points):\n",
    "        # States\n",
    "        w_value = vals[0]\n",
    "        r_value = vals[1]\n",
    "        δ_value = vals[2]\n",
    "        ps_value = vals[3:] #p1, p2, ..., pl\n",
    "\n",
    "        # shocks\n",
    "        # Gaussian quadrature\n",
    "        # ORder e_r, e_δ, e_p1, e_p2, ..., e_pl\n",
    "        e_r = np.transpose(params.nodes)[:,0]\n",
    "        e_δ = np.transpose(params.nodes)[:,1]\n",
    "        e_ps = np.transpose(params.nodes)[:,2:] #e_p1, ..., e_p2\n",
    "\n",
    "        c_star = scipy.optimize.brentq(euler_diff_quadrature, 1e-10, w_value + params.bc, args=(σ_fun, w_value, r_value, δ_value, ps_value, e_r, e_δ, e_ps, params))\n",
    "        #print(c_star)\n",
    "        σ_new[indices] = c_star\n",
    "        \n",
    "    return σ_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6f9ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy_TI_MC_sparse(n, n_Monte_Carlo, params, debug = False):\n",
    "    \"\"\"\n",
    "    # Function to evaluate the accuracy using Monte Carlo for the expectations\n",
    "    # Using the sparse grid solution.\n",
    "    # n: number of draws for current state\n",
    "    # n_Monte_Carlo: number of draws for next state\n",
    "    \"\"\"\n",
    "    # To repeat vectors\n",
    "    repeat_vector = np.ones(n_Monte_Carlo)\n",
    "\n",
    "    # Sparse version\n",
    "    A = sparse.eye(n)\n",
    "    B = sparse.csr_matrix(np.ones(n_Monte_Carlo)/n_Monte_Carlo)\n",
    "    # Sparse kronecker product. Then convert to pytorch sparse\n",
    "    W = sparse.kron(A, B)\n",
    "\n",
    "    # randomly drawing current states    \n",
    "    r = np.random.normal(loc=0, scale=params.σ_e_r, size=(n,)) \n",
    "    δ = np.random.normal(loc=0, scale=params.σ_e_δ, size=(n,)) \n",
    "    # matrix: row: obs, column: dimension (p1, p2, ... , pl)\n",
    "    ps = np.random.normal(loc=0, scale=params.σ_e_p, size=(n, params.dim_p)) \n",
    "    \n",
    "    #w = (params.wmin - params.wmax) * torch.rand(n) + params.wmax #uniform\n",
    "    if params.use_Sobol == False:\n",
    "        w = (params.wmin - params.wmax) * torch.rand(n) + params.wmax #uniform\n",
    "    else:\n",
    "    #Very slow if T is large\n",
    "        w = (params.wmin - params.wmax) * params.soboleng.draw(n) + params.wmax #uniform\n",
    "        w = w.squeeze(1)\n",
    "    \n",
    "    w = w.detach().numpy()\n",
    "    \n",
    "    # n_Monte_Carlo for each value today\n",
    "    e_r = np.random.normal(loc=0, scale=params.σ_r, size=(n*n_Monte_Carlo,)) \n",
    "    e_δ = np.random.normal(loc=0, scale=params.σ_δ, size=(n*n_Monte_Carlo,)) \n",
    "    # matrix: row: obs, column: dimension (p1, p2, ... , pl)\n",
    "    e_ps = np.random.normal(loc=0, scale=params.σ_p, size=(n*n_Monte_Carlo,  params.dim_p)) \n",
    "    \n",
    "\n",
    "    #-------------------\n",
    "    # Evaluate the model\n",
    "    #-------------------\n",
    "    # state: w, r, delta, p1, ..., pl\n",
    "    wrδ = np.column_stack((w, r, δ)) #w, r, delta\n",
    "    state = np.hstack((wrδ, ps)) #add p1, ..., pl\n",
    "    \n",
    "    if debug == False:\n",
    "        c = params.c_function_TI_sparse(state)\n",
    "    else:\n",
    "        c = params.c_function_TI(state)\n",
    "    \n",
    "    r_repeated = np.kron(r, repeat_vector)\n",
    "    δ_repeated = np.kron(δ, repeat_vector)\n",
    "    w_repeated = np.kron(w, repeat_vector)\n",
    "    c_repeated = np.kron(c, repeat_vector)\n",
    "    # repeat in matrix form:\n",
    "    ps_repeated = ps.repeat(n_Monte_Carlo, axis=0) #repeat n_Monte_Carlo times\n",
    "\n",
    "    # transitions of the exogenous processes\n",
    "    #r = torch.normal(mean=0, std=σ_e_r, size=(n,)) \n",
    "    #torch.kron(r, repeat_vector).shape\n",
    "    rnext = r_repeated*params.ρ_r + e_r\n",
    "    δnext = δ_repeated*params.ρ_δ + e_δ\n",
    "\n",
    "    # Matrix\n",
    "    ps_next = ps_repeated*params.ρ_p + e_ps\n",
    "    # Take the sum rowise (only the sum matters for y_t)\n",
    "    sum_ps_next = np.sum(ps_next, axis=1)\n",
    "    \n",
    "    # transition of endogenous states (next denotes variables at t+1)\n",
    "    #wnext = torch.exp(pnext+qnext) + (w[i]-c[i])*rbar*torch.exp(rnext)\n",
    "    wnext = np.exp(sum_ps_next) + (w_repeated-c_repeated)*params.rbar*np.exp(rnext)\n",
    "\n",
    "    # state: w, r, delta, p1, ..., pl\n",
    "    wrδ_next = np.column_stack((wnext, rnext, δnext)) #w, r, delta\n",
    "    state_next = np.hstack((wrδ_next, ps_next)) #add p1, ..., pl\n",
    "    \n",
    "    if debug == False:\n",
    "        cnext = params.c_function_TI_sparse(state_next)\n",
    "    else:\n",
    "        cnext = params.c_function_TI(state_next)\n",
    "\n",
    "    # Sparse\n",
    "    # torch.exp(δnext-δ[i]+rnext)*(cnext**(-γ))\n",
    "    vals = np.exp(δnext-δ_repeated+rnext)*(cnext**(-params.γ))\n",
    "    # value implied by the RHS of the Euler equation when non binding\n",
    "    c_RHS = params.β*params.rbar*(W*vals)\n",
    "\n",
    "    #-----------------\n",
    "    # Evaluate errors\n",
    "    #-----------------\n",
    "    # Euler error: \n",
    "    #print(c_RHS.shape)\n",
    "\n",
    "    f_nodes_euler =  np.abs((np.maximum(c_RHS, (w + params.bc)**(-params.γ))/(c**(-params.γ))) - 1)\n",
    "    f_nodes_euler_bis = np.abs((((np.maximum(c_RHS, (w + params.bc)**(-params.γ)))**(-1/params.γ))/c) - 1)\n",
    "\n",
    "    return f_nodes_euler, f_nodes_euler_bis, c, c_RHS, w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb95984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the accuracy using Gaussian quadrature for the expectations\n",
    "# When sparse or adaptive sparse grids are used\n",
    "def evaluate_accuracy_TI_Gaussian_sparse(n, params, debug=False):\n",
    "    # n: number of draws for current state\n",
    "    \n",
    "    # To repeat vectors\n",
    "    n_Monte_Carlo = len(params.weights) #length of nodes\n",
    "    repeat_vector = np.ones(n_Monte_Carlo)\n",
    "\n",
    "    # Create sparse matrix for fast evaluation of expectations\n",
    "    B_gaussian = sparse.csr_matrix(params.weights)\n",
    "    A_gaussian = sparse.eye(n) \n",
    "    W_gaussian = sparse.kron(A_gaussian, B_gaussian)\n",
    "\n",
    "    # randomly drawing current states    \n",
    "    r = np.random.normal(loc=0, scale=params.σ_e_r, size=(n,)) \n",
    "    δ = np.random.normal(loc=0, scale=params.σ_e_δ, size=(n,))  \n",
    "    # matrix: row: obs, column: dimension (p1, p2, ... , pl)\n",
    "    ps = np.random.normal(loc=0, scale=params.σ_e_p, size=(n, params.dim_p))                \n",
    "\n",
    "    #w = (params.wmin - params.wmax) * torch.rand(n) + params.wmax #uniform\n",
    "    if params.use_Sobol == False:\n",
    "        w = (params.wmin - params.wmax) * torch.rand(n) + params.wmax #uniform\n",
    "    else:\n",
    "    #Very slow if T is large\n",
    "        w = (params.wmin - params.wmax) * params.soboleng.draw(n) + params.wmax #uniform\n",
    "        w = w.squeeze(1)\n",
    "\n",
    "    w = w.detach().numpy()\n",
    "\n",
    "    e_r = np.tile(params.nodes[0,:], n)\n",
    "    e_δ = np.tile(params.nodes[1,:], n)\n",
    "    e_ps = np.transpose(np.tile(params.nodes[2:,:], n)) #e_p1, ..., e_p2\n",
    "\n",
    "    #-------------------\n",
    "    # Evaluate the model\n",
    "    #-------------------\n",
    "    # state: w, r, delta, p1, ..., pl\n",
    "    wrδ = np.column_stack((w, r, δ)) #w, r, delta\n",
    "    state = np.hstack((wrδ, ps)) #add p1, ..., pl\n",
    "\n",
    "    if debug == False:\n",
    "        c = params.c_function_TI_sparse(state)\n",
    "    else:\n",
    "        c = params.c_function_TI(state)\n",
    "        \n",
    "    # Similar to repeat_interleave\n",
    "    #x = torch.tensor([1, 2, 3])\n",
    "    #>>> x.repeat_interleave(2)\n",
    "    #tensor([1, 1, 2, 2, 3, 3])\n",
    "    r_repeated = np.repeat(r, n_Monte_Carlo)\n",
    "    δ_repeated = np.repeat(δ, n_Monte_Carlo)\n",
    "    w_repeated = np.repeat(w, n_Monte_Carlo)\n",
    "    c_repeated = np.repeat(c, n_Monte_Carlo)\n",
    "    # repeat in matrix form:\n",
    "    ps_repeated = ps.repeat(n_Monte_Carlo, axis=0) #repeat n_Monte_Carlo times\n",
    "\n",
    "    # transitions of the exogenous processes\n",
    "    # Vectors\n",
    "    rnext = r_repeated*params.ρ_r + e_r\n",
    "    δnext = δ_repeated*params.ρ_δ + e_δ\n",
    "    # Matrix\n",
    "    ps_next = ps_repeated*params.ρ_p + e_ps\n",
    "    # Take the sum rowise (only the sum matters for y_t)\n",
    "    sum_ps_next = np.sum(ps_next, axis=1)\n",
    "\n",
    "    # transition of endogenous states (next denotes variables at t+1)\n",
    "    wnext = np.exp(sum_ps_next) + (w_repeated-c_repeated)*params.rbar*np.exp(rnext)\n",
    "\n",
    "    # state: w, r, delta, p1, ..., pl\n",
    "    wrδ_next = np.column_stack((wnext, rnext, δnext)) #w, r, delta\n",
    "    state_next = np.hstack((wrδ_next, ps_next)) #add p1, ..., pl\n",
    "\n",
    "    # consumption next period\n",
    "    if debug == False:\n",
    "        cnext = params.c_function_TI_sparse(state_next)\n",
    "    else:\n",
    "        cnext = params.c_function_TI(state_next)\n",
    "        \n",
    "    vals = np.exp(δnext - δ_repeated + rnext)*(cnext**(-params.γ))\n",
    "    # value implied by the RHS of the Euler equation when non binding\n",
    "    c_RHS = params.β*params.rbar*(W_gaussian*vals)\n",
    "\n",
    "    #-----------------\n",
    "    # Evaluate errors\n",
    "    #-----------------\n",
    "    # Euler error: \n",
    "    #print(c_RHS.shape)\n",
    "    f_nodes_euler =  np.abs((np.maximum(c_RHS, (w + params.bc)**(-params.γ))/(c**(-params.γ))) - 1)\n",
    "    f_nodes_euler_bis = np.abs((((np.maximum(c_RHS, (w + params.bc)**(-params.γ)))**(-1/params.γ))/c) - 1)\n",
    "    \n",
    "    return f_nodes_euler, f_nodes_euler_bis, c, c_RHS, w\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fd0f49",
   "metadata": {},
   "source": [
    "## Variance estimation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2079d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Given a model, calculate the current variance of the loss\n",
    "#Brute force\n",
    "def calculate_variance_loss_model(model, params, nb_draws_loss):\n",
    "    model.eval() #eval mode\n",
    "    with torch.no_grad():        \n",
    "        Xms = torch.zeros(nb_draws_loss)\n",
    "        # Loop over realizations of loss function\n",
    "        for (j_index, j) in enumerate(range(0, nb_draws_loss)):\n",
    "            Xms[j] = Ξ_torch_MC(model, params)\n",
    "        # Calculate mean and variance:\n",
    "        var_loss = torch.var(Xms)\n",
    "        mean_loss = torch.mean(Xms)\n",
    "    model.train() #train mode\n",
    "    return var_loss, mean_loss\n",
    "\n",
    "#Given a model, calculate the current variance of the loss\n",
    "def calculate_variance_loss_model_grid(model, params, nb_draws_loss, grid_N, grid_M):\n",
    "    var_loss = torch.zeros(len(grid_N)) #to store results\n",
    "    # Loop over choice of N and M\n",
    "    for (ind, (N_chosen, M_chosen)) in enumerate(zip(grid_N, grid_M)):\n",
    "        # Change M and N\n",
    "        params_local = MyParams(int(N_chosen), int(M_chosen), params.lr, params.pre_train_model,\n",
    "                  params.nb_epochs, params.bc, params.order_gauss,\n",
    "                  params.σ_shocks, params.use_Sobol, params.optimizer, \n",
    "                  params.dim_p, params.grid_depth, \n",
    "                  params.nb_refinements, params.surplus_threshold, \n",
    "                  \"params\", params.n_points_w, params.n_points_grid,\n",
    "                  params.w1, params.w2)\n",
    "\n",
    "        var, mean = calculate_variance_loss_model(model, params_local, nb_draws_loss)\n",
    "        var_loss[ind] = var\n",
    "    return var_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2e84dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_states(params, len_series):\n",
    "    \"\"\"\n",
    "    Simulate state vector\n",
    "    \"\"\"\n",
    "    # randomly drawing current states\n",
    "    r_vector = torch.normal(mean=0, std=params.σ_e_r, size=(len_series,)) \n",
    "    δ_vector = torch.normal(mean=0, std=params.σ_e_δ, size=(len_series,)) \n",
    "    # Matrix\n",
    "    ps_matrix = torch.normal(mean=0, std=params.σ_e_p, size=(len_series, params.dim_p)) \n",
    "\n",
    "    #w = (params.wmin - params.wmax) * torch.rand(n) + params.wmax #uniform\n",
    "    if params.use_Sobol == False:\n",
    "        w_vector = (params.wmin - params.wmax) * torch.rand(len_series) + params.wmax #uniform\n",
    "    else:\n",
    "    #Very slow if T is large\n",
    "        w_vector = (params.wmin - params.wmax) * params.soboleng.draw(len_series) + params.wmax #uniform\n",
    "        w_vector = w_vector.squeeze(1)\n",
    "            \n",
    "    return w_vector, r_vector, δ_vector, ps_matrix\n",
    "\n",
    "def simulate_shocks(params, len_series):\n",
    "    # randomly drawing 1st realization for shocks    \n",
    "    e_r = torch.normal(mean=0, std=params.σ_r, size=(len_series,)) \n",
    "    e_δ = torch.normal(mean=0, std=params.σ_δ, size=(len_series,)) \n",
    "    # Matrix for e_p1, e_p2, ..., e_pl\n",
    "    e_ps = torch.normal(mean=0, std=params.σ_p, size=(len_series, params.dim_p))\n",
    "    \n",
    "    return e_r, e_δ, e_ps  \n",
    "\n",
    "def calculate_variance_gaussian(params, model, nb_draws, grid_M, grid_N):\n",
    "    \"\"\"\n",
    "    Calculate variance of the loss when joint gaussian assumption holds\n",
    "    Use var(f(s_m,e^i_m))\n",
    "    and cov(f(s_m,e^i_m), f(s_m,e^j_m))\n",
    "    Return: variance loss function on grid, var(resid), cov(resid)\n",
    "    \"\"\"\n",
    "    grid_T = torch.tensor(grid_M*grid_N/2)\n",
    "    grid_N = torch.tensor(grid_N)\n",
    "    \n",
    "    # Calculate variance and covariance\n",
    "    with torch.no_grad(): \n",
    "        # state vector\n",
    "        # w, r, delta, p1, p2, ..., pl\n",
    "        w, r, δ, ps = sim_states(params, nb_draws)\n",
    "\n",
    "        e1_r, e1_δ, e1_ps   = simulate_shocks(params, nb_draws)\n",
    "        e2_r, e2_δ, e2_ps   = simulate_shocks(params, nb_draws)\n",
    "                \n",
    "        # residuals for n random grid points under 2 realizations of shocks        \n",
    "        R1_e1, R2_e1 = Residuals_torch(model, e1_r, e1_δ, e1_ps, w, r, δ, ps, params)\n",
    "        R1_e2, R2_e2 = Residuals_torch(model, e2_r, e2_δ, e2_ps, w, r, δ, ps, params)\n",
    "\n",
    "        R1 = params.w1*R1_e1 + params.w2*R2_e1\n",
    "        R2 = params.w1*R1_e2 + params.w2*R2_e2\n",
    "\n",
    "\n",
    "        # Construct combinations\n",
    "        # mean\n",
    "        mean_val = 0.5*torch.mean(R1) + 0.5*torch.mean(R2)\n",
    "        \n",
    "        ## Var\n",
    "        var_R1 = torch.var(R1)\n",
    "        var_R2 = torch.var(R2)\n",
    "        var_val = 0.5*var_R1 + 0.5*var_R2\n",
    "        \n",
    "        ## Cov\n",
    "        cov_val = torch.cov(torch.column_stack((R1, R2)).T)[0,1]\n",
    "        \n",
    "        var_L = (1/(grid_T*(grid_N - 1)))*((grid_N**2 - 3*grid_N + 3)*(cov_val**2) + (2*(grid_N - 2)*cov_val + var_val)*var_val + 2*(grid_N - 1)*(var_val + (grid_N - 1)*cov_val)*(mean_val**2))\n",
    "        \n",
    "        return var_L, var_val, cov_val\n",
    "    \n",
    "    \n",
    "def calculate_variance_loss_fast(params, model, nb_draws, grid_M, grid_N):\n",
    "    \"\"\"\n",
    "    Calculate variance of the loss using proposition appendix\n",
    "    Use four independent shocks \n",
    "    \"\"\"\n",
    "    grid_T = torch.tensor(grid_M*grid_N/2)\n",
    "    grid_N = torch.tensor(grid_N)\n",
    "    \n",
    "    # Calculate variance and covariance\n",
    "    with torch.no_grad():            \n",
    "        # state vector\n",
    "        # w, r, delta, p1, p2, ..., pl\n",
    "        w, r, δ, ps = sim_states(params, nb_draws)\n",
    "\n",
    "        e1_r, e1_δ, e1_ps   = simulate_shocks(params, nb_draws)\n",
    "        e2_r, e2_δ, e2_ps   = simulate_shocks(params, nb_draws)\n",
    "        e3_r, e3_δ, e3_ps   = simulate_shocks(params, nb_draws)\n",
    "        e4_r, e4_δ, e4_ps   = simulate_shocks(params, nb_draws)\n",
    "        \n",
    "        \n",
    "        # residuals for n random grid points under 2 realizations of shocks\n",
    "        R1_e1, R2_e1 = Residuals_torch(model, e1_r, e1_δ, e1_ps, w, r, δ, ps, params)\n",
    "        R1_e2, R2_e2 = Residuals_torch(model, e2_r, e2_δ, e2_ps, w, r, δ, ps, params)\n",
    "        R1_e3, R2_e3 = Residuals_torch(model, e3_r, e3_δ, e3_ps, w, r, δ, ps, params)\n",
    "        R1_e4, R2_e4 = Residuals_torch(model, e4_r, e4_δ, e4_ps, w, r, δ, ps, params)\n",
    "\n",
    "        R1 = params.w1*R1_e1 + params.w2*R2_e1\n",
    "        R2 = params.w1*R1_e2 + params.w2*R2_e2\n",
    "        R3 = params.w1*R1_e3 + params.w2*R2_e3\n",
    "        R4 = params.w1*R1_e4 + params.w2*R2_e4\n",
    "        \n",
    "        # Construct combinations\n",
    "        R1_R2 = R1*R2\n",
    "        R1_R3 = R1*R3\n",
    "        R1_R4 = R1*R4\n",
    "        R2_R3 = R2*R3\n",
    "        R2_R4 = R2*R4\n",
    "        R3_R4 = R3*R4\n",
    "\n",
    "        # Variance cross\n",
    "        var_R1_R2 = torch.var(R1_R2)\n",
    "\n",
    "        # Co-variances with one shared element\n",
    "        cov_R1R2_R1R3 = torch.cov(torch.column_stack((R1_R2, R1_R3)).T)[0,1]\n",
    "\n",
    "        # Covariances with four different terms\n",
    "        cov_R1R2_R3R4 = torch.cov(torch.column_stack((R1_R2, R3_R4)).T)[0,1]\n",
    "\n",
    "        var_L = (1/(grid_T*(grid_N - 1)))*(var_R1_R2 + 2*(grid_N - 2)*(cov_R1R2_R1R3) + 2*((grid_N*(grid_N - 1)/4) - grid_N + 3/2)*(cov_R1R2_R3R4))\n",
    "\n",
    "    return var_L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f1d441",
   "metadata": {},
   "source": [
    "## Logging and other utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9e0121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_flat(a):\n",
    "    \"\"\"\n",
    "    Function to flatten a list\n",
    "    \"\"\"\n",
    "    return list(np.array(a).flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60ff7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSystemInfo():\n",
    "    \"\"\"\n",
    "    Get info on computer hardware\n",
    "    \"\"\"\n",
    "    try:\n",
    "        info={}\n",
    "        info['platform']=platform.system()\n",
    "        info['platform-release']=platform.release()\n",
    "        info['platform-version']=platform.version()\n",
    "        info['architecture']=platform.machine()\n",
    "        info['hostname']=socket.gethostname()\n",
    "        info['ip-address']=socket.gethostbyname(socket.gethostname())\n",
    "        info['mac-address']=':'.join(re.findall('..', '%012x' % uuid.getnode()))\n",
    "        info['processor']=platform.processor()\n",
    "        info['ram']=str(round(psutil.virtual_memory().total / (1024.0 **3)))+\" GB\"\n",
    "        return json.dumps(info)\n",
    "    except Exception as e:\n",
    "        logging.exception(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
