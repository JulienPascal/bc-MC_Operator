{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccbae77f",
   "metadata": {},
   "source": [
    "# Functions for neogrowth model\n",
    "\n",
    "## Description\n",
    "\n",
    "store functions for the file neogrowth_model.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4173ead7",
   "metadata": {},
   "source": [
    "## I. Time iteration\n",
    "\n",
    "Time iteration code is from QuantEcon website.\n",
    "\n",
    "Source: https://python.quantecon.org/coleman_policy_iter.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03c3969b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def v_star(y, α, β, μ):\n",
    "    \"\"\"\n",
    "    True value function\n",
    "    \"\"\"\n",
    "    c1 = np.log(1 - α * β) / (1 - β)\n",
    "    c2 = (μ + α * np.log(α * β)) / (1 - α)\n",
    "    c3 = 1 / (1 - β)\n",
    "    c4 = 1 / (1 - α * β)\n",
    "    return c1 + c2 * (c3 - c4) + c4 * np.log(y)\n",
    "\n",
    "def σ_star(y, α, β):\n",
    "    \"\"\"\n",
    "    True optimal policy\n",
    "    \"\"\"\n",
    "    return (1 - α * β) * y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddf35cf5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'float64' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_434275/2162555624.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m opt_growth_data = [\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0;34m'α'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m          \u001b[0;31m# Production parameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;34m'β'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m          \u001b[0;31m# Discount factor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;34m'μ'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m          \u001b[0;31m# Shock location parameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;34m's'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m          \u001b[0;31m# Shock scale parameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'float64' is not defined"
     ]
    }
   ],
   "source": [
    "opt_growth_data = [\n",
    "    ('α', float64),          # Production parameter\n",
    "    ('β', float64),          # Discount factor\n",
    "    ('μ', float64),          # Shock location parameter\n",
    "    ('s', float64),          # Shock scale parameter\n",
    "    ('grid', float64[:]),    # Grid (array)\n",
    "    ('shocks', float64[:])   # Shock draws (array)\n",
    "]\n",
    "\n",
    "@jitclass(opt_growth_data)\n",
    "class OptimalGrowthModel:\n",
    "\n",
    "    def __init__(self,\n",
    "                α=0.4, \n",
    "                β=0.96, \n",
    "                μ=0,\n",
    "                s=0.5,\n",
    "                grid_max=4,\n",
    "                grid_size=120,\n",
    "                shock_size=250,\n",
    "                seed=1234):\n",
    "\n",
    "        self.α, self.β, self.μ, self.s = α, β, μ, s\n",
    "\n",
    "        # Set up grid\n",
    "        self.grid = np.linspace(1e-5, grid_max, grid_size)\n",
    "\n",
    "        # Store shocks (with a seed, so results are reproducible)\n",
    "        np.random.seed(seed)\n",
    "        self.shocks = np.exp(μ + s * np.random.randn(shock_size))\n",
    "       \n",
    "\n",
    "    def f(self, k):\n",
    "        \"The production function\"\n",
    "        return k**self.α\n",
    "       \n",
    "\n",
    "    def u(self, c):\n",
    "        \"The utility function\"\n",
    "        return np.log(c)\n",
    "\n",
    "    def f_prime(self, k):\n",
    "        \"Derivative of f\"\n",
    "        return self.α * (k**(self.α - 1))\n",
    "\n",
    "\n",
    "    def u_prime(self, c):\n",
    "        \"Derivative of u\"\n",
    "        return 1/c\n",
    "\n",
    "    def u_prime_inv(self, c):\n",
    "        \"Inverse of u'\"\n",
    "        return 1/c\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b85b147c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'njit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_434275/58803119.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mnjit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0meuler_diff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mσ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \"\"\"\n\u001b[1;32m      4\u001b[0m     \u001b[0mSet\u001b[0m \u001b[0mup\u001b[0m \u001b[0ma\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0msuch\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mrespect\u001b[0m \u001b[0mto\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mgiven\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mσ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mequal\u001b[0m \u001b[0mto\u001b[0m \u001b[0mKσ\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'njit' is not defined"
     ]
    }
   ],
   "source": [
    "@njit\n",
    "def euler_diff(c, σ, y, og):\n",
    "    \"\"\"\n",
    "    Set up a function such that the root with respect to c,\n",
    "    given y and σ, is equal to Kσ(y).\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    β, shocks, grid = og.β, og.shocks, og.grid\n",
    "    f, f_prime, u_prime = og.f, og.f_prime, og.u_prime\n",
    "\n",
    "    # First turn σ into a function via interpolation\n",
    "    σ_func = lambda x: interp(grid, σ, x)\n",
    "\n",
    "    # Now set up the function we need to find the root of.\n",
    "    vals = u_prime(σ_func(f(y - c) * shocks)) * f_prime(y - c) * shocks\n",
    "    return u_prime(c) - β * np.mean(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "077c2986",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'njit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_434275/25751699.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mnjit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mσ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \"\"\"\n\u001b[1;32m      4\u001b[0m     \u001b[0mThe\u001b[0m \u001b[0mColeman\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mReffett\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'njit' is not defined"
     ]
    }
   ],
   "source": [
    "@njit\n",
    "def K(σ, og):\n",
    "    \"\"\"\n",
    "    The Coleman-Reffett operator\n",
    "\n",
    "     Here og is an instance of OptimalGrowthModel.\n",
    "    \"\"\"\n",
    "\n",
    "    β = og.β\n",
    "    f, f_prime, u_prime = og.f, og.f_prime, og.u_prime\n",
    "    grid, shocks = og.grid, og.shocks\n",
    "\n",
    "    σ_new = np.empty_like(σ)\n",
    "    for i, y in enumerate(grid):\n",
    "        # Solve for optimal c at y\n",
    "        c_star = brentq(euler_diff, 1e-12, y-1e-12, args=(σ, y, og))[0]\n",
    "        σ_new[i] = c_star\n",
    "\n",
    "    return σ_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ea9ea2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_model_time_iter(model,    # Class with model information\n",
    "                          σ,        # Initial condition\n",
    "                          tol=1e-8,\n",
    "                          max_iter=10000,\n",
    "                          verbose=True,\n",
    "                          print_skip=25):\n",
    "\n",
    "    # Set up loop\n",
    "    i = 0\n",
    "    error = tol + 1\n",
    "\n",
    "    while i < max_iter and error > tol:\n",
    "        σ_new = K(σ, model)\n",
    "        error = np.max(np.abs(σ - σ_new))\n",
    "        i += 1\n",
    "        if verbose and i % print_skip == 0:\n",
    "            print(f\"Error at iteration {i} is {error}.\")\n",
    "        σ = σ_new\n",
    "\n",
    "    if error > tol:\n",
    "        print(\"Failed to converge!\")\n",
    "    elif verbose:\n",
    "        print(f\"\\nConverged in {i} iterations.\")\n",
    "\n",
    "    return σ_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9611c49f",
   "metadata": {},
   "source": [
    "## II. All-in-One\n",
    "\n",
    "$$\\mathcal{L}_2(\\theta) = \\frac{1}{T} \\sum_{t=1}^{T} \\Big( f(s_t,\\epsilon_{1,t}|\\theta) f(s_t,\\epsilon_{2,t}|\\theta) \\Big) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18c8b947",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_434275/2789746850.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Root mean square error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mRMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape) \n",
    "\n",
    "def convert_sparse_matrix_to_sparse_tensor(X):\n",
    "    \"\"\"Convert a scipy sparse matrix to a tf sparse tensor.\"\"\"\n",
    "    coo = X.tocoo()\n",
    "    indices = np.mat([coo.row, coo.col]).transpose()\n",
    "    return tf.SparseTensor(indices, coo.data, coo.shape)\n",
    "\n",
    "# Root mean square error\n",
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        \n",
    "    def forward(self,yhat,y):\n",
    "        return torch.sqrt(self.mse(yhat,y))\n",
    "    \n",
    "# Gaussian quadrature rule\n",
    "# See: https://chaospy.readthedocs.io/en/master/api/chaospy.generate_quadrature.html\n",
    "def dist(order, distribution, rule = \"gaussian\", sp=True):\n",
    "    #order=int(n**(1/d))-1\n",
    "    x, w = chaospy.generate_quadrature(order, distribution, rule=(rule), sparse=sp)\n",
    "    return x, w\n",
    "\n",
    "def create_W_expanded_matrix(M, N):\n",
    "    \"\"\"\n",
    "    create a sparse matrix W_expanded with U repeate M times on the diagonal elements\n",
    "    where U is an upper triangular matrix with 0 on the diagonal and 1 on the other upper elements\n",
    "    W_expanded is a sparse torch matrix\n",
    "    \"\"\"\n",
    "    A_expanded = np.ones((N, N))\n",
    "    U = np.triu(A_expanded) # upper trianguler matrix of ones\n",
    "    np.fill_diagonal(U, 0) #fill diagonal with 0\n",
    "    U = sparse.csr_matrix(U) # convert to sparse\n",
    "    # Unity matrix of size (M*M)\n",
    "    B = sparse.csr_matrix(np.eye(M, M))\n",
    "    W_expanded = sparse_mx_to_torch_sparse_tensor(sparse.kron(B, U))\n",
    "    return W_expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de506c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncated_normal(mean, std, lower, upper, len_series):\n",
    "    draws = torch.empty(len_series, 1)\n",
    "    nn.init.trunc_normal_(draws, mean, std, lower, upper)\n",
    "    return draws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d6a37cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_shocks(e_distribution, μ_e, σ_e, trunc_low, trunc_high, len_series):\n",
    "    \"\"\"\n",
    "    Function to simulate shocks\n",
    "    \"\"\"\n",
    "    # Generate shocks\n",
    "    if e_distribution == \"Normal\":\n",
    "        e_shock_series = torch.normal(mean=0, std=σ_e, size=(len_series,)).unsqueeze(1)\n",
    "    elif e_distribution == \"T\":\n",
    "        m = torch.distributions.studentT.StudentT(torch.tensor([3.0]))\n",
    "        e_shock_series = m.sample([len_series]).squeeze(1)\n",
    "    elif e_distribution == \"Lognormal\":\n",
    "        # Standard Normal iid shocks\n",
    "        e_shock_series = torch.normal(mean=0, std=σ_e, size=(len_series,)).unsqueeze(1)\n",
    "        # Transform to lognormal\n",
    "        e_shock_series = torch.exp(μ_e + σ_e * e_shock_series)\n",
    "    elif e_distribution == \"Lognormal_2\":\n",
    "        # Standard Normal iid shocks\n",
    "        e_shock_series = torch.normal(mean=0, std=1.0, size=(len_series,)).unsqueeze(1)\n",
    "        # Transform to lognormal\n",
    "        e_shock_series = torch.exp(μ_e + σ_e * e_shock_series)\n",
    "    elif e_distribution == \"Trunc_Lognormal\":\n",
    "        # Trunc normal\n",
    "        e_shock_series = truncated_normal(μ_e, σ_e, trunc_low, trunc_high, len_series)\n",
    "        # Transform to lognormal\n",
    "        e_shock_series = torch.exp(e_shock_series)\n",
    "    elif e_distribution == \"Beta\":\n",
    "        m = torch.distributions.beta.Beta(torch.tensor([0.5]), torch.tensor([0.5]))\n",
    "        e_shock_series = torch.exp(m.sample((len_series,)))\n",
    "    else:\n",
    "        raise(f\"Distribution {e_distribution} unknown.\")\n",
    "    return e_shock_series\n",
    "\n",
    "def simulate_shocks(params, len_series):\n",
    "    return sim_shocks(params.e_distribution, params.μ_e, params.σ_e, params.trunc_low, params.trunc_high, len_series)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "086f10e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_states(params, len_series):\n",
    "    if params.x_distribution == \"Uniform\":\n",
    "        if params.use_Sobol_T == False:\n",
    "            x = ((params.x_low - params.x_high) * torch.rand(len_series) + params.x_high).unsqueeze(1)\n",
    "        else:\n",
    "            #Very slow if T is large\n",
    "            x = ((params.x_low - params.x_high) * params.soboleng.draw(len_series) + params.x_high)\n",
    "    elif params.x_distribution == \"Uniform_centered\":\n",
    "        x = ((params.x_low - params.x_high) * torch.rand(len_series) + params.x_high).unsqueeze(1)\n",
    "    elif params.x_distribution == \"Lognormal_2\":\n",
    "        # log(y_t) is Normal(self.α/(1 - self.α)*np.log(self.α*self.β), (self.σ_e**2 /(1 - self.α**2))\n",
    "        log_x = torch.normal(mean=params.mm, std=params.ss, size=(len_series,)).unsqueeze(1)\n",
    "        # Transform to lognormal\n",
    "        x = torch.exp(log_x)\n",
    "    elif params.x_distribution == \"Lognormal_3\":\n",
    "        # log(y_t) is Normal, with mean (self.α/(1 - self.α)*np.log(self.α*self.β)\n",
    "        # std dev is stored in params.ss_Lognormal_3\n",
    "        log_x = torch.normal(mean=params.mm, std=params.ss_Lognormal_3, size=(len_series,)).unsqueeze(1)\n",
    "        # Transform to lognormal\n",
    "        x = torch.exp(log_x)\n",
    "    elif params.x_distribution == \"Normal\":\n",
    "        x = torch.normal(mean=0, std=params.σ_x, size=(len_series,)).unsqueeze(1)\n",
    "    else:\n",
    "        raise(f\"Distribution {params.x_distribution} unknown.\")\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53492fb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Ξ_torch(model, params): # objective function if using all-in-one\n",
    "\n",
    "    # randomly drawing current states    \n",
    "    x = sim_states(params, params.T)\n",
    "    \n",
    "    # Draws 2 series of independent shocks\n",
    "    e1 = simulate_shocks(params, params.T)\n",
    "    e2 = simulate_shocks(params, params.T)\n",
    "    \n",
    "    # residuals for n random grid points under 2 realizations of shocks\n",
    "    R1 = Residuals_torch(model, params, x, e1)\n",
    "    R2 = Residuals_torch(model, params, x, e2)\n",
    "\n",
    "    # construct all-in-one expectation operator\n",
    "    R_squared = R1*R2\n",
    "    \n",
    "    # V1. give a summary of all the draws:\n",
    "    return torch.mean(R_squared)\n",
    "\n",
    "def Residuals_torch(model, params, y, e_r, version_resid = 6):\n",
    "    # consumption today\n",
    "    c = model(y)\n",
    "    \n",
    "    # implies some investment\n",
    "    investment = y - c\n",
    "    # investment scaled by shock\n",
    "    state_tomorrow = params.f(investment)*e_r\n",
    "    \n",
    "    # consumption tomorrow\n",
    "    # c_tomorrow = c_share_tomorrow*state_tomorrow \n",
    "    c_tomorrow = model(state_tomorrow)\n",
    "    \n",
    "    if version_resid < 5:\n",
    "        #LHS\n",
    "        LHS = params.u_prime(c)\n",
    "        #RHS\n",
    "        vals = params.u_prime(c_tomorrow)*params.f_prime(investment)*e_r\n",
    "        RHS = params.β * vals\n",
    "\n",
    "    if version_resid == 1: #No rescaling\n",
    "        R = (RHS - LHS)\n",
    "    elif version_resid == 2: #V2: good scaling. Default choice.\n",
    "        R = (RHS - LHS)/(0.5*RHS + 0.5*LHS)\n",
    "    elif version_resid == 3: #Other rescaling\n",
    "        R = 1 - (LHS/RHS)\n",
    "    elif version_resid == 4: #Other rescaling\n",
    "        R = 1 - (RHS/LHS)\n",
    "    elif version_resid == 5: #Simplify\n",
    "        R = 1 - params.α*params.β*(y/investment)\n",
    "    elif version_resid == 6: #logs\n",
    "        R = torch.log(torch.tensor(params.α*params.β)) + torch.log(c) - torch.log(c_tomorrow) + (params.α-1)*torch.log(investment) + torch.log(e_r)\n",
    "    return R\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ee95036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def approx_linear(y, α, β, gamma):\n",
    "    # Almost the true solution\n",
    "    #return (1 - α*β + gamma*torch.log(y))*y\n",
    "     # Almost the true solution, correct for the mean error\n",
    "    return (1 - α*β + gamma*torch.log(y))*y - (gamma/(1 - α*β))*(α/(1 - α))*torch.log(α*β)\n",
    "    #return (1 - α*β + gamma*y)*y\n",
    "    #return torch.exp(torch.log(torch.tensor([1 - α*β])) + ((1 - α*β + gamma)/(1 - α*β))*torch.log(y))\n",
    "    \n",
    "def Residuals_torch_linear(model, params, y, e_r, gamma, version_resid = 6):\n",
    "    \"\"\"\n",
    "    Use an approximation of the Neural net.\n",
    "    Good when close to final solution\n",
    "    \"\"\"\n",
    "    # y: current state\n",
    "    # e: shock\n",
    "    # consumption today\n",
    "    c = approx_linear(y, params.α, params.β, gamma)\n",
    "    LHS = params.u_prime(c)\n",
    "    \n",
    "    # implies some investment\n",
    "    investment = y - c\n",
    "    \n",
    "    # investment scaled by shock\n",
    "    state_tomorrow = params.f(investment)*e_r\n",
    "    #state_tomorrow =  (investment**params.α)*e_r\n",
    "    \n",
    "    # consumption tomorrow\n",
    "    c_tomorrow = approx_linear(state_tomorrow, params.α, params.β, gamma)\n",
    "    \n",
    "    if version_resid == 1: #No rescaling\n",
    "        R = (RHS - LHS)\n",
    "    elif version_resid == 2: #V2: good scaling. Default choice.\n",
    "        R = (RHS - LHS)/(0.5*RHS + 0.5*LHS)\n",
    "    elif version_resid == 3: #Other rescaling\n",
    "        R = 1 - (LHS/RHS)\n",
    "    elif version_resid == 4: #Other rescaling\n",
    "        R = 1 - (RHS/LHS)\n",
    "    elif version_resid == 5: #Simplify\n",
    "        R = 1 - params.α*params.β*(y/investment)\n",
    "    elif version_resid == 6: #logs\n",
    "        R = torch.log(torch.tensor(params.α*params.β)) + torch.log(c) - torch.log(c_tomorrow) + (params.α-1)*torch.log(investment) + torch.log(e_r)\n",
    "    return R\n",
    "\n",
    "\n",
    "def sim_states_linear(params, len_series, gamma):\n",
    "    # Simulate approximate ergodic\n",
    "    if params.x_distribution == \"Lognormal_2\":\n",
    "        #mean_ = ((params.α*params.β)/(params.β*(1 - params.α) + gamma))*torch.log(torch.tensor(params.α*params.β))\n",
    "        #var_ = (params.σ_e**2 * params.β**2)/(params.β**2 - (params.α*params.β - gamma)**2)\n",
    "        mean_ = (params.α/(1 - params.α))*torch.log(torch.tensor(params.α*params.β))\n",
    "        var_ = (params.σ_e**2)/(1 - params.α**2)\n",
    "        std_ = torch.sqrt(torch.tensor([var_]))\n",
    "        # log(y_t) is Normal\n",
    "        log_x = torch.normal(mean=mean_.item(), std=std_.item(), size=(len_series,)).unsqueeze(1)\n",
    "        # return y_t\n",
    "        x = torch.exp(log_x)\n",
    "    elif params.x_distribution == \"Lognormal_3\":\n",
    "        # std dev is stored in params.ss_Lognormal_3\n",
    "        log_x = torch.normal(mean=params.mm, std=params.ss_Lognormal_3, size=(len_series,)).unsqueeze(1)\n",
    "        # Transform to lognormal.\n",
    "        x = torch.exp(log_x)\n",
    "    return x\n",
    "\n",
    "def calculate_variance_gaussian_linear(params, model, nb_draws, grid_M, grid_N, gamma = 1e-6):\n",
    "    \"\"\"\n",
    "    Calculate variance of the loss when joint gaussian assumption holds\n",
    "    Use approximate solution\n",
    "    \"\"\"\n",
    "    grid_T = torch.tensor(grid_M*grid_N/2)\n",
    "    grid_N = torch.tensor(grid_N)\n",
    "    \n",
    "    # Calculate variance and covariance\n",
    "    with torch.no_grad(): \n",
    "        # State\n",
    "        ## Draw y_t\n",
    "        #x = sim_states_linear(params, nb_draws, gamma)\n",
    "        x = sim_states(params, nb_draws)\n",
    "        \n",
    "        e1 = simulate_shocks(params, nb_draws)\n",
    "        e2 = simulate_shocks(params, nb_draws)\n",
    "                \n",
    "        # residuals for n random grid points under 2 realizations of shocks\n",
    "        R1 = Residuals_torch_linear(model_bcMC, params, x, e1, gamma)\n",
    "        R2 = Residuals_torch_linear(model_bcMC, params, x, e2, gamma)\n",
    "        \n",
    "        # Construct combinations\n",
    "        # mean\n",
    "        mean_val = 0.5*torch.mean(R1) + 0.5*torch.mean(R2)\n",
    "        ## Var\n",
    "        var_R1 = torch.var(R1)\n",
    "        var_R2 = torch.var(R2)\n",
    "        var_val = 0.5*var_R1 + 0.5*var_R2\n",
    "        \n",
    "        ## Cov\n",
    "        cov_val = torch.cov(torch.column_stack((R1, R2)).T)[0,1]\n",
    "        \n",
    "        var_L = (1/(grid_T*(grid_N - 1)))*((grid_N**2 - 3*grid_N + 3)*(cov_val**2) + (2*(grid_N - 2)*cov_val + var_val)*var_val + 2*(grid_N - 1)*(var_val + (grid_N - 1)*cov_val)*(mean_val**2))\n",
    "        \n",
    "        # Break the variance of the loss into different elements\n",
    "        # increasing with N\n",
    "        b1 = (1/grid_T)*((grid_N **2 - 3*grid_N + 3)/(grid_N - 1))*(cov_val**2)\n",
    "        b2 = (1/grid_T)*(2*(grid_N - 2)/(grid_N - 1))*cov_val*var_val\n",
    "        b3 =(1/grid_T)*(2*(grid_N - 1))*(cov_val*mean_val**2)\n",
    "        # decreasing in N\n",
    "        b4 = (1/grid_T)*(var_val**2)/(grid_N - 1)\n",
    "        # constant in N\n",
    "        b5 = (1/grid_T)*2*var_val*mean_val**2\n",
    "\n",
    "        return var_L, var_val, cov_val, b1, b2, b3, b4, b5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62e2ff3d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_24440/1174522944.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0msim_true_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mα\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mβ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me_distribution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mμ_e\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mσ_e\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrunc_low\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrunc_high\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen_series\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1e-6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmult_std\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \"\"\"\n\u001b[1;32m      3\u001b[0m     \u001b[0mFunction\u001b[0m \u001b[0mto\u001b[0m \u001b[0msimulate\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtrue\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \"\"\"\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "def sim_true_model(α, β, func, e_distribution, μ_e, σ_e, trunc_low, trunc_high, len_series, tol=torch.tensor([1e-6]), mult_std = 2.0):\n",
    "    \"\"\"\n",
    "    Function to simulate the true model\n",
    "    \"\"\"\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        y_series = torch.zeros(len_series)\n",
    "        c_series = torch.zeros(len_series)\n",
    "        k_series = torch.zeros(len_series)\n",
    "\n",
    "        # Initialization NS SS:\n",
    "        y_series[0] = (α*β)**(α/(1 - α))\n",
    "        c_series[0] = (1.0 - α*β)*y_series[0]\n",
    "        k_series[0] = α*β*y_series[0]\n",
    "                \n",
    "        # Generate shocks\n",
    "        e_shock_series = sim_shocks(e_distribution, μ_e, σ_e, trunc_low, trunc_high, len_series).squeeze(1) #simulate_shocks(params, len_series)\n",
    "    \n",
    "        for t in range(1, len_series):\n",
    "            c_series[t] = (1.0 - α*β)*y_series[t-1]\n",
    "            k_series[t] = α*β*y_series[t-1]\n",
    "            y_series[t] = (k_series[t]**α)*e_shock_series[t] #func(k_series[t])*e_shock_series[t]\n",
    "\n",
    "    df_series = pd.DataFrame({'y': y_series, 'c': c_series, 'k': k_series, 'e': e_shock_series})\n",
    "    # calculate CI:\n",
    "    series_names = ['y', 'c', 'k', 'e']\n",
    "    list_mean = []\n",
    "    list_std = []\n",
    "    list_CI_lower = []\n",
    "    list_CI_upper = []\n",
    "    for var in series_names:\n",
    "        list_mean.append(np.mean(df_series[var]))\n",
    "        list_std.append(np.std(df_series[var]))\n",
    "        #CI\n",
    "        logged = np.log(df_series[var])\n",
    "        mean_logged = np.mean(logged)\n",
    "        std_logged = np.std(logged)\n",
    "        list_CI_lower.append(np.exp(mean_logged - mult_std*std_logged))\n",
    "        list_CI_upper.append(np.exp(mean_logged + mult_std*std_logged))\n",
    "\n",
    "    return df_series, series_names, list_mean, list_std, list_CI_lower, list_CI_upper\n",
    "\n",
    "def simulate_true_model(params, len_series =10000):\n",
    "    return sim_true_model(params.α, params.β, params.f, params.e_distribution, params.μ_e, params.σ_e,  params.trunc_low, params.trunc_high, len_series) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84919be1",
   "metadata": {},
   "source": [
    "## III. bc-MC operator\n",
    "\n",
    "* Implement the developed formula\n",
    "\n",
    "$$ \\frac{1}{M} \\frac{2}{(N)(N-1)} \\sum_{m=1}^{M} \\sum_{1\\leq i < j}^{n} f(s_m, \\epsilon_{m}^{(i)})f(s_m, \\epsilon_{m}^{(j)})  $$\n",
    "\n",
    "* This formula can be vectorized as follows:\n",
    "\n",
    "\n",
    "$$ f' \\Big(I_N \\otimes U\\Big). f $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "950d4e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ξ_torch_MC(model, params): # objective function\n",
    "    # randomly drawing current states  \n",
    "    # M draws\n",
    "    x = sim_states(params, params.M)\n",
    "    \n",
    "    # repeat elements N times\n",
    "    x_repeated = x.repeat_interleave(params.N).unsqueeze(1) #MN*1 matrix\n",
    "    \n",
    "    # N draws for each value of x. MN draws\n",
    "    e_shock = simulate_shocks(params, params.MN)\n",
    "    \n",
    "    # residuals for n random grid points under 2 realizations of shocks\n",
    "    R1 = Residuals_torch(model, params, x_repeated, e_shock).squeeze(1)\n",
    "    R_squared = torch.mean((2/((params.M)*(params.N)*(params.N - 1)))*torch.matmul(R1.unsqueeze(1).t(), torch.matmul(params.W_expanded, R1.unsqueeze(1))))\n",
    "   \n",
    "    return R_squared \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f7d5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_residual(model, params): # objective function\n",
    "    \"\"\"\n",
    "    Function to draw from residual function\n",
    "    \"\"\"\n",
    "    with torch.no_grad(): \n",
    "        x = sim_states(params, params.M)\n",
    "\n",
    "        # repeat elements N times\n",
    "        x_repeated = x.repeat_interleave(params.N).unsqueeze(1) #MN*1 matrix\n",
    "\n",
    "        # N draws for each value of x. MN draws\n",
    "        e_shock = simulate_shocks(params, params.MN)\n",
    "\n",
    "        # residuals for n random grid points under 2 realizations of shocks\n",
    "        R = Residuals_torch(model, params, x_repeated, e_shock).squeeze(1)\n",
    "    \n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2175b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy_pytorch_frozen_Gaussian(model, params, debug = False, distance_f = torch.abs):\n",
    "    \"\"\"\n",
    "    Function to evaluate the accuracy using Monte Carlo for the expectation\n",
    "    Use a pre-determined series of shocks to approximate the expectations\n",
    "    \"\"\"\n",
    "    with torch.no_grad(): \n",
    "        # (M,1)\n",
    "        # consumption today\n",
    "        if debug == True:\n",
    "            c = params.true_function(params.xvec_test_torch)\n",
    "        else:\n",
    "            c = model(params.xvec_test_torch) \n",
    "        u_prime_c = params.u_prime(c)\n",
    "        #print(c.shape)\n",
    "        #print(LHS.shape)\n",
    "\n",
    "        # implies some investment\n",
    "        # (M,1)\n",
    "        investment = params.xvec_test_torch - c\n",
    "        #print(investment.shape)\n",
    "\n",
    "        # repeat values\n",
    "        # shape (MN, 1)\n",
    "        c_repeated = c.repeat_interleave(params.n_nodes).unsqueeze(1)\n",
    "        u_prime_c_repeated  = u_prime_c.repeat_interleave(params.n_nodes).unsqueeze(1)\n",
    "        investment_repeated = investment.repeat_interleave(params.n_nodes).unsqueeze(1)\n",
    "        #print(c_repeated.shape)\n",
    "        #print(investment_repeated.shape)\n",
    "\n",
    "        # shape (MN, 1)\n",
    "        # repeat nodes\n",
    "        nodes_torch_repeated = params.nodes_torch.repeat(len(c)).unsqueeze(1)\n",
    "\n",
    "        # investment scaled by shock (MN, 1)\n",
    "        state_tomorrow = (params.f(investment_repeated)*nodes_torch_repeated).float()\n",
    "        #print(state_tomorrow.shape)\n",
    "\n",
    "        if debug == True:\n",
    "            c_tomorrow = params.true_function(state_tomorrow)\n",
    "        else:\n",
    "            c_tomorrow = model(state_tomorrow)\n",
    "        #print(c_tomorrow.shape)\n",
    "        vals = (params.u_prime(c_tomorrow)/u_prime_c_repeated)*params.α*(state_tomorrow/investment_repeated)\n",
    "        #print(vals.shape)\n",
    "\n",
    "        # (100, 1)\n",
    "        expect_t = params.β*torch.sparse.mm(params.W_gaussian, vals).squeeze(1)\n",
    "        #print(expect_t.shape)\n",
    "        #print(torch.mean(expect_t))\n",
    "        euler_resid = distance_f(1.0 - (1.0/expect_t))\n",
    "\n",
    "        #print(torch.mean(euler_resid)) \n",
    "        # Euler error: \n",
    "        return euler_resid.numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62305d2b",
   "metadata": {},
   "source": [
    "## Optimal choice of training parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb084d7",
   "metadata": {},
   "source": [
    "### Monte Carlo estimator: optimal choice of M and N\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743fc8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grad_norm(parameters, norm_type=2.0):\n",
    "    \"\"\"\n",
    "    Compute norm over gradients of model parameters.\n",
    "\n",
    "    :param parameters:\n",
    "        the model parameters for gradient norm calculation. Iterable of\n",
    "        Tensors or single Tensor\n",
    "    :param norm_type:\n",
    "        type of p-norm to use\n",
    "\n",
    "    :returns:\n",
    "        the computed gradient norm\n",
    "    \"\"\"\n",
    "    if isinstance(parameters, torch.Tensor):\n",
    "        parameters = [parameters]\n",
    "    parameters = [p for p in parameters if p is not None and p.grad is not None]\n",
    "    total_norm = 0\n",
    "    for p in parameters:\n",
    "        param_norm = p.grad.data.norm(norm_type)\n",
    "        total_norm += param_norm.item() ** norm_type\n",
    "    return total_norm ** (1.0 / norm_type) \n",
    "\n",
    "# Calculate the variance of the loss for several choices of M and N\n",
    "# Start with a fresh model. Then train it for some iterations.\n",
    "# Then calculate variance of loss\n",
    "def calculate_variance_loss(params, nb_epoch_opt_M_N, nb_rep, \n",
    "                            nb_draws_loss, grid_M, grid_N,\n",
    "                            norm_chosen = 2.0, freq_accuracy=1000, \n",
    "                            calculate_variance_gradient = False,\n",
    "                            initial_guess = [1.0, 1.0]):\n",
    "    # To store results\n",
    "    df_variance_loss = pd.DataFrame()\n",
    "    print(\"Norm chosen: {}\".format(norm_chosen))\n",
    "    \n",
    "    for k in range(0, nb_rep):\n",
    "        #-----------------------------------------\n",
    "        # A. Train the model for nb_epoch_opt_M_N\n",
    "        #-----------------------------------------\n",
    "        # Initialize a network\n",
    "        print(\"Rep {} / {}. Training a model for {} Iterations\".format(int(k), nb_rep, nb_epoch_opt_M_N))\n",
    "        model_opt_M_N = NeuralNetwork().to(device) # New model\n",
    "        \n",
    "        # Set initial value\n",
    "        set_initial_values(model_opt_M_N, initial_guess[0], initial_guess[1])\n",
    "        \n",
    "        # Training mode\n",
    "        model_opt_M_N.train()\n",
    "\n",
    "        if params.optimizer == \"Adam\":\n",
    "            optimizer = torch.optim.Adam(model_opt_M_N.parameters(), lr=params.lr, eps=1e-07, betas=(0.9, 0.999)) \n",
    "        elif params.optimizer == \"SGD\":\n",
    "            optimizer = torch.optim.SGD(model_opt_M_N.parameters(), params.lr)\n",
    "        elif params.optimizer == \"SWA\":\n",
    "            base_opt = torch.optim.Adam(model_opt_M_N.parameters(), lr=params.lr, eps=1e-07, betas=(0.9, 0.999)) \n",
    "            optimizer = SWA(base_opt, swa_start=params.swa_start, swa_freq=params.swa_freq, swa_lr=params.lr)\n",
    "        else:\n",
    "            raise(\"optimizer unknown\")\n",
    "\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=params.freq_gamma)\n",
    "        list_perc_abs_error_opt_M_N = [] #store abs value percentage error\n",
    "        list_perc_abs_error_opt_M_N_i = []\n",
    "        loss_epochs_opt_M_N = torch.zeros(nb_epoch_opt_M_N)\n",
    "\n",
    "        for i in range(0, nb_epoch_opt_M_N):\n",
    "            \n",
    "            optimizer.zero_grad() #clear gradient\n",
    "            \n",
    "            loss = Ξ_torch_MC(model_opt_M_N, params) #loss \n",
    "            loss_epochs_opt_M_N[[i]] = float(loss.item())\n",
    "\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if i % freq_accuracy == 0: #Monitor the predictive power\n",
    "                # Define the grid\n",
    "                with torch.no_grad():\n",
    "                    xvec = params.xvec_test_torch #add a dimension\n",
    "                    y = model_opt_M_N(xvec) \n",
    "                xvec = xvec.detach().numpy()\n",
    "                y = y.detach().numpy()\n",
    "                perc_abs_error = 100*(np.abs((y - params.true_function(xvec))/params.true_function(xvec)))\n",
    "                list_perc_abs_error_opt_M_N.append(np.median(perc_abs_error))\n",
    "                list_perc_abs_error_opt_M_N_i.append(i)\n",
    "            if i % 1000 == 0:\n",
    "                loss, current = float(loss.item()), i\n",
    "                print(f\"loss: {loss:>7f}, median percentage euler error {list_perc_abs_error_opt_M_N[-1]:>7f}, [{current:>5d}/{nb_epoch_opt_M_N:>5d}]\")\n",
    "            if (i % params.freq_scheduler == 0) & (i != 0) & (params.use_scheduler == True):\n",
    "                scheduler.step()\n",
    "                print(\"i : {}. Decreasing learning rate: {}\".format(i, scheduler.get_last_lr()))\n",
    "\n",
    "        if params.optimizer == \"SWA\":\n",
    "            optimizer.swap_swa_sgd()\n",
    "        #--------------------------------------\n",
    "        # B. Calculate the variance of the loss\n",
    "        #--------------------------------------\n",
    "        print(\"Rep {} / {}. Calculting variance loss for {} Iterations\".format(int(k), nb_rep, nb_draws_loss))\n",
    "    \n",
    "        model_opt_M_N.eval()\n",
    "        var_loss = torch.zeros(len(grid_N))\n",
    "        std_loss = torch.zeros(len(grid_N))\n",
    "        mean_loss = torch.zeros(len(grid_N))\n",
    "\n",
    "        # Loop over choice of N and M\n",
    "        for (ind, (N_chosen, M_chosen)) in enumerate(zip(grid_N.astype('int'), grid_M.astype('int'))):\n",
    "            # Change M and N\n",
    "            params_local = MyParams(N_chosen, M_chosen, params.lr, \n",
    "                                    params.pre_train_model, params.nb_epochs,\n",
    "                                    params.order_gauss, params.σ_e, params.use_Sobol, \n",
    "                                    params.optimizer)\n",
    "\n",
    "            with torch.no_grad():        \n",
    "                Xms = torch.zeros(nb_draws_loss)\n",
    "                # Loop over realizations of loss function\n",
    "                for (j_index, j) in enumerate(range(0, nb_draws_loss)):\n",
    "                    Xms[j] = Ξ_torch_MC(model_opt_M_N, params_local)\n",
    "                # Calculate mean and variance:\n",
    "                var_loss[ind] = torch.var(Xms)\n",
    "                std_loss[ind] = torch.sqrt(var_loss[ind])\n",
    "                mean_loss[ind] = torch.mean(Xms)\n",
    "        #-----------------------------------------------\n",
    "        # C. Calculate variance of the norm the gradient\n",
    "        #-----------------------------------------------\n",
    "        print(\"Rep {} / {}. Calculting variance norm gradient for {} Iterations\".format(int(k), nb_rep, nb_draws_loss))\n",
    "        var_gradient_loss = torch.zeros(len(grid_N )) #variance \n",
    "        std_gradient_loss = torch.zeros(len(grid_N )) #standard deviation\n",
    "        mean_gradient_loss = torch.zeros(len(grid_N ))\n",
    "        \n",
    "        if calculate_variance_gradient == True:\n",
    "            for (ind, (N_chosen, M_chosen)) in enumerate(zip(grid_N.astype('int'), grid_M.astype('int'))):\n",
    "                \n",
    "                params_local = MyParams(N_chosen, M_chosen, params.lr, \n",
    "                        params.pre_train_model, params.nb_epochs,\n",
    "                        params.order_gauss, params.σ_e, params.use_Sobol, \n",
    "                        params.optimizer)\n",
    "    \n",
    "                Xms = torch.zeros(nb_draws_loss)\n",
    "                # Loop over draws \n",
    "                for (j_index, j) in enumerate(range(0, nb_draws_loss)):\n",
    "                    optimizer.zero_grad() # clear gradient\n",
    "                    loss = Ξ_torch_MC(model_opt_M_N, params_local)\n",
    "                    loss.backward() #calculate gradient\n",
    "                    # Store the norm of the gradient\n",
    "                    total_norm = compute_grad_norm(model_opt_M_N.parameters(), norm_type=norm_chosen)\n",
    "                    Xms[j] = total_norm\n",
    "                var_gradient_loss[ind] = torch.var(Xms)\n",
    "                std_gradient_loss[ind] = torch.sqrt(var_gradient_loss[ind])\n",
    "                mean_gradient_loss[ind] = torch.mean(Xms)\n",
    "\n",
    "        # Save to dataframe\n",
    "        if df_variance_loss.empty == True:\n",
    "            df_variance_loss = pd.DataFrame({'N': grid_N,\n",
    "                                  'M': grid_M,\n",
    "                                  'var_loss': var_loss,\n",
    "                                  'std_loss': std_loss,\n",
    "                                  'mean_loss': mean_loss,\n",
    "                                  'var_gradient_loss': var_gradient_loss,\n",
    "                                  'std_gradient_loss': std_gradient_loss,\n",
    "                                  'mean_gradient_loss': mean_gradient_loss,\n",
    "                                  'nb_rep': k})\n",
    "        else:\n",
    "            df_variance_loss_bis = pd.DataFrame({'N': grid_N,\n",
    "                                  'M': grid_M,\n",
    "                                  'var_loss': var_loss,\n",
    "                                  'std_loss': std_loss,\n",
    "                                  'mean_loss': mean_loss,\n",
    "                                  'var_gradient_loss': var_gradient_loss,\n",
    "                                  'std_gradient_loss': std_gradient_loss,\n",
    "                                  'mean_gradient_loss': mean_gradient_loss,\n",
    "                                  'nb_rep': k})\n",
    "            df_variance_loss = pd.concat([df_variance_loss, df_variance_loss_bis])\n",
    "        \n",
    "    # Replace NAN by large values. Need to penalize non convergence of gradient descent.\n",
    "    list_cols = [\"var_loss\", \"std_loss\", \"mean_loss\", \n",
    "                \"std_gradient_loss\", \"var_gradient_loss\", \"mean_gradient_loss\"]\n",
    "    \n",
    "    for col in list_cols:\n",
    "        df_variance_loss[col] = df_variance_loss[col].fillna(np.nanmax(df_variance_loss[col]))\n",
    "    \n",
    "    # Stats on df_var\n",
    "    # Median values\n",
    "    df_variance_loss_median = df_variance_loss.groupby('M').median().reset_index()\n",
    "\n",
    "    for col in list_cols:\n",
    "        df_variance_loss_median[\"min_\" + col] = df_variance_loss.groupby('M')[col].min().reset_index()[col]                     \n",
    "        df_variance_loss_median[\"max_\" + col] = df_variance_loss.groupby('M')[col].max().reset_index()[col]\n",
    "        df_variance_loss_median[\"std_\" + col] = df_variance_loss.groupby('M')[col].std().reset_index()[col]\n",
    "        for qq in [10, 25, 50, 75, 90]:\n",
    "            df_variance_loss_median[\"P\" + str(qq) + \"_\" + col] = df_variance_loss.groupby('M')[col].quantile(qq/100).reset_index()[col]\n",
    "    \n",
    "\n",
    "    return df_variance_loss, df_variance_loss_median\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9859c496",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Given a model, calculate the current variance of the loss\n",
    "def calculate_variance_loss_model(model, params, nb_draws_loss):\n",
    "    model.eval() #eval mode\n",
    "    with torch.no_grad():        \n",
    "        Xms = torch.zeros(nb_draws_loss)\n",
    "        # Loop over realizations of loss function\n",
    "        for (j_index, j) in enumerate(range(0, nb_draws_loss)):\n",
    "            Xms[j] = Ξ_torch_MC(model, params)\n",
    "        # Calculate mean and variance:\n",
    "        var_loss = torch.var(Xms)\n",
    "        mean_loss = torch.mean(Xms)\n",
    "    model.train() #train mode\n",
    "    return var_loss, mean_loss\n",
    "\n",
    "#Given a model, calculate the current variance of the loss\n",
    "def calculate_variance_loss_model_grid(model, params, nb_draws_loss, grid_N, grid_M):\n",
    "    var_loss = torch.zeros(len(grid_N)) #to store results\n",
    "    # Loop over choice of N and M\n",
    "    for (ind, (N_chosen, M_chosen)) in enumerate(zip(grid_N, grid_M)):\n",
    "        # Change M and N\n",
    "        params_local = MyParams(int(N_chosen), int(M_chosen), params.lr, params.pre_train_model, \n",
    "                      params.nb_epochs, params.order_gauss, params.σ_e, params.use_Sobol, params.optimizer,\n",
    "                      params.α, params.β, params.e_distribution, params.x_distribution)\n",
    "\n",
    "        var, mean = calculate_variance_loss_model(model, params_local, nb_draws_loss)\n",
    "        var_loss[ind] = var\n",
    "    return var_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b0667f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimizer(model, optimizer_name, lr, momentum):\n",
    "    \"\"\"\n",
    "    Function to create an optimizer\n",
    "    \"\"\"\n",
    "    if optimizer_name == \"Adam\":\n",
    "        #optimizer = torch.optim.Adam(model.parameters(), lr=lr, eps=1e-07, betas=(0.9, 0.999)) \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr) \n",
    "    elif optimizer_name == \"SGD\":\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr)\n",
    "    elif optimizer_name == \"SGD-momentum\":\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr, momentum)\n",
    "    elif optimizer_name == \"Adadelta\":\n",
    "        optimizer = torch.optim.Adadelta(model.parameters(), lr)\n",
    "    elif optimizer_name == \"RMSprop\":\n",
    "        optimizer = torch.optim.RMSprop(model.parameters(), lr)\n",
    "    #elif optimizer_name == \"LBFGS\":\n",
    "    #    torch.optim.LBFGS(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        raise NameError(f\"optimizer {optimizer_name} unknown\")\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def calculate_effective_lr(optimizer, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "    # default pytroch params for Adam betas=(0.9, 0.999), eps=1e-08\n",
    "    \"\"\"\n",
    "    Function to calculate the effective learning rate used in Adam update\n",
    "    \"\"\"\n",
    "    effective_lr_list = []\n",
    "    for param_group in optimizer.param_groups:\n",
    "        for param in param_group['params']:\n",
    "            if param.grad is not None:\n",
    "                state = optimizer.state[param]\n",
    "                if 'step' in state and state['step'] > 0:\n",
    "                    m_t = state['exp_avg']\n",
    "                    v_t = state['exp_avg_sq']\n",
    "                    step_size = param_group['lr'] * np.sqrt(1 - beta2 ** state['step']) / (1 - beta1 ** state['step'])\n",
    "                    effective_lr = step_size / (torch.sqrt(v_t) + epsilon)\n",
    "                    effective_lr_list.append(effective_lr.mean().item())\n",
    "    return effective_lr_list\n",
    "\n",
    "def set_initial_values(model, w, b):\n",
    "    \"\"\"\n",
    "    Function to analyse the weigth and bias to certain values\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'linear_relu_stack.0.weight' in name:\n",
    "                param.copy_(torch.tensor([w]))\n",
    "            elif 'linear_relu_stack.0.bias' in name:\n",
    "                param.copy_(torch.tensor([b]))\n",
    "                \n",
    "def show_initial_values(model):\n",
    "    \"\"\"\n",
    "    Function to analyse the weigth and bias to certain values\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'linear_relu_stack.0.weight' in name:\n",
    "                print(print(param))\n",
    "            elif 'linear_relu_stack.0.bias' in name:\n",
    "                print(print(param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05821e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve a model several times, holding initial parameters constant\n",
    "def calculate_several_runs(params, nb_rep, freq_loss = 1, initial_guess = [1.0, 1.0]):\n",
    "    #initialization of lists\n",
    "    list_elapsed_time = []\n",
    "    list_M = []\n",
    "    list_N = []\n",
    "    list_list_losses = [] \n",
    "    list_list_perc_abs_error_MC = [] \n",
    "    list_list_perc_abs_error_MC_i = [] \n",
    "    list_list_perc_abs_error_MC_loss = [] \n",
    "    list_lr = []\n",
    "    list_index_rep = []\n",
    "    list_list_beta = [] #store coefficients\n",
    "    list_list_abs_perc_dev_bias = [] #absolute percentage deviation from true bias \n",
    "    list_list_abs_perc_dev_weight = [] #absolute percentage deviation from true weight\n",
    "            \n",
    "    # A.\n",
    "    # Run seral times\n",
    "    for k in range(0, nb_rep):\n",
    "            print(\"Training model : {} / {}\".format(int(k), nb_rep))\n",
    "            list_index_rep.append(k)\n",
    "            list_M.append(params.M)\n",
    "            list_N.append(params.N)\n",
    "            list_lr.append(params.lr)\n",
    "\n",
    "            #-----------------------------------------------------\n",
    "            # Train a model for some periods\n",
    "            # Then look at the expected variance of the gradient\n",
    "            #-----------------------------------------------------\n",
    "            # Initialize a network\n",
    "            model_MC = NeuralNetwork().to(device) \n",
    "\n",
    "            # Set initial value\n",
    "            set_initial_values(model_MC, initial_guess[0], initial_guess[1])\n",
    "        \n",
    "            if params.optimizer == \"Adam\":\n",
    "                optimizer_MC = torch.optim.Adam(model_MC.parameters(), lr=params.lr, eps=1e-07, betas=(0.9, 0.999)) \n",
    "            elif params.optimizer == \"SGD\":\n",
    "                optimizer_MC = torch.optim.SGD(model_MC.parameters(), params.lr)\n",
    "            elif params.optimizer == \"SWA\":\n",
    "                base_opt_MC = torch.optim.Adam(model_MC.parameters(), lr=params.lr, eps=1e-07, betas=(0.9, 0.999)) \n",
    "                optimizer_MC = SWA(base_opt_MC, swa_start=params.swa_start, swa_freq=params.swa_freq, swa_lr=params.lr)\n",
    "            else:\n",
    "                raise(\"optimizer unknown\")\n",
    "\n",
    "            scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer_MC, gamma=params.freq_gamma)\n",
    "            loss_epochs_MC = torch.zeros(params.nb_epochs)\n",
    "            list_perc_abs_error_MC = [] #store abs value percentage error\n",
    "            list_perc_abs_error_MC_i = [] #store index i\n",
    "            list_perc_abs_error_MC_loss = [] #store loss\n",
    "            list_beta = [] #store coefficients\n",
    "            list_abs_perc_dev_bias = [] #np.zeros(params.nb_epochs) #absolute percentage deviation from true bias \n",
    "            list_abs_perc_dev_weight = [] #np.zeros(params.nb_epochs) #absolute percentage deviation from true weight\n",
    "            \n",
    "            for i in range(0, params.nb_epochs):\n",
    "                \n",
    "                optimizer_MC.zero_grad() #clear gradient\n",
    "                \n",
    "                loss = Ξ_torch_MC(model_MC, params) #calculate loss\n",
    "                loss_epochs_MC[[i]] = float(loss.item())\n",
    "                \n",
    "                # Store coefficients\n",
    "                if i % freq_loss == 0:\n",
    "                    with torch.no_grad():\n",
    "                        # Extract weight and bias\n",
    "                        b_current = np.array([k.item() for k in model_MC.parameters()])\n",
    "                        b_current_ordered = np.array((b_current[1], b_current[0])) #reorder (bias, weight)\n",
    "                    list_beta.append(b_current_ordered)\n",
    "\n",
    "                    # Calculate the absolute percentage deviation from true value:\n",
    "                    ## Bias: b_current_ordered[0]\n",
    "                    with torch.no_grad():\n",
    "                        dev_b = 100*(np.abs((b_current_ordered[0] - params.true_bias)/params.true_bias))\n",
    "                        #list_abs_perc_dev_bias[i] = dev_b #absolute percentage deviation from true bias \n",
    "                        list_abs_perc_dev_bias.append(dev_b[0])\n",
    "                        ## Cannot divide by 0\n",
    "                        dev_w = 100*(np.abs((b_current_ordered[1] - params.true_weight)))\n",
    "                        #list_abs_perc_dev_weight[i] = dev_w #absolute percentage deviation from true weight\n",
    "                        list_abs_perc_dev_weight.append(dev_w)\n",
    "                        \n",
    "                # Backpropagation\n",
    "                loss.backward()\n",
    "                \n",
    "                # Gradient descent step\n",
    "                optimizer_MC.step()\n",
    "\n",
    "                if i % freq_loss == 0: #Monitor the predictive power\n",
    "                    # Define the grid\n",
    "                    with torch.no_grad():\n",
    "                        xvec = params.xvec_test_torch #add a dimension\n",
    "                        y_MC = model_MC(xvec)\n",
    "                    xvec = xvec.detach().numpy()\n",
    "                    y_MC = y_MC.detach().numpy()\n",
    "                    perc_abs_error_MC = 100*(np.abs((y_MC - params.true_function(xvec))/params.true_function(xvec)))\n",
    "                    list_perc_abs_error_MC.append(np.median(perc_abs_error_MC))\n",
    "                    list_perc_abs_error_MC_i.append(i)\n",
    "                    list_perc_abs_error_MC_loss.append(float(loss.item()))\n",
    "                if i % 1000 == 0:\n",
    "                    loss, current = float(loss.item()), i\n",
    "                    print(f\"loss: {loss:>7f} [{current:>5d}/{params.nb_epochs:>5d}]\")\n",
    "                if (i % params.freq_scheduler == 0) & (i != 0) & (params.use_scheduler == True):\n",
    "                    scheduler.step()\n",
    "                    print(\"i : {}. Decreasing learning rate: {}\".format(i, scheduler.get_last_lr()))\n",
    "                    print(f\"loss: {loss:>7f}, median percentage error {list_perc_abs_error_MC[-1]:>7f}, [{current:>5d}/{params.nb_epochs:>5d}]\")\n",
    "\n",
    "\n",
    "            list_list_losses.append(list_perc_abs_error_MC_loss) #list_list_losses.append(loss_epochs_MC)\n",
    "            list_list_perc_abs_error_MC.append(list_perc_abs_error_MC)\n",
    "            list_list_perc_abs_error_MC_i.append(list_perc_abs_error_MC_i)\n",
    "            list_list_perc_abs_error_MC_loss.append(list_perc_abs_error_MC_loss)\n",
    "            list_list_beta.append(list_beta)\n",
    "            list_list_abs_perc_dev_bias.append(list_abs_perc_dev_bias) #absolute percentage deviation from true bias \n",
    "            list_list_abs_perc_dev_weight.append(list_abs_perc_dev_weight) #absolute percentage deviation from true weight\n",
    "       \n",
    "            if params.optimizer == \"SWA\":\n",
    "                optimizer_MC.swap_swa_sgd()\n",
    "                \n",
    "    # B. Compile results\n",
    "    #-------------------\n",
    "    df_MC = pd.DataFrame({'M': list_M[0],\n",
    "                        'N': list_N[0],\n",
    "                        'repetition': list_index_rep[0],\n",
    "                        'iter': list_list_perc_abs_error_MC_i[0],\n",
    "                        'loss': np.sqrt(np.abs(list_list_losses[0])),\n",
    "                        'med_percentage_error': list_list_perc_abs_error_MC[0],\n",
    "                        'intercept': np.array(list_list_beta[0])[:,0],\n",
    "                        'slope': np.array(list_list_beta[0])[:,1],\n",
    "                        'abs_perc_dev_bias': list_list_abs_perc_dev_bias[0], #absolute percentage deviation from true bias \n",
    "                        'abs_perc_dev_weight': list_list_abs_perc_dev_weight[0] #absolute percentage deviation from true weight\n",
    "       })\n",
    "\n",
    "    for k in range(0, len(list_list_perc_abs_error_MC)):\n",
    "        df_MC_bis = pd.DataFrame({'M': list_M[k],\n",
    "                                'N': list_N[k],\n",
    "                                'repetition': list_index_rep[k],\n",
    "                                'iter': list_list_perc_abs_error_MC_i[0],\n",
    "                                'loss': np.sqrt(np.abs(list_list_losses[k])),\n",
    "                                'med_percentage_error': list_list_perc_abs_error_MC[k],\n",
    "                                'intercept': np.array(list_list_beta[k])[:,0],\n",
    "                                'slope': np.array(list_list_beta[k])[:,1],\n",
    "                                'abs_perc_dev_bias': list_list_abs_perc_dev_bias[k], #absolute percentage deviation from true bias \n",
    "                                'abs_perc_dev_weight': list_list_abs_perc_dev_weight[k]}) #absolute percentage deviation from true weight)\n",
    "        df_MC = pd.concat([df_MC, df_MC_bis])\n",
    "    \n",
    "    # Replace NAN by large values\n",
    "    for col in [\"loss\", \"med_percentage_error\", \"intercept\", \"slope\"]:\n",
    "        df_MC[col] = df_MC[col].fillna(np.nanmax(df_MC[col]))\n",
    "    \n",
    "    # C. Statistics on results\n",
    "    df_MC_average = df_MC.groupby('iter').mean().reset_index() #mean value by iteration\n",
    "\n",
    "\n",
    "    list_cols = [\"loss\", \"med_percentage_error\", \n",
    "                 \"intercept\", \"slope\",\n",
    "                 \"abs_perc_dev_bias\", \"abs_perc_dev_weight\"]\n",
    "    \n",
    "    for col in list_cols:\n",
    "        df_MC_average[\"min_\" + col] = df_MC.groupby('iter')[col].min().reset_index()[col]                     \n",
    "        df_MC_average[\"max_\" + col] = df_MC.groupby('iter')[col].max().reset_index()[col]\n",
    "        df_MC_average[\"std_\" + col] = df_MC.groupby('iter')[col].std().reset_index()[col]\n",
    "        for qq in [1, 5, 10, 25, 50, 75, 90, 95, 99]:\n",
    "            df_MC_average[\"P\" + str(qq) + \"_\" + col] = df_MC.groupby('iter')[col].quantile(qq/100).reset_index()[col]\n",
    "\n",
    "    # Add extra info\n",
    "    df_MC_average['optimizer'] = params.optimizer\n",
    "    df_MC_average['sigma_e'] = params.σ_e\n",
    "    df_MC_average['Sobol'] = params.use_Sobol\n",
    "    df_MC_average['user_scheduler'] = params.use_scheduler\n",
    "    df_MC_average['gamma_scheduler'] = params.freq_gamma\n",
    "    df_MC_average['lr'] = params.lr\n",
    "    \n",
    "    return df_MC, df_MC_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77116f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_variance_gaussian(params, model, nb_draws, grid_M, grid_N):\n",
    "    \"\"\"\n",
    "    Calculate variance of the loss when joint gaussian assumption holds\n",
    "    Use var(f(s_m,e^i_m))\n",
    "    and cov(f(s_m,e^i_m), f(s_m,e^j_m))\n",
    "    Return: variance loss function on grid, var(resid), cov(resid)\n",
    "    \"\"\"\n",
    "    grid_T = torch.tensor(grid_M*grid_N/2)\n",
    "    grid_N = torch.tensor(grid_N)\n",
    "    \n",
    "    # Calculate variance and covariance\n",
    "    with torch.no_grad(): \n",
    "        # state\n",
    "        x = sim_states(params, nb_draws)\n",
    "\n",
    "        e1 = simulate_shocks(params, nb_draws)\n",
    "        e2 = simulate_shocks(params, nb_draws)\n",
    "                \n",
    "        # residuals for n random grid points under 2 realizations of shocks\n",
    "        R1 = Residuals_torch(model, params, x, e1)\n",
    "        R2 = Residuals_torch(model, params, x, e2)\n",
    "\n",
    "        # Construct combinations\n",
    "        # mean\n",
    "        mean_val = 0.5*torch.mean(R1) + 0.5*torch.mean(R2)\n",
    "        ## Var\n",
    "        var_R1 = torch.var(R1)\n",
    "        var_R2 = torch.var(R2)\n",
    "        var_val = 0.5*var_R1 + 0.5*var_R2\n",
    "        \n",
    "        ## Cov\n",
    "        cov_val = torch.cov(torch.column_stack((R1, R2)).T)[0,1]\n",
    "        \n",
    "        var_L = (1/(grid_T*(grid_N - 1)))*((grid_N**2 - 3*grid_N + 3)*(cov_val**2) + (2*(grid_N - 2)*cov_val + var_val)*var_val + 2*(grid_N - 1)*(var_val + (grid_N - 1)*cov_val)*(mean_val**2))\n",
    "        \n",
    "        return var_L, var_val, cov_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bcd999",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_variance_loss_fast(params, model, nb_draws, grid_M, grid_N):\n",
    "    \"\"\"\n",
    "    Calculate variance of the loss using proposition appendix\n",
    "    Use four independent shocks \n",
    "    \"\"\"\n",
    "    grid_T = torch.tensor(grid_M*grid_N/2)\n",
    "    grid_N = torch.tensor(grid_N)\n",
    "    \n",
    "    # Calculate variance and covariance\n",
    "    with torch.no_grad(): \n",
    "        x = sim_states(params, nb_draws)\n",
    "           \n",
    "        e1 = simulate_shocks(params, nb_draws)\n",
    "        e2 = simulate_shocks(params, nb_draws)\n",
    "        e3 = simulate_shocks(params, nb_draws)\n",
    "        e4 = simulate_shocks(params, nb_draws)\n",
    "        \n",
    "        # residuals for n random grid points under 2 realizations of shocks\n",
    "        R1 = Residuals_torch(model, params, x, e1)\n",
    "        R2 = Residuals_torch(model, params, x, e2)\n",
    "        R3 = Residuals_torch(model, params, x, e3)\n",
    "        R4 = Residuals_torch(model, params, x, e4)\n",
    "\n",
    "        # Construct combinations\n",
    "        R1_R2 = R1*R2\n",
    "        R1_R3 = R1*R3\n",
    "        R1_R4 = R1*R4\n",
    "        R2_R3 = R2*R3\n",
    "        R2_R4 = R2*R4\n",
    "        R3_R4 = R3*R4\n",
    "\n",
    "        # Variance cross\n",
    "        var_R1_R2 = torch.var(R1_R2)\n",
    "\n",
    "        # Co-variances with one shared element\n",
    "        cov_R1R2_R1R3 = torch.cov(torch.column_stack((R1_R2, R1_R3)).T)[0,1]\n",
    "\n",
    "        # Covariances with four different terms\n",
    "        cov_R1R2_R3R4 = torch.cov(torch.column_stack((R1_R2, R3_R4)).T)[0,1]\n",
    "\n",
    "        var_L = (1/(grid_T*(grid_N - 1)))*(var_R1_R2 + 2*(grid_N - 2)*(cov_R1R2_R1R3) + 2*((grid_N*(grid_N - 1)/4) - grid_N + 3/2)*(cov_R1R2_R3R4))\n",
    "\n",
    "    return var_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387db96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_variance_loss_fast_2(params, model, nb_draws, grid_M, grid_N):\n",
    "    \"\"\"\n",
    "    Calculate variance of the loss using proposition appendix\n",
    "    Use the four shocks to calculate more accurate values\n",
    "    \"\"\"\n",
    "    grid_T = torch.tensor(grid_M*grid_N/2)\n",
    "    grid_N = torch.tensor(grid_N)\n",
    "    \n",
    "    # Calculate variance and covariance\n",
    "    with torch.no_grad(): \n",
    "        # States\n",
    "        x = sim_states(params, nb_draws)\n",
    "        # Shocks\n",
    "        e1 = simulate_shocks(params, nb_draws)\n",
    "        e2 = simulate_shocks(params, nb_draws)\n",
    "        e3 = simulate_shocks(params, nb_draws)\n",
    "        e4 = simulate_shocks(params, nb_draws)\n",
    "        \n",
    "        # residuals for n random grid points under 2 realizations of shocks\n",
    "        R1 = Residuals_torch(model, params, x, e1)\n",
    "        R2 = Residuals_torch(model, params, x, e2)\n",
    "        R3 = Residuals_torch(model, params, x, e3)\n",
    "        R4 = Residuals_torch(model, params, x, e4)\n",
    "\n",
    "        # Construct combinations\n",
    "        R1_R2 = R1*R2\n",
    "        R1_R3 = R1*R3\n",
    "        R1_R4 = R1*R4\n",
    "        R2_R3 = R2*R3\n",
    "        R2_R4 = R2*R4\n",
    "        R3_R4 = R3*R4\n",
    "\n",
    "        #var_R1 = torch.var(R1)\n",
    "\n",
    "        # Variance cross\n",
    "        var_R1_R2 = torch.var(R1_R2)\n",
    "        var_R1_R3 = torch.var(R1_R3)\n",
    "        var_R1_R4 = torch.var(R1_R4)\n",
    "        var_R2_R3 = torch.var(R2_R3)\n",
    "        var_R2_R4 = torch.var(R2_R4)\n",
    "        var_R3_R4 = torch.var(R3_R4)\n",
    "\n",
    "        mean_var_R1_R2 = (1/6)*(var_R1_R2 + var_R1_R3 + var_R1_R4 + var_R2_R3 + var_R2_R4 +  var_R3_R4)\n",
    "\n",
    "        # Co-variances with one shared element\n",
    "        cov_R1R2_R1R3 = torch.cov(torch.column_stack((R1_R2, R1_R3)).T)[0,1]\n",
    "        cov_R1R2_R1R4 = torch.cov(torch.column_stack((R1_R2, R1_R4)).T)[0,1]\n",
    "        cov_R1R2_R2R3 = torch.cov(torch.column_stack((R1_R2, R2_R3)).T)[0,1]\n",
    "        cov_R1R2_R2R4 = torch.cov(torch.column_stack((R1_R2, R2_R4)).T)[0,1]\n",
    "        cov_R1R3_R3R4 = torch.cov(torch.column_stack((R1_R3, R3_R4)).T)[0,1]\n",
    "\n",
    "        mean_cov_R1R2_R1R3 = (1/5)*(cov_R1R2_R1R3 + cov_R1R2_R1R4 + cov_R1R2_R2R3 + cov_R1R2_R2R4 + cov_R1R3_R3R4)\n",
    "\n",
    "        # Covariances with four different terms\n",
    "        cov_R1R2_R3R4 = torch.cov(torch.column_stack((R1_R2, R3_R4)).T)[0,1]\n",
    "\n",
    "        var_L = (1/(grid_T*(grid_N - 1)))*(mean_var_R1_R2 + 2*(grid_N - 2)*(mean_cov_R1R2_R1R3) + 2*((grid_N*(grid_N - 1)/4) - grid_N + 3/2)*(cov_R1R2_R3R4))\n",
    "\n",
    "    return var_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3c4d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Residuals_torch_2(model, params, s, e):\n",
    "    # s: state\n",
    "    # e: innovation\n",
    "    # transform e into innnovation\n",
    "    eta_t = torch.exp(params.μ_e + params.σ_e*e)\n",
    "    return Residuals_torch(model, params, s, eta_t)\n",
    "    \n",
    "def calculate_N_star(params, model, mean_s, mean_e, var_cov_s, var_cov_e, T, grid_N):\n",
    "    \"\"\"\n",
    "    Use formula N*. Assumption of joint normality.\n",
    "    Also assume that mu_f^2 is close to 0\n",
    "    \"\"\"\n",
    "    #Take the gradient of the residual wrt to input variables, evaluated at the mean\n",
    "    ## Gradient wrt to space vector\n",
    "    input_tensor = torch.tensor([[mean_s]], requires_grad=True)\n",
    "    output_net = Residuals_torch_2(model, params, input_tensor, torch.tensor([[mean_e]]))\n",
    "    output_net.backward(torch.ones_like(output_net))\n",
    "    grad_s = input_tensor.grad\n",
    "\n",
    "    ## Gradient wrt to innovation vector\n",
    "    input_tensor = torch.tensor([[mean_e]], requires_grad=True)\n",
    "    output_net = Residuals_torch_2(model, params, torch.tensor([[mean_s]]), input_tensor)\n",
    "    output_net.backward(torch.ones_like(output_net))\n",
    "    grad_e = input_tensor.grad\n",
    "\n",
    "    ## calculate a and b\n",
    "    sandwich_s = torch.matmul(grad_s.t(), torch.matmul(var_cov_s, grad_s))\n",
    "    sandwich_e = torch.matmul(grad_e.t(), torch.matmul(var_cov_e, grad_e))\n",
    "    #print(sandwich_s)\n",
    "    #print(sandwich_e)\n",
    "    a = sandwich_s**2\n",
    "    b = 2*sandwich_s*sandwich_e + sandwich_e**2\n",
    "    # ignore the integer constraint\n",
    "    N_star = 1 + torch.sqrt(1 + (b/a))\n",
    "    # find nearest corresponding element on grid\n",
    "    N_star = grid_N[torch.argmin(torch.abs(N_star - grid_N))]\n",
    "    return N_star\n",
    "\n",
    "def calculate_propto_var(params, model, mean_s, mean_e, var_cov_s, var_cov_e, T, grid_N):\n",
    "    \"\"\"\n",
    "    Calculate value proportional to variance of loss.\n",
    "    Assumption of joint normality.\n",
    "    Also assume that mu_f^2 is close to 0\n",
    "    \"\"\"\n",
    "    #Take the gradient of the residual wrt to input variables, evaluated at the mean\n",
    "    ## Gradient wrt to space vector\n",
    "    input_tensor = torch.tensor([[mean_s]], requires_grad=True)\n",
    "    output_net = Residuals_torch_2(model, params, input_tensor, torch.tensor([[mean_e]]))\n",
    "    output_net.backward(torch.ones_like(output_net))\n",
    "    grad_s = input_tensor.grad\n",
    "\n",
    "    ## Gradient wrt to innovation vector\n",
    "    input_tensor = torch.tensor([[mean_e]], requires_grad=True)\n",
    "    output_net = Residuals_torch_2(model, params, torch.tensor([[mean_s]]), input_tensor)\n",
    "    output_net.backward(torch.ones_like(output_net))\n",
    "    grad_e = input_tensor.grad\n",
    "\n",
    "    ## calculate a and b\n",
    "    sandwich_s = torch.matmul(grad_s.t(), torch.matmul(var_cov_s, grad_s))\n",
    "    sandwich_e = torch.matmul(grad_e.t(), torch.matmul(var_cov_e, grad_e))\n",
    "    #print(sandwich_s)\n",
    "    #print(sandwich_e)\n",
    "    a = sandwich_s**2\n",
    "    b = 2*sandwich_s*sandwich_e + sandwich_e**2\n",
    "    # fomula (49):\n",
    "    vals = (1/params.T)*(a * (torch.tensor(grid_N) + 2 + (1/(torch.tensor(grid_N) - 1))) + (1/(torch.tensor(grid_N) - 1))*b) \n",
    "    return vals.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849eb940",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_flat(a):\n",
    "    \"\"\"\n",
    "    Function to flatten a list\n",
    "    \"\"\"\n",
    "    return list(np.array(a).flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121d99e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSystemInfo():\n",
    "    try:\n",
    "        info={}\n",
    "        info['platform']=platform.system()\n",
    "        info['platform-release']=platform.release()\n",
    "        info['platform-version']=platform.version()\n",
    "        info['architecture']=platform.machine()\n",
    "        info['hostname']=socket.gethostname()\n",
    "        info['ip-address']=socket.gethostbyname(socket.gethostname())\n",
    "        info['mac-address']=':'.join(re.findall('..', '%012x' % uuid.getnode()))\n",
    "        info['processor']=platform.processor()\n",
    "        info['ram']=str(round(psutil.virtual_memory().total / (1024.0 **3)))+\" GB\"\n",
    "        return json.dumps(info)\n",
    "    except Exception as e:\n",
    "        logging.exception(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8a5899",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
