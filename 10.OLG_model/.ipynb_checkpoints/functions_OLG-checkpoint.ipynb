{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccbae77f",
   "metadata": {},
   "source": [
    "# Functions for OLG\n",
    "\n",
    "Create functions used when solving an OLG Model with the [bc-MC Operator](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4476122)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034146bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import quantecon as qe\n",
    "from interpolation import interp\n",
    "from quantecon.optimize import brentq\n",
    "from numba import njit, float64\n",
    "from numba.experimental import jitclass\n",
    "import Tasmanian # sparse grids\n",
    "\n",
    "import random\n",
    "import scipy.stats\n",
    "import chaospy  ## for quadrature\n",
    "from itertools import product\n",
    "import os\n",
    "\n",
    "import time\n",
    "from math import sqrt\n",
    "import seaborn as sns; sns.set()\n",
    "from tqdm import tqdm as tqdm         # tqdm is a nice library to visualize ongoing loops\n",
    "import datetime\n",
    "# followint lines are used for indicative typing\n",
    "from typing import Tuple\n",
    "class Vector: pass\n",
    "from scipy.stats import norm\n",
    "import Tasmanian\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "\n",
    "# To create copies of NN\n",
    "import copy\n",
    "import matplotlib.ticker as mtick\n",
    "# To use sparse kronecker product\n",
    "from scipy import sparse\n",
    "from torchcontrib.optim import SWA\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60cba15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "# Ouputs:\n",
    "# 1. the share of cash-in-hand consumed\n",
    "# 2. the lagrange multiplier h \n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(5, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba1e288",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_params(params, limited=True):\n",
    "    \"\"\"\n",
    "    Function to display parameter values\n",
    "    \"\"\"\n",
    "    print(\"learning rate: {}\".format(params.lr))\n",
    "    print(\"nb epochs: {}\".format(params.nb_epochs))\n",
    "    print(\"W_expanded.shape: {}\".format(params.W_expanded.shape))\n",
    "    print(\"M: {}\".format(params.M))\n",
    "    print(\"N: {}\".format(params.N))\n",
    "    print(\"MN: {}\".format(params.MN))\n",
    "    print(\"T: {}\".format(params.T))\n",
    "    print(\"optimizer_chosen: {}\".format(params.optimizer))\n",
    "    print(\"use_scheduler: {}\".format(params.use_scheduler))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c8b947",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape) \n",
    "\n",
    "# RMSE\n",
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        \n",
    "    def forward(self,yhat,y):\n",
    "        return torch.sqrt(self.mse(yhat,y))\n",
    "    \n",
    "# Gaussian quadrature rule\n",
    "# See: https://chaospy.readthedocs.io/en/master/api/chaospy.generate_quadrature.html\n",
    "def dist(order, distribution, rule = \"gaussian\", sp=True):\n",
    "    #order=int(n**(1/d))-1\n",
    "    x, w = chaospy.generate_quadrature(order, distribution, rule=(rule), sparse=sp)\n",
    "    return x, w\n",
    "\n",
    "def create_W_expanded_matrix(M, N, rep):\n",
    "    \"\"\"\n",
    "    create a sparse matrix W_expanded with U repeate M times on the diagonal elements\n",
    "    where U is an upper triangular matrix with 0 on the diagonal and 1 on the other upper elements\n",
    "    W_expanded is a sparse torch matrix\n",
    "    rep: number of times the matrix must be repeat vertically\n",
    "    \"\"\"\n",
    "    A_expanded = np.ones((N, N))\n",
    "    U = np.triu(A_expanded) # upper trianguler matrix of ones\n",
    "    np.fill_diagonal(U, 0) #fill diagonal with 0\n",
    "    U = sparse.csr_matrix(U) # convert to sparse\n",
    "    # Unity matrix of size (M*M)\n",
    "    B = sparse.csr_matrix(np.eye(M, M))\n",
    "    D = sparse.kron(B, U)\n",
    "    # To \"repeat\" D vertically M times\n",
    "    #D_repeated_vertical = sparse.vstack([D] * rep )\n",
    "    #D_repeated_horizontal = sparse.hstack([D] * rep)\n",
    "    # create a larger block diagonal matrix with D on the diagonal\n",
    "    I_rep = sparse.csr_matrix(np.eye(rep, rep))\n",
    "    D_repeated = sparse.kron(I_rep, D)\n",
    "    \n",
    "    # Convert to sparse tensor\n",
    "    W_expanded = sparse_mx_to_torch_sparse_tensor(D_repeated)\n",
    "    \n",
    "    return W_expanded\n",
    "\n",
    "min_FB = lambda a,b: a+b-tf.sqrt(a**2+b**2)\n",
    "min_FB_torch = lambda a,b: a+b-torch.sqrt(a**2+b**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abcca2a",
   "metadata": {},
   "source": [
    "## Data management with Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d359a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_states(dataloader_replacement):\n",
    "    \"\"\"\n",
    "    Simulate state vector. Use data to approximate density.\n",
    "    Input is a pytorch data loader (with replacement)\n",
    "    \"\"\"\n",
    "    # Create an iterator from the DataLoader\n",
    "    iterator = iter(dataloader_replacement)\n",
    "    # Draw one batch using next\n",
    "    batch = next(iterator)  \n",
    "    return batch\n",
    "            \n",
    "def simulate_shocks(params, len_series):\n",
    "    # randomly drawing shocks \n",
    "    # chaospy.Normal(self.mean_tfp, self.std_tfp), chaospy.Normal(self.mean_delta, self.std_delta)\n",
    "    if params.distribution_shocks == \"Normal\":\n",
    "        e_tfp = torch.normal(mean=params.mean_tfp, std=params.std_tfp, size=(len_series,)) \n",
    "        e_delta = torch.normal(mean=params.mean_delta, std=params.std_delta, size=(len_series,)) \n",
    "        # Ensure we don't get negative values\n",
    "        if (torch.sum((e_tfp < 0.0)) > 0) | (torch.sum((e_delta <0.0)) > 0):\n",
    "            raise Exception(f\"Negative values happened for tfp or delta\") \n",
    "    else:\n",
    "        raise Exception(f\"{params.distribution_shocks} not implemented.\") \n",
    "    return e_tfp, e_delta \n",
    "\n",
    "class InfiniteSampler(torch.utils.data.Sampler):\n",
    "    \"\"\"Infinite Sampler that generates infinite indices by sampling with replacement.\"\"\"\n",
    "    def __init__(self, data_source):\n",
    "        self.num_samples = len(data_source)\n",
    "\n",
    "    def __iter__(self):\n",
    "        while True:  # Infinite loop\n",
    "            yield torch.randint(high=self.num_samples, size=(1,), dtype=torch.int64).item()\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    \"\"\"Custom Dataset class.\"\"\"\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "def generate_data_debug(params, T, N):\n",
    "    \"\"\"\n",
    "    Generate test data. Random draws from Uniform.\n",
    "    \"\"\"\n",
    "    # T: Number of observations. represents length of simulation\n",
    "    # N: Dimension of each observation vector. dimension of state vector.\n",
    "    # Generate random observations (T vectors, each of N dimensions)\n",
    "    # observations = torch.rand(T, N) + 1e-6 #torch.randn(T, N)\n",
    "    \n",
    "    # random capital allocation vectors\n",
    "    # row: obs\n",
    "    # col: variable\n",
    "    r1 = 0.25\n",
    "    r2 = 3.0\n",
    "    distribution_capital = (r1 - r2) * torch.rand(T, params.nb_agents) + r2\n",
    "    distribution_capital[:,0] = 0\n",
    "\n",
    "    # random exo state vetors:\n",
    "    e_tfp, e_delta = simulate_shocks(params, T)\n",
    "    exo_states = torch.column_stack((e_tfp, e_delta))\n",
    "    \n",
    "    observations = torch.hstack((distribution_capital, exo_states)) \n",
    "    return observations\n",
    "\n",
    "def generate_dataloaders(observations, batch_size):\n",
    "    \"\"\"\n",
    "    Generate dataloaders\n",
    "    \"\"\"\n",
    "    # batch_size: dimension of each draw. (M)\n",
    "    # Create a TensorDataset\n",
    "    dataset = TensorDataset(observations)\n",
    "\n",
    "    # Create a DataLoader\n",
    "    ## Draw without replacement\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    ## Draw with replacement\n",
    "    # Create a DataLoader with the InfiniteSampler\n",
    "    dataset = MyDataset(observations)\n",
    "    dataloader_with_replacement = DataLoader(dataset, batch_size=batch_size, sampler=InfiniteSampler(dataset))\n",
    "\n",
    "    return dataloader, dataloader_with_replacement\n",
    "\n",
    "def generate_data_and_dataloaders_debug(params, T, N, batch_size):\n",
    "    if T < batch_size:\n",
    "        raise Exception(f\"T: {T} < batch_size: {batch_size}\") \n",
    "    observations = generate_data_debug(params, T, N)\n",
    "    dataloader, dataloader_with_replacement = generate_dataloaders(observations, batch_size)\n",
    "    return dataloader, dataloader_with_replacement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689e16a8",
   "metadata": {},
   "source": [
    "## True model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8543a69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_true_model(len_T, params, non_stochastic_ss = False):\n",
    "    \"\"\"\n",
    "    Simulate true model\n",
    "    \"\"\"\n",
    "    #add burnin\n",
    "    burnin = 100\n",
    "    T = len_T + burnin\n",
    "    \n",
    "    # random draws\n",
    "    if non_stochastic_ss == False:\n",
    "        tfp_vec, delta_vec = simulate_shocks(params, T)\n",
    "    else:\n",
    "        tfp_vec = params.mean_tfp*torch.ones(T)\n",
    "        delta_vec = params.mean_delta*torch.ones(T)\n",
    "\n",
    "    #distribution capital holdings:\n",
    "    h_matrix = torch.zeros((T, params.nb_agents))\n",
    "    h_matrix[0,1:] = 1 #first periods\n",
    "    K_vec = torch.zeros(T)\n",
    "    ## Exogeneous labour supply. One first period, 0, else\n",
    "    l_matrix = torch.zeros((T, params.nb_agents))\n",
    "    l_matrix[:, 0] = 1\n",
    "    L_vec = torch.zeros(T)\n",
    "    r = torch.zeros(T)\n",
    "    w = torch.zeros(T)\n",
    "    wealth = torch.zeros((T, params.nb_agents))\n",
    "    a_matrix = torch.zeros((T, params.nb_agents))\n",
    "    c_matrix = torch.zeros((T, params.nb_agents))\n",
    "\n",
    "    # Loop over time periods:\n",
    "    for t in range(0, T):\n",
    "        # inherit from last period\n",
    "\n",
    "        # infer sum of capital\n",
    "        K_vec[t] = torch.sum(h_matrix[t,:])\n",
    "        ## Total labour supply (useless calculations, but then we can generalize the code)\n",
    "        L_vec[t] = torch.sum(l_matrix[t,:])\n",
    "\n",
    "        r[t] = interest_rate(K_vec[t], L_vec[t], delta_vec[t], tfp_vec[t], params)\n",
    "        w[t] = wage(K_vec[t], L_vec[t], tfp_vec[t], params)\n",
    "\n",
    "        ## calculate wealth, before consumption decision made\n",
    "        wealth[t, :] = h_matrix[t,:]*r[t] + l_matrix[t,:]*w[t]\n",
    "\n",
    "        ## savings choice\n",
    "        a_matrix[t,:] = wealth[t, :]*params.mult_wealth\n",
    "        c_matrix[t,:] = wealth[t, :] - a_matrix[t,:]\n",
    "\n",
    "        ## next period\n",
    "        # Sift by one period\n",
    "        if t < (T-1):\n",
    "            h_matrix[t+1,1:] = a_matrix[t,0:-1].clone()\n",
    "      \n",
    "    exo_states = torch.column_stack((tfp_vec, delta_vec))\n",
    "    observations = torch.hstack((h_matrix[burnin:, :], exo_states[burnin:, :])) \n",
    "    \n",
    "    return observations\n",
    " \n",
    "def generate_data_and_dataloaders_true(params, T, batch_size):\n",
    "    \"\"\"\n",
    "    Generate data from true model\n",
    "    \"\"\"\n",
    "    if T < batch_size:\n",
    "        raise Exception(f\"T: {T} < batch_size: {batch_size}\") \n",
    "    observations = simulate_true_model(T, params)\n",
    "    dataloader, dataloader_with_replacement = generate_dataloaders(observations, batch_size)\n",
    "    return dataloader, dataloader_with_replacement\n",
    "\n",
    "# Simulation of length 1000, batch size = M\n",
    "#d, d_replacement = generate_data_and_dataloaders_true(params, 10000, params.M)\n",
    "\n",
    "def generate_n_batches(nb_batches, d_replacement):\n",
    "    \"\"\"\n",
    "    Draw nb_draws times M draws from ergodic distribution\n",
    "    \"\"\"\n",
    "    for i in range(0, nb_batches):\n",
    "        if i == 0:\n",
    "            state_vec = sim_states(d_replacement)\n",
    "        else:\n",
    "            state_vec = torch.vstack((state_vec, sim_states(d_replacement)))\n",
    "    return state_vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5fbe8b",
   "metadata": {},
   "source": [
    "Generate data using neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c6b683",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_current_model(neural_net, len_T, params, use_true_model = False, non_stochastic_ss = False):\n",
    "    \"\"\"\n",
    "    Use current neural net to simulate the model\n",
    "    Input:\n",
    "    neural_net: a pytorch neural network\n",
    "    len_T: length of simulation\n",
    "    params: a Params object\n",
    "    use_true_model: if true, use analytic solution. Else, use neural net\n",
    "    non_stochastic_ss: if true, simulation with exo variables constant\n",
    "    \"\"\"\n",
    "    #add burnin\n",
    "    burnin = int(len_T/10) #10% burnin\n",
    "    #print(burnin)\n",
    "    T = len_T + burnin\n",
    "    \n",
    "    # random draws\n",
    "    if non_stochastic_ss == False:\n",
    "        tfp_vec, delta_vec = simulate_shocks(params, T)\n",
    "    else:\n",
    "        tfp_vec = params.mean_tfp*torch.ones(T)\n",
    "        delta_vec = params.mean_delta*torch.ones(T)\n",
    "\n",
    "    #distribution capital holdings:\n",
    "    h_matrix = torch.zeros((T, params.nb_agents))\n",
    "    h_matrix[0,1:] = 1 #first periods\n",
    "    K_vec = torch.zeros(T)\n",
    "    ## Exogeneous labour supply. One first period, 0, else\n",
    "    l_matrix = torch.zeros((T, params.nb_agents))\n",
    "    l_matrix[:, 0] = 1\n",
    "    L_vec = torch.zeros(T)\n",
    "    r = torch.zeros(T)\n",
    "    w = torch.zeros(T)\n",
    "    wealth = torch.zeros((T, params.nb_agents))\n",
    "    a_matrix = torch.zeros((T, params.nb_agents))\n",
    "    c_matrix = torch.zeros((T, params.nb_agents))\n",
    "\n",
    "    # Loop over time periods:\n",
    "    for t in range(0, T):\n",
    "        # inherit from last period\n",
    "\n",
    "        # infer sum of capital\n",
    "        K_vec[t] = torch.sum(h_matrix[t,:])\n",
    "        ## Total labour supply (useless calculations, but then we can generalize the code)\n",
    "        L_vec[t] = torch.sum(l_matrix[t,:])\n",
    "\n",
    "        r[t] = interest_rate(K_vec[t], L_vec[t], delta_vec[t], tfp_vec[t], params)\n",
    "        w[t] = wage(K_vec[t], L_vec[t], tfp_vec[t], params)\n",
    "\n",
    "        ## calculate wealth, before consumption decision made\n",
    "        wealth[t, :] = h_matrix[t,:]*r[t] + l_matrix[t,:]*w[t]\n",
    "\n",
    "        ## savings choice\n",
    "        if use_true_model == True:\n",
    "            a_matrix[t,:] = wealth[t, :]*params.mult_wealth\n",
    "            c_matrix[t,:] = wealth[t, :] - a_matrix[t,:]\n",
    "        else:\n",
    "            c_matrix[t, :] = model_normalized(neural_net, wealth[t, :].view(1,-1), params)\n",
    "            ## Infer capital decision\n",
    "            a_matrix[t, :] = wealth[t, :] - c_matrix[t, :]\n",
    "            \n",
    "        ## next period\n",
    "        # Sift by one period\n",
    "        if t < (T-1):\n",
    "            h_matrix[t+1,1:] = a_matrix[t,0:-1].clone()\n",
    "      \n",
    "    exo_states = torch.column_stack((tfp_vec, delta_vec))\n",
    "    observations = torch.hstack((h_matrix[burnin:, :], exo_states[burnin:, :])) \n",
    "    \n",
    "    return observations\n",
    "\n",
    "def generate_data_and_dataloaders_current_model(neural_net, params, T, batch_size, use_true_model = False, non_stochastic_ss = False):\n",
    "    \"\"\"\n",
    "    Generate data using current neural network. Return a dataloader without and with replacement.\n",
    "    Input:\n",
    "    neural_net: a pytorch neural network\n",
    "    len_T: length of simulation\n",
    "    params: a Params object\n",
    "    use_true_model: if true, use analytic solution. Else, use neural net\n",
    "    non_stochastic_ss: if true, simulation with exo variables constant\n",
    "    \"\"\"\n",
    "    if T < batch_size:\n",
    "        raise Exception(f\"T: {T} < batch_size: {batch_size}\") \n",
    "    observations = simulate_current_model(neural_net, T, params, use_true_model, non_stochastic_ss)\n",
    "    dataloader, dataloader_with_replacement = generate_dataloaders(observations, batch_size)\n",
    "    return dataloader, dataloader_with_replacement\n",
    "\n",
    "# Simulation of length 1000, batch size = M\n",
    "#d, d_replacement = generate_data_and_dataloaders_current_model(model_bcMC, params, 1000, params.M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0c69ba",
   "metadata": {},
   "source": [
    "## bc-MC Operator\n",
    "\n",
    "For a single age category, the bc-MC operator is:\n",
    "\n",
    "$$ \\frac{1}{M} \\frac{2}{(N)(N-1)} \\sum_{m=1}^{M} \\sum_{1\\leq i < j}^{n} f(s_m, \\epsilon_{m}^{(i)})f(s_m, \\epsilon_{m}^{(j)})  $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0a6c7f",
   "metadata": {},
   "source": [
    "### To measure the accuracy of the bc-MC operator\n",
    "\n",
    "\n",
    "#### Monte Carlo integration\n",
    "\n",
    "We want to calculate the mean value of the Euler equation error (EEE):\n",
    "\n",
    "$$EEE =  \\frac{1}{c_t}(u^{\\prime-1})\\Big(\\mathbf{E}_{\\varepsilon}\\big[{\\beta u^{\\prime}(c_{t+1}) r_{t+1}} \\big]\\Big) - 1$$\n",
    "\n",
    "Let's first use Monte Carlo to approximate the integral with respect to the innovation vector. Let's first fix the value of the state vector to $s_m$. Conditional on this value, the expectation with respect to the innovation vector is approximated by:\n",
    "\n",
    "$$ \\mathbf{E}_{\\varepsilon} g(s_m,  \\epsilon) \\approx \\frac{1}{N} \\sum_{i=1}^{N}  g(s_m,  \\epsilon^{(i)}) $$\n",
    "\n",
    "wich can be vectorized as\n",
    "\n",
    "$$ \\mathbf{1}_N^{T} . \\begin{pmatrix} g(s_m, \\epsilon^{(1)}) \\\\ \\vdots \\\\ g(s_m, \\epsilon^{(N)}) \\end{pmatrix} $$\n",
    "\n",
    "where $\\mathbf{1}_N^{T} = (\\frac{1}{n}, \\frac{1}{n}, ..., \\frac{1}{n})$ is a $(N, 1)$ row vector.\n",
    "\n",
    "Now, for several draws of $s_m$, we can calculate conditional means as:\n",
    "\n",
    "$$ \\begin{pmatrix} \\frac{1}{N} \\sum_{i=1}^{N}  g(s_1,  \\epsilon^{(i)}) \\\\ \\vdots \\\\ \\frac{1}{N} \\sum_{i=1}^{N}  g(s_m,  \\epsilon^{(i)})) \\end{pmatrix} = \\begin{pmatrix}\n",
    "\\mathbf{1}_N^{T} & \\mathbf{0} & ... & \\mathbf{0}\\\\\n",
    "\\mathbf{0} & \\mathbf{1}_N^{T} & \\mathbf{0} & \\mathbf{0} \\\\\n",
    "... & \\mathbf{0} & ... & ... \\\\\n",
    "\\mathbf{0} & \\mathbf{0} & ... & \\mathbf{1}_N^{T}\n",
    "\\end{pmatrix}  \\begin{pmatrix} g(s_1,  \\epsilon_1^{(i)}) \\\\ \\vdots \\\\ g(s_1,  \\epsilon_1^{(N)}) \\\\ \\vdots \\\\ g(s_M,  \\epsilon_M^{(1)}) \\\\ ... \\\\ g(s_M,  \\epsilon_M^{(N)}) \\end{pmatrix}$$\n",
    "\n",
    "This is what I do in the function `evaluate_accuracy_pytorch_MC` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30ce9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy_pytorch_MC(neural_net, n, n_Monte_Carlo, params, dataloader_replacement, debug=True):\n",
    "    \"\"\"\n",
    "    Function to evaluate the accuracy using Monte Carlo for the expectations\n",
    "    \"\"\"\n",
    "    # n: number of draws for current state\n",
    "    # n_Monte_Carlo: number of draws for next state\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # To calculate means quickly, vectorize\n",
    "        # Sparse version\n",
    "        A = sparse.eye(n) #(n,n) identity matrix\n",
    "        B = sparse.csr_matrix(np.ones(n_Monte_Carlo)/n_Monte_Carlo) #(1, n_Monte_Carlo) row vector\n",
    "        # Sparse kronecker product. Then convert to pytorch sparse.\n",
    "        D = sparse.kron(A, B) #(n, n_Monte_Carlo) matrix, with repeated row vectors B. \n",
    "        # Repeat A-1 times\n",
    "        rep = params.nb_agents - 1\n",
    "        I_rep = sparse.csr_matrix(np.eye(rep, rep))\n",
    "        D_repeated = sparse.kron(I_rep, D)\n",
    "        # Convert to sparse tensor\n",
    "        W = sparse_mx_to_torch_sparse_tensor(D_repeated)\n",
    "    \n",
    "        # State vector\n",
    "        ## Get the numer batch size necessary to get n draws\n",
    "        if n < params.M:\n",
    "            nb_draws = params.M\n",
    "        else:\n",
    "            nb_draws = int(n/params.M)\n",
    "        state_vec = generate_n_batches(nb_draws, dataloader_replacement) #each draw is of size M\n",
    "        # Select right size\n",
    "        state_vec = state_vec[:n,:]\n",
    "        \n",
    "        #print(state_vec.shape)\n",
    "        h_matrix = state_vec[:,:-2] \n",
    "        # current value for z and delta\n",
    "        tfp_vec = state_vec[:, -2]\n",
    "        delta_vec = state_vec[:, -1]\n",
    "        \n",
    "        ## Innovation vector\n",
    "        # n_Monte_Carlo for each value today\n",
    "        e_tfp, e_delta = simulate_shocks(params, n*n_Monte_Carlo)\n",
    "        innovation_vec =  torch.column_stack((e_tfp, e_delta))\n",
    "    \n",
    "        ## Current period\n",
    "        # infer sum of capital\n",
    "        K_vec = torch.sum(h_matrix, 1)\n",
    "\n",
    "        ## Exogeneous labour supply. One first period, 0, else\n",
    "        l_matrix = torch.zeros_like(h_matrix)\n",
    "        l_matrix[:, 0] = 1\n",
    "        ## Total labour supply (useless calculations, but then we can generalize the code)\n",
    "        L_vec = torch.sum(l_matrix, 1)\n",
    "\n",
    "        r = interest_rate(K_vec, L_vec, delta_vec, tfp_vec, params)\n",
    "        w = wage(K_vec, L_vec, tfp_vec, params)\n",
    "\n",
    "        ## calculate wealth, before consumption decision made\n",
    "        wealth = h_matrix*r.view(-1,1) + l_matrix*w.view(-1,1)\n",
    "\n",
    "        ## Consumption curent period\n",
    "        if debug == False:\n",
    "            c = model_normalized(neural_net, wealth, params)\n",
    "            ## Infer capital decision\n",
    "            a = wealth - c\n",
    "        else:\n",
    "            a = wealth*params.mult_wealth.view(1, -1)\n",
    "            c = wealth - a\n",
    "       \n",
    "        ## Period t+1\n",
    "        ## comes from last period. But first generation has zero capital\n",
    "        h_matrix_next = torch.zeros_like(a)\n",
    "        # Sift by one period\n",
    "        h_matrix_next[:,1:] = a[:,0:-1].clone()\n",
    "        h_matrix_next = h_matrix_next.repeat_interleave(n_Monte_Carlo, dim=0) # shape (n*n_Monte_Carlo, A)\n",
    "\n",
    "        ## Repeat values from period t to vectorize code\n",
    "        c_repeated = c.repeat_interleave(n_Monte_Carlo, dim=0) # shape (n*n_Monte_Carlo, A)\n",
    "        a_repeated = a.repeat_interleave(n_Monte_Carlo, dim=0) # shape (n*n_Monte_Carlo, A)\n",
    "        \n",
    "        # transitions of the exogenous processes\n",
    "        ## No persistence here (but easy to change that)\n",
    "        ## No need to repeat. Already rigth shape\n",
    "        tfp_vec_next = innovation_vec[:, -2]\n",
    "        delta_vec_next = innovation_vec[:, -1]\n",
    "\n",
    "        K_vec_next = torch.sum(h_matrix_next, 1)\n",
    "\n",
    "        ## Exogeneous labour supply. One first period, 0, else\n",
    "        l_matrix_next = torch.zeros_like(h_matrix_next)\n",
    "        l_matrix_next[:, 0] = 1\n",
    "        ## Total labour supply (useless calculations, but then we can generalize the code)\n",
    "        L_vec_next = torch.sum(l_matrix_next, 1)\n",
    "\n",
    "        ## get factor prices (wages and interest rate)\n",
    "        r_next = interest_rate(K_vec_next, L_vec_next, delta_vec_next, tfp_vec_next, params)\n",
    "        w_next = wage(K_vec_next, L_vec_next, tfp_vec_next, params)\n",
    "\n",
    "        ## calculate wealth, before consumption decision made\n",
    "        wealth_next = h_matrix_next*r_next.view(-1,1) + l_matrix_next*w_next.view(-1,1)\n",
    "\n",
    "        ## Consumption curent period\n",
    "        if debug == False:\n",
    "            c_next = model_normalized(neural_net, wealth_next, params)\n",
    "        else:\n",
    "            a_next = wealth_next*params.mult_wealth.view(1, -1)\n",
    "            c_next = wealth_next - a_next\n",
    "\n",
    "        # Each column is the euler equation for one agent\n",
    "        # rows are observations\n",
    "        #s = c_next[:, 1:params.nb_agents].shape\n",
    "        #print(f\"shape c next: {s}\")\n",
    "        u_prime_next = params.u_prime(c_next)\n",
    "        #u_prime_next = c_next**(-params.gamma) \n",
    "\n",
    "        # Calculate beta (u'-1){E[r_{t+1} u'(c_{t+1})]}\n",
    "        vals = u_prime_next[:, 1:params.nb_agents]*r_next.view(-1,1)\n",
    "        #print(vals.shape)\n",
    "        # Reshape matrix (MN, nb_agents) to a single column array of size (nb_agents*MN, 1)\n",
    "        # First column, then second column, the third column, and so on..\n",
    "        vals_reshaped = vals.t().contiguous().view(-1, 1)\n",
    "        \n",
    "        #print(vals_reshaped.shape)\n",
    "        #print(W.shape)\n",
    "        #torch.sparse.mm(W, vals_reshaped)\n",
    "        u_prime_inverse = (params.beta*torch.sparse.mm(W, vals_reshaped))**(-1.0/params.gamma)\n",
    "        #print(u_prime_inverse.shape)\n",
    "        \n",
    "        # Euler equation error\n",
    "        c_reshaped = c[:, 0:params.nb_agents-1].t().contiguous().view(-1, 1)\n",
    "        EEE = (u_prime_inverse/c_reshaped) - 1\n",
    "        \n",
    "    return EEE.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b10055b",
   "metadata": {},
   "source": [
    "#### Gaussian quadrature\n",
    "\n",
    "We can also use [Gaussian quadrature](https://en.wikipedia.org/wiki/Gaussian_quadrature) to approximate the integral with respect to the innovation vector. \n",
    "Once again, let's first fix the value of the state vector to $s_m$. Conditional on this value, the expectation with respect to the innovation vector is approximated by:\n",
    "\n",
    "$$ \\mathbf{E}_{\\varepsilon} g(s_m,  \\epsilon) \\approx \\sum_{i=1}^{N} w_{i}  g(s_m,  \\epsilon^{(i)}) $$\n",
    "\n",
    "If we set $w_i = \\frac{1}{n}$ and if we use random and independent draws for $\\epsilon^{(i)}$, we are back to the Monte Carlo case discussed above. This observation makes us realize that we can reuse the same vectorization scheme as above, with minor modifications. More specifically, conditional on a given value for $s_m$, the expectation with respect to the innovation vector is approximated by:\n",
    "\n",
    "$$ \\mathbf{E}_{\\varepsilon} g(s_m,  \\epsilon) \\approx \\sum_{i=1}^{N} w_i g(s_m,  \\epsilon^{(i)}) $$\n",
    "\n",
    "with $w_i$ Gaussian quadrature weights and $\\epsilon^{(i)}$ the corresponding Gaussian quadrature nodes.\n",
    "\n",
    "This can be vectorized as\n",
    "\n",
    "$$ \\mathbf{1}_{w}^{T} . \\begin{pmatrix} g(s_m, \\epsilon^{(1)}) \\\\ \\vdots \\\\ g(s_m, \\epsilon^{(N)}) \\end{pmatrix} $$\n",
    "\n",
    "where $\\mathbf{1}_w^{T} = (w_1, w_2, ..., w_N)$ is a $(N, 1)$ row vector.\n",
    "\n",
    "Now, for several draws of $s_m$, we can calculate conditional means as:\n",
    "\n",
    "$$ \\begin{pmatrix} \\frac{1}{N} \\sum_{i=1}^{N}  g(s_1,  \\epsilon^{(i)}) \\\\ \\vdots \\\\ \\frac{1}{N} \\sum_{i=1}^{N}  g(s_m,  \\epsilon^{(i)})) \\end{pmatrix} = \\begin{pmatrix}\n",
    "\\mathbf{1}_{w}^{T} & \\mathbf{0} & ... & \\mathbf{0}\\\\\n",
    "\\mathbf{0} & \\mathbf{1}_{w}^{T} & \\mathbf{0} & \\mathbf{0} \\\\\n",
    "... & \\mathbf{0} & ... & ... \\\\\n",
    "\\mathbf{0} & \\mathbf{0} & ... & \\mathbf{1}_{w}^{T}\n",
    "\\end{pmatrix}  \\begin{pmatrix} g(s_1,  \\epsilon_1^{(i)}) \\\\ \\vdots \\\\ g(s_1,  \\epsilon_1^{(N)}) \\\\ \\vdots \\\\ g(s_M,  \\epsilon_M^{(1)}) \\\\ ... \\\\ g(s_M,  \\epsilon_M^{(N)}) \\end{pmatrix}$$\n",
    "\n",
    "This is what I do in the function `evaluate_accuracy_pytorch_Gaussian` below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7e6532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy_pytorch_Gaussian(neural_net, n, params, dataloader_replacement, debug=True):\n",
    "    \"\"\"\n",
    "    Function to evaluate the accuracy using Gaussian quadrature for the expectations\n",
    "    Use new draws at each call\n",
    "    \"\"\"\n",
    "    # n: number of draws for current state\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # To repeat vectors\n",
    "        n_Monte_Carlo = len(params.weights) #length of nodes\n",
    "        \n",
    "        # To calculate means quickly, vectorize\n",
    "        # Sparse version\n",
    "        A = sparse.eye(n)\n",
    "        B = sparse.csr_matrix(params.weights)\n",
    "        # Sparse kronecker product. Then convert to pytorch sparse\n",
    "        D = sparse.kron(A, B)\n",
    "        \n",
    "        # Repeat A-1 times\n",
    "        rep = params.nb_agents - 1\n",
    "        I_rep = sparse.csr_matrix(np.eye(rep, rep))\n",
    "        D_repeated = sparse.kron(I_rep, D)\n",
    "        # Convert to sparse tensor\n",
    "        W = sparse_mx_to_torch_sparse_tensor(D_repeated)\n",
    "        \n",
    "        # State vector\n",
    "        ## Get the numer batch size necessary to get n draws\n",
    "        if n < params.M:\n",
    "            nb_draws = params.M\n",
    "        else:\n",
    "            nb_draws = int(n/params.M)\n",
    "        state_vec = generate_n_batches(nb_draws, dataloader_replacement) #each draw is of size M\n",
    "        # Select right size\n",
    "        state_vec = state_vec[:n,:]\n",
    "        \n",
    "        #print(state_vec.shape)\n",
    "        h_matrix = state_vec[:,:-2] \n",
    "        # current value for z and delta\n",
    "        tfp_vec = state_vec[:, -2]\n",
    "        delta_vec = state_vec[:, -1]\n",
    "          \n",
    "        # Innovation vector\n",
    "        #e_r = params.nodes_torch[:,0].float().repeat(n)\n",
    "        #e_δ = params.nodes_torch[:,1].float().repeat(n)\n",
    "        #e_ps = params.nodes_torch[:,2:].float().repeat(n, 1) #e_p1, ..., e_p2\n",
    "\n",
    "        ## Innovation vector\n",
    "        # n_Monte_Carlo for each value today\n",
    "        #e_tfp, e_delta = simulate_shocks(params, n*n_Monte_Carlo)\n",
    "        ## Gaussian quadrature nodes\n",
    "        ## repeat to match the number of draws for the state vector\n",
    "        innovation_vec =  params.nodes_torch.float().repeat(n, 1)\n",
    "    \n",
    "        ## Current period\n",
    "        # infer sum of capital\n",
    "        K_vec = torch.sum(h_matrix, 1)\n",
    "\n",
    "        ## Exogeneous labour supply. One first period, 0, else\n",
    "        l_matrix = torch.zeros_like(h_matrix)\n",
    "        l_matrix[:, 0] = 1\n",
    "        ## Total labour supply (useless calculations, but then we can generalize the code)\n",
    "        L_vec = torch.sum(l_matrix, 1)\n",
    "\n",
    "        r = interest_rate(K_vec, L_vec, delta_vec, tfp_vec, params)\n",
    "        w = wage(K_vec, L_vec, tfp_vec, params)\n",
    "\n",
    "        ## calculate wealth, before consumption decision made\n",
    "        wealth = h_matrix*r.view(-1,1) + l_matrix*w.view(-1,1)\n",
    "\n",
    "        ## Consumption curent period\n",
    "        if debug == False:\n",
    "            c = model_normalized(neural_net, wealth, params)\n",
    "            ## Infer capital decision\n",
    "            a = wealth - c\n",
    "        else:\n",
    "            a = wealth*params.mult_wealth.view(1, -1)\n",
    "            c = wealth - a\n",
    "       \n",
    "        ## Period t+1\n",
    "        ## comes from last period. But first generation has zero capital\n",
    "        h_matrix_next = torch.zeros_like(a)\n",
    "        # Sift by one period\n",
    "        h_matrix_next[:,1:] = a[:,0:-1].clone()\n",
    "        h_matrix_next = h_matrix_next.repeat_interleave(n_Monte_Carlo, dim=0) # shape (n*n_Monte_Carlo, A)\n",
    "\n",
    "        ## Repeat values from period t to vectorize code\n",
    "        c_repeated = c.repeat_interleave(n_Monte_Carlo, dim=0) # shape (n*n_Monte_Carlo, A)\n",
    "        a_repeated = a.repeat_interleave(n_Monte_Carlo, dim=0) # shape (n*n_Monte_Carlo, A)\n",
    "        \n",
    "        # transitions of the exogenous processes\n",
    "        ## No persistence here (but easy to change that)\n",
    "        ## No need to repeat. Already rigth shape\n",
    "        tfp_vec_next = innovation_vec[:, -2]\n",
    "        delta_vec_next = innovation_vec[:, -1]\n",
    "\n",
    "        K_vec_next = torch.sum(h_matrix_next, 1)\n",
    "\n",
    "        ## Exogeneous labour supply. One first period, 0, else\n",
    "        l_matrix_next = torch.zeros_like(h_matrix_next)\n",
    "        l_matrix_next[:, 0] = 1\n",
    "        ## Total labour supply (useless calculations, but then we can generalize the code)\n",
    "        L_vec_next = torch.sum(l_matrix_next, 1)\n",
    "\n",
    "        ## get factor prices (wages and interest rate)\n",
    "        r_next = interest_rate(K_vec_next, L_vec_next, delta_vec_next, tfp_vec_next, params)\n",
    "        w_next = wage(K_vec_next, L_vec_next, tfp_vec_next, params)\n",
    "\n",
    "        ## calculate wealth, before consumption decision made\n",
    "        wealth_next = h_matrix_next*r_next.view(-1,1) + l_matrix_next*w_next.view(-1,1)\n",
    "\n",
    "        ## Consumption curent period\n",
    "        if debug == False:\n",
    "            c_next = model_normalized(neural_net, wealth_next, params)\n",
    "        else:\n",
    "            a_next = wealth_next*params.mult_wealth.view(1, -1)\n",
    "            c_next = wealth_next - a_next\n",
    "\n",
    "        # Each column is the euler equation for one agent\n",
    "        # rows are observations\n",
    "        #s = c_next[:, 1:params.nb_agents].shape\n",
    "        #print(f\"shape c next: {s}\")\n",
    "        u_prime_next = params.u_prime(c_next)\n",
    "        #u_prime_next = c_next**(-params.gamma) \n",
    "\n",
    "        # Calculate beta (u'-1){E[r_{t+1} u'(c_{t+1})]}\n",
    "        vals = u_prime_next[:, 1:params.nb_agents]*r_next.view(-1,1)\n",
    "        #print(vals.shape)\n",
    "        # Reshape matrix (MN, nb_agents) to a single column array of size (nb_agents*MN, 1)\n",
    "        # First column, then second column, the third column, and so on..\n",
    "        vals_reshaped = vals.t().contiguous().view(-1, 1)\n",
    "        \n",
    "        #print(vals_reshaped.shape)\n",
    "        #print(W.shape)\n",
    "        #torch.sparse.mm(W, vals_reshaped)\n",
    "        u_prime_inverse = (params.beta*torch.sparse.mm(W, vals_reshaped))**(-1.0/params.gamma)\n",
    "        #print(u_prime_inverse.shape)\n",
    "        \n",
    "        # Euler equation error\n",
    "        c_reshaped = c[:, 0:params.nb_agents-1].t().contiguous().view(-1, 1)\n",
    "        EEE = (u_prime_inverse/c_reshaped) - 1\n",
    "        \n",
    "    return EEE.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743fc8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grad_norm(parameters, norm_type=2.0):\n",
    "    \"\"\"\n",
    "    Compute norm over gradients of model parameters.\n",
    "\n",
    "    :param parameters:\n",
    "        the model parameters for gradient norm calculation. Iterable of\n",
    "        Tensors or single Tensor\n",
    "    :param norm_type:\n",
    "        type of p-norm to use\n",
    "\n",
    "    :returns:\n",
    "        the computed gradient norm\n",
    "    \"\"\"\n",
    "    if isinstance(parameters, torch.Tensor):\n",
    "        parameters = [parameters]\n",
    "    parameters = [p for p in parameters if p is not None and p.grad is not None]\n",
    "    total_norm = 0\n",
    "    for p in parameters:\n",
    "        param_norm = p.grad.data.norm(norm_type)\n",
    "        total_norm += param_norm.item() ** norm_type\n",
    "    return total_norm ** (1.0 / norm_type) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4fe858",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimizer(model, optimizer_name, lr, momentum):\n",
    "    \"\"\"\n",
    "    Function to create an optimizer\n",
    "    \"\"\"\n",
    "    if optimizer_name == \"Adam\":\n",
    "        #optimizer = torch.optim.Adam(model.parameters(), lr=lr, eps=1e-07, betas=(0.9, 0.999)) \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr) \n",
    "    elif optimizer_name == \"SGD\":\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr)\n",
    "    elif optimizer_name == \"SGD-momentum\":\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr, momentum)\n",
    "    elif optimizer_name == \"Adadelta\":\n",
    "        optimizer = torch.optim.Adadelta(model.parameters(), lr)\n",
    "    elif optimizer_name == \"RMSprop\":\n",
    "        optimizer = torch.optim.RMSprop(model.parameters(), lr)\n",
    "    #elif optimizer_name == \"LBFGS\":\n",
    "    #    torch.optim.LBFGS(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        raise NameError(f\"optimizer {optimizer_name} unknown\")\n",
    "    return optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fd0f49",
   "metadata": {},
   "source": [
    "## Variance estimation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2079d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Given a model, calculate the current variance of the loss\n",
    "#Brute force\n",
    "def calculate_variance_loss_model(model, params, nb_draws_loss):\n",
    "    model.eval() #eval mode\n",
    "    with torch.no_grad():        \n",
    "        Xms = torch.zeros(nb_draws_loss)\n",
    "        # Loop over realizations of loss function\n",
    "        for (j_index, j) in enumerate(range(0, nb_draws_loss)):\n",
    "            Xms[j] = Ξ_torch_MC(model, params)\n",
    "        # Calculate mean and variance:\n",
    "        var_loss = torch.var(Xms)\n",
    "        mean_loss = torch.mean(Xms)\n",
    "    model.train() #train mode\n",
    "    return var_loss, mean_loss\n",
    "\n",
    "#Given a model, calculate the current variance of the loss\n",
    "\"\"\"\n",
    "def calculate_variance_loss_model_grid(model, params, nb_draws_loss, grid_N, grid_M):\n",
    "    var_loss = torch.zeros(len(grid_N)) #to store results\n",
    "    # Loop over choice of N and M\n",
    "    for (ind, (N_chosen, M_chosen)) in enumerate(zip(grid_N, grid_M)):\n",
    "        # Change M and N\n",
    "        params_local = MyParams(int(N_chosen), int(M_chosen), params.lr, params.pre_train_model,\n",
    "                  params.nb_epochs, params.bc, params.order_gauss,\n",
    "                  params.σ_shocks, params.use_Sobol, params.optimizer, \n",
    "                  params.dim_p, params.grid_depth, \n",
    "                  params.nb_refinements, params.surplus_threshold, \n",
    "                  \"params\", params.n_points_w, params.n_points_grid,\n",
    "                  params.w1, params.w2)\n",
    "\n",
    "        var, mean = calculate_variance_loss_model(model, params_local, nb_draws_loss)\n",
    "        var_loss[ind] = var\n",
    "    return var_loss\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e84dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_variance_gaussian(params, neural_net, nb_draws, d_replacement, grid_M, grid_N, debug=False, tol = torch.tensor([1e-6])):\n",
    "    \"\"\"\n",
    "    Calculate variance of the loss when joint gaussian assumption holds\n",
    "    Use var(f(s_m,e^i_m))\n",
    "    and cov(f(s_m,e^i_m), f(s_m,e^j_m))\n",
    "    Return: variance loss function on grid, var(resid), cov(resid)\n",
    "    \"\"\"\n",
    "    grid_T = torch.tensor(grid_M*grid_N/2)\n",
    "    grid_N = torch.tensor(grid_N)\n",
    "    # Calculate nb of batches required to get the right number of draws\n",
    "    if nb_draws < params.M:\n",
    "        nb_batches_var = params.M\n",
    "    else:\n",
    "        nb_batches_var = int(nb_draws/params.M)\n",
    "\n",
    "    # Calculate variance and covariance\n",
    "    with torch.no_grad(): \n",
    "        ## state vector\n",
    "        state_vec = generate_n_batches(nb_batches_var, d_replacement)\n",
    "        # endo state\n",
    "        #h_matrix = state_vec[:,:-2] #distribution of capital\n",
    "        # exo states\n",
    "        #tfp_vec = state_vec[:, -2]\n",
    "        #delta_vec = state_vec[:, -1]\n",
    "    \n",
    "        ## Get two independent innovation vectors\n",
    "        e_tfp_1, e_delta_1  = simulate_shocks(params, nb_draws)\n",
    "        innovation_vec_1 =  torch.column_stack((e_tfp_1, e_delta_1))\n",
    "        \n",
    "        e_tfp_2, e_delta_2   = simulate_shocks(params, nb_draws)\n",
    "        innovation_vec_2 =  torch.column_stack((e_tfp_2, e_delta_2))\n",
    "        \n",
    "        # residuals for n random grid points under 2 realizations of shocks        \n",
    "        R1 = Residuals_torch(neural_net, state_vec, innovation_vec_1, params, debug, tol)\n",
    "        R2 = Residuals_torch(neural_net, state_vec, innovation_vec_2, params, debug, tol)\n",
    "\n",
    "        ## Age-specific losses\n",
    "        # Reshape to (MN, nb_agents)\n",
    "        #R1_matrix = R1.view(params.nb_agents-1, params.MN).t()\n",
    "        #R2_matrix = R2.view(params.nb_agents-1, params.MN).t()\n",
    "        #R1_mean = torch.mean(R1_matrix, axis=0) #mean by age group\n",
    "        #R2_mean = torch.mean(R2_matrix, axis=0) #mean by age group\n",
    "        # Construct combinations\n",
    "        # mean\n",
    "        ##mean_val = 0.5*torch.mean(R1_mean) + 0.5*torch.mean(R2_mean)\n",
    "        ## Var\n",
    "        #var_R1 = torch.var(R1_matrix, axis=0) #var by age group\n",
    "        #var_R2 = torch.var(R2_matrix, axis=0) #var by age group\n",
    "        #var_val = 0.5*var_R1 + 0.5*var_R2\n",
    "\n",
    "        # Construct combinations\n",
    "        # mean\n",
    "        mean_val = 0.5*torch.mean(R1) + 0.5*torch.mean(R2)\n",
    "        \n",
    "        ## Var\n",
    "        var_R1 = torch.var(R1)\n",
    "        var_R2 = torch.var(R2)\n",
    "        var_val = 0.5*var_R1 + 0.5*var_R2\n",
    "        \n",
    "        ## Cov\n",
    "        cov_val = torch.cov(torch.column_stack((R1, R2)).T)[0,1]\n",
    "        \n",
    "        var_L = (1/(grid_T*(grid_N - 1)))*((grid_N**2 - 3*grid_N + 3)*(cov_val**2) + (2*(grid_N - 2)*cov_val + var_val)*var_val + 2*(grid_N - 1)*(var_val + (grid_N - 1)*cov_val)*(mean_val**2))\n",
    "        \n",
    "        return var_L, var_val, cov_val\n",
    "    \n",
    "def calculate_variance_loss_fast(params, neural_net, nb_draws, d_replacement, grid_M, grid_N, debug=False, tol = torch.tensor([1e-6])):\n",
    "    \"\"\"\n",
    "    Calculate variance of the loss using proposition appendix\n",
    "    Use four independent shocks \n",
    "    \"\"\"\n",
    "    grid_T = torch.tensor(grid_M*grid_N/2)\n",
    "    grid_N = torch.tensor(grid_N)\n",
    "    \n",
    "    # Calculate nb of batches required to get the right number of draws\n",
    "    if nb_draws < params.M:\n",
    "        nb_batches_var = params.M\n",
    "    else:\n",
    "        nb_batches_var = int(nb_draws/params.M)\n",
    "        \n",
    "    # Calculate variance and covariance\n",
    "    with torch.no_grad():            \n",
    "        # state vector\n",
    "        state_vec = generate_n_batches(nb_batches_var, d_replacement)\n",
    "\n",
    "        # innovation vectors\n",
    "        e_tfp_1, e_delta_1  = simulate_shocks(params, nb_draws)\n",
    "        innovation_vec_1 =  torch.column_stack((e_tfp_1, e_delta_1))\n",
    "        \n",
    "        e_tfp_2, e_delta_2   = simulate_shocks(params, nb_draws)\n",
    "        innovation_vec_2 =  torch.column_stack((e_tfp_2, e_delta_2))\n",
    "        \n",
    "        e_tfp_3, e_delta_3  = simulate_shocks(params, nb_draws)\n",
    "        innovation_vec_3 =  torch.column_stack((e_tfp_3, e_delta_3))\n",
    "        \n",
    "        e_tfp_4, e_delta_4   = simulate_shocks(params, nb_draws)\n",
    "        innovation_vec_4 =  torch.column_stack((e_tfp_4, e_delta_4))\n",
    "        \n",
    "        # residuals for n random grid points under 2 realizations of shocks\n",
    "        R1 = Residuals_torch(neural_net, state_vec, innovation_vec_1, params, debug, tol)\n",
    "        R2 = Residuals_torch(neural_net, state_vec, innovation_vec_2, params, debug, tol)\n",
    "        R3 = Residuals_torch(neural_net, state_vec, innovation_vec_3, params, debug, tol)\n",
    "        R4 = Residuals_torch(neural_net, state_vec, innovation_vec_4, params, debug, tol)\n",
    "\n",
    "        # Construct combinations\n",
    "        R1_R2 = R1*R2\n",
    "        R1_R3 = R1*R3\n",
    "        R1_R4 = R1*R4\n",
    "        R2_R3 = R2*R3\n",
    "        R2_R4 = R2*R4\n",
    "        R3_R4 = R3*R4\n",
    "\n",
    "        # Variance cross\n",
    "        var_R1_R2 = torch.var(R1_R2)\n",
    "\n",
    "        # Co-variances with one shared element\n",
    "        cov_R1R2_R1R3 = torch.cov(torch.column_stack((R1_R2, R1_R3)).T)[0,1]\n",
    "\n",
    "        # Covariances with four different terms\n",
    "        cov_R1R2_R3R4 = torch.cov(torch.column_stack((R1_R2, R3_R4)).T)[0,1]\n",
    "\n",
    "        var_L = (1/(grid_T*(grid_N - 1)))*(var_R1_R2 + 2*(grid_N - 2)*(cov_R1R2_R1R3) + 2*((grid_N*(grid_N - 1)/4) - grid_N + 3/2)*(cov_R1R2_R3R4))\n",
    "\n",
    "    return var_L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f1d441",
   "metadata": {},
   "source": [
    "## Logging and other utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9e0121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_flat(a):\n",
    "    \"\"\"\n",
    "    Function to flatten a list\n",
    "    \"\"\"\n",
    "    return list(np.array(a).flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60ff7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSystemInfo():\n",
    "    \"\"\"\n",
    "    Get info on computer hardware\n",
    "    \"\"\"\n",
    "    try:\n",
    "        info={}\n",
    "        info['platform']=platform.system()\n",
    "        info['platform-release']=platform.release()\n",
    "        info['platform-version']=platform.version()\n",
    "        info['architecture']=platform.machine()\n",
    "        info['hostname']=socket.gethostname()\n",
    "        info['ip-address']=socket.gethostbyname(socket.gethostname())\n",
    "        info['mac-address']=':'.join(re.findall('..', '%012x' % uuid.getnode()))\n",
    "        info['processor']=platform.processor()\n",
    "        info['ram']=str(round(psutil.virtual_memory().total / (1024.0 **3)))+\" GB\"\n",
    "        return json.dumps(info)\n",
    "    except Exception as e:\n",
    "        logging.exception(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
