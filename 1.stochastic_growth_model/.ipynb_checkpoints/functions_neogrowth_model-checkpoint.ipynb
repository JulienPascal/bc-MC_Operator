{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccbae77f",
   "metadata": {},
   "source": [
    "# Functions for neogrowth model\n",
    "\n",
    "## Description\n",
    "\n",
    "store functions for the file neogrowth_model.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4173ead7",
   "metadata": {},
   "source": [
    "## I. Time iteration\n",
    "\n",
    "Time iteration code is from QuantEcon website.\n",
    "\n",
    "Source: https://python.quantecon.org/coleman_policy_iter.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03c3969b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def v_star(y, α, β, μ):\n",
    "    \"\"\"\n",
    "    True value function\n",
    "    \"\"\"\n",
    "    c1 = np.log(1 - α * β) / (1 - β)\n",
    "    c2 = (μ + α * np.log(α * β)) / (1 - α)\n",
    "    c3 = 1 / (1 - β)\n",
    "    c4 = 1 / (1 - α * β)\n",
    "    return c1 + c2 * (c3 - c4) + c4 * np.log(y)\n",
    "\n",
    "def σ_star(y, α, β):\n",
    "    \"\"\"\n",
    "    True optimal policy\n",
    "    \"\"\"\n",
    "    return (1 - α * β) * y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddf35cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_growth_data = [\n",
    "    ('α', float64),          # Production parameter\n",
    "    ('β', float64),          # Discount factor\n",
    "    ('μ', float64),          # Shock location parameter\n",
    "    ('s', float64),          # Shock scale parameter\n",
    "    ('grid', float64[:]),    # Grid (array)\n",
    "    ('shocks', float64[:])   # Shock draws (array)\n",
    "]\n",
    "\n",
    "@jitclass(opt_growth_data)\n",
    "class OptimalGrowthModel:\n",
    "\n",
    "    def __init__(self,\n",
    "                α=0.4, \n",
    "                β=0.96, \n",
    "                μ=0,\n",
    "                s=0.5,\n",
    "                grid_max=4,\n",
    "                grid_size=120,\n",
    "                shock_size=250,\n",
    "                seed=1234):\n",
    "\n",
    "        self.α, self.β, self.μ, self.s = α, β, μ, s\n",
    "\n",
    "        # Set up grid\n",
    "        self.grid = np.linspace(1e-5, grid_max, grid_size)\n",
    "\n",
    "        # Store shocks (with a seed, so results are reproducible)\n",
    "        np.random.seed(seed)\n",
    "        self.shocks = np.exp(μ + s * np.random.randn(shock_size))\n",
    "       \n",
    "\n",
    "    def f(self, k):\n",
    "        \"The production function\"\n",
    "        return k**self.α\n",
    "       \n",
    "\n",
    "    def u(self, c):\n",
    "        \"The utility function\"\n",
    "        return np.log(c)\n",
    "\n",
    "    def f_prime(self, k):\n",
    "        \"Derivative of f\"\n",
    "        return self.α * (k**(self.α - 1))\n",
    "\n",
    "\n",
    "    def u_prime(self, c):\n",
    "        \"Derivative of u\"\n",
    "        return 1/c\n",
    "\n",
    "    def u_prime_inv(self, c):\n",
    "        \"Inverse of u'\"\n",
    "        return 1/c\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b85b147c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def euler_diff(c, σ, y, og):\n",
    "    \"\"\"\n",
    "    Set up a function such that the root with respect to c,\n",
    "    given y and σ, is equal to Kσ(y).\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    β, shocks, grid = og.β, og.shocks, og.grid\n",
    "    f, f_prime, u_prime = og.f, og.f_prime, og.u_prime\n",
    "\n",
    "    # First turn σ into a function via interpolation\n",
    "    σ_func = lambda x: interp(grid, σ, x)\n",
    "\n",
    "    # Now set up the function we need to find the root of.\n",
    "    vals = u_prime(σ_func(f(y - c) * shocks)) * f_prime(y - c) * shocks\n",
    "    return u_prime(c) - β * np.mean(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "077c2986",
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def K(σ, og):\n",
    "    \"\"\"\n",
    "    The Coleman-Reffett operator\n",
    "\n",
    "     Here og is an instance of OptimalGrowthModel.\n",
    "    \"\"\"\n",
    "\n",
    "    β = og.β\n",
    "    f, f_prime, u_prime = og.f, og.f_prime, og.u_prime\n",
    "    grid, shocks = og.grid, og.shocks\n",
    "\n",
    "    σ_new = np.empty_like(σ)\n",
    "    for i, y in enumerate(grid):\n",
    "        # Solve for optimal c at y\n",
    "        c_star = brentq(euler_diff, 1e-12, y-1e-12, args=(σ, y, og))[0]\n",
    "        σ_new[i] = c_star\n",
    "\n",
    "    return σ_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ea9ea2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_model_time_iter(model,    # Class with model information\n",
    "                          σ,        # Initial condition\n",
    "                          tol=1e-8,\n",
    "                          max_iter=10000,\n",
    "                          verbose=True,\n",
    "                          print_skip=25):\n",
    "\n",
    "    # Set up loop\n",
    "    i = 0\n",
    "    error = tol + 1\n",
    "\n",
    "    while i < max_iter and error > tol:\n",
    "        σ_new = K(σ, model)\n",
    "        error = np.max(np.abs(σ - σ_new))\n",
    "        i += 1\n",
    "        if verbose and i % print_skip == 0:\n",
    "            print(f\"Error at iteration {i} is {error}.\")\n",
    "        σ = σ_new\n",
    "\n",
    "    if error > tol:\n",
    "        print(\"Failed to converge!\")\n",
    "    elif verbose:\n",
    "        print(f\"\\nConverged in {i} iterations.\")\n",
    "\n",
    "    return σ_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9611c49f",
   "metadata": {},
   "source": [
    "## II. All-in-One\n",
    "\n",
    "$$\\mathcal{L}_2(\\theta) = \\frac{1}{T} \\sum_{t=1}^{T} \\Big( f(s_t,\\epsilon_{1,t}|\\theta) f(s_t,\\epsilon_{2,t}|\\theta) \\Big) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18c8b947",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape) \n",
    "\n",
    "def convert_sparse_matrix_to_sparse_tensor(X):\n",
    "    \"\"\"Convert a scipy sparse matrix to a tf sparse tensor.\"\"\"\n",
    "    coo = X.tocoo()\n",
    "    indices = np.mat([coo.row, coo.col]).transpose()\n",
    "    return tf.SparseTensor(indices, coo.data, coo.shape)\n",
    "\n",
    "# Root mean square error\n",
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        \n",
    "    def forward(self,yhat,y):\n",
    "        return torch.sqrt(self.mse(yhat,y))\n",
    "    \n",
    "# Gaussian quadrature rule\n",
    "# See: https://chaospy.readthedocs.io/en/master/api/chaospy.generate_quadrature.html\n",
    "def dist(order, distribution, rule = \"gaussian\", sp=True):\n",
    "    #order=int(n**(1/d))-1\n",
    "    x, w = chaospy.generate_quadrature(order, distribution, rule=(rule), sparse=sp)\n",
    "    return x, w\n",
    "\n",
    "def create_W_expanded_matrix(M, N):\n",
    "    \"\"\"\n",
    "    create a sparse matrix W_expanded with U repeate M times on the diagonal elements\n",
    "    where U is an upper triangular matrix with 0 on the diagonal and 1 on the other upper elements\n",
    "    W_expanded is a sparse torch matrix\n",
    "    \"\"\"\n",
    "    A_expanded = np.ones((N, N))\n",
    "    U = np.triu(A_expanded) # upper trianguler matrix of ones\n",
    "    np.fill_diagonal(U, 0) #fill diagonal with 0\n",
    "    U = sparse.csr_matrix(U) # convert to sparse\n",
    "    # Unity matrix of size (M*M)\n",
    "    B = sparse.csr_matrix(np.eye(M, M))\n",
    "    W_expanded = sparse_mx_to_torch_sparse_tensor(sparse.kron(B, U))\n",
    "    return W_expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53492fb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Vector' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11712/262410861.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR_squared\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mResiduals_torch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mVector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me_r\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mVector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;31m# vals = u_prime(σ_func(f(y - c) * shocks)) * f_prime(y - c) * shocks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# V1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Vector' is not defined"
     ]
    }
   ],
   "source": [
    "def Ξ_torch(model, params): # objective function if using all-in-one\n",
    "\n",
    "    # randomly drawing current states    \n",
    "    # If normal\n",
    "    #x = torch.normal(mean=0, std=params.σ_x, size=(params.T,)).unsqueeze(1)\n",
    "    #if Uniform\n",
    "    if params.x_distribution == \"Uniform\":\n",
    "        if params.use_Sobol_T == False:\n",
    "            x = ((params.x_low - params.x_high) * torch.rand(params.T) + params.x_high).unsqueeze(1)\n",
    "        else:\n",
    "            #Very slow if T is large\n",
    "            x = ((params.x_low - params.x_high) * params.soboleng.draw(params.T) + params.x_high)\n",
    "    else:\n",
    "        x = torch.normal(mean=0, std=params.σ_x, size=(params.T,)).unsqueeze(1)\n",
    "\n",
    "    # randomly drawing 1st realization for shocks    \n",
    "    if params.e_distribution == \"Normal\":\n",
    "        e1 = torch.normal(mean=0, std=params.σ_e, size=(params.T,)).unsqueeze(1)\n",
    "        # randomly drawing 2nd realization for shocks\n",
    "        e2 = torch.normal(mean=0, std=params.σ_e, size=(params.T,)).unsqueeze(1)\n",
    "    elif params.e_distribution == \"T\":\n",
    "        m = torch.distributions.studentT.StudentT(torch.tensor([3.0]))\n",
    "        e1 = m.sample([params.T])\n",
    "        e2 = m.sample([params.T])\n",
    "    elif params.e_distribution == \"Lognormal\":\n",
    "        # Standard Normal iid shocks\n",
    "        e1 = torch.normal(mean=0, std=params.σ_e, size=(params.T,)).unsqueeze(1)\n",
    "        # randomly drawing 2nd realization for shocks\n",
    "        e2 = torch.normal(mean=0, std=params.σ_e, size=(params.T,)).unsqueeze(1)\n",
    "        # Transform to lognormlaparams.σ_e\n",
    "        e1 = np.exp(params.μ_e + params.σ_e * e1)\n",
    "        e2 = np.exp(params.μ_e + params.σ_e * e2)\n",
    "    else:\n",
    "        raise(\"Distribution unknown.\")\n",
    "    \n",
    "    # residuals for n random grid points under 2 realizations of shocks\n",
    "    R1 = Residuals_torch(model, params, x, e1)\n",
    "    R2 = Residuals_torch(model, params, x, e2)\n",
    "\n",
    "    # construct all-in-one expectation operator\n",
    "    R_squared = R1*R2\n",
    "    \n",
    "    # V1. give a summary of all the draws:\n",
    "    return torch.mean(R_squared)\n",
    "\n",
    "def Residuals_torch(model, params, y: Vector, e_r: Vector):\n",
    "    # consumption today\n",
    "    c = model(y)\n",
    "    LHS = params.u_prime(c)\n",
    "    \n",
    "    # implies some investment\n",
    "    investment = y - c\n",
    "    # investment scaled by shock\n",
    "    state_tomorrow = params.f(investment)*e_r\n",
    "    \n",
    "    # consumption tomorrow\n",
    "    # c_tomorrow = c_share_tomorrow*state_tomorrow \n",
    "    c_tomorrow = model(state_tomorrow)\n",
    "    vals = params.u_prime(c_tomorrow) * params.f_prime(investment)*e_r\n",
    "    RHS = params.β * vals\n",
    "    #V1\n",
    "    #R = (RHS - LHS)\n",
    "    \n",
    "    #V2: good scaling. Default choice.\n",
    "    R = (RHS - LHS)/(0.5*RHS + 0.5*LHS)\n",
    "        \n",
    "    return R\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84919be1",
   "metadata": {},
   "source": [
    "## III. bc-MC operator\n",
    "\n",
    "* Implement the developed formula\n",
    "\n",
    "$$ \\frac{1}{M} \\frac{2}{(N)(N-1)} \\sum_{m=1}^{M} \\sum_{1\\leq i < j}^{n} f(s_m, \\epsilon_{m}^{(i)})f(s_m, \\epsilon_{m}^{(j)})  $$\n",
    "\n",
    "* This formula can be vectorized as follows:\n",
    "\n",
    "\n",
    "$$ f' \\Big(I_N \\otimes U\\Big). f $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "950d4e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ξ_torch_MC(model, params): # objective function\n",
    "\n",
    "    # Monte Carlo outside\n",
    "    # randomly drawing current states  \n",
    "    # if Normal\n",
    "    #x = torch.normal(mean=0, std=params.σ_x, size=(params.M,)).unsqueeze(1)\n",
    "    if params.x_distribution == \"Uniform\":\n",
    "        #if Sobol\n",
    "        if params.use_Sobol == True:\n",
    "            x = ((params.x_low - params.x_high) * params.soboleng.draw(params.M) + params.x_high)\n",
    "        else:\n",
    "            x = ((params.x_low - params.x_high) * torch.rand(params.M) + params.x_high).unsqueeze(1)\n",
    "    else:\n",
    "        x = torch.normal(mean=0, std=params.σ_x, size=(params.M,)).unsqueeze(1)\n",
    "        \n",
    "    # repeat elements N times\n",
    "    #print(x.shape)\n",
    "    x_repeated = x.repeat_interleave(params.N).unsqueeze(1) #MN*1 matrix\n",
    "    \n",
    "    # V1. \n",
    "    # Monte Carlo inside\n",
    "    # N for each value today\n",
    "    # randomly drawing 1st realization for shocks \n",
    "    if params.e_distribution == \"Normal\":\n",
    "        e_shock = torch.normal(mean=0, std=params.σ_e, size=(params.MN,)).unsqueeze(1)\n",
    "    elif params.e_distribution == \"T\":\n",
    "        m = torch.distributions.studentT.StudentT(torch.tensor([3.0]))\n",
    "        e_shock = m.sample([params.MN]).squeeze(1)\n",
    "    elif params.e_distribution == \"Lognormal\":\n",
    "        # Standard Normal iid shocks\n",
    "        e_shock = torch.normal(mean=0, std=params.σ_e, size=(params.MN,)).unsqueeze(1)\n",
    "        # Transform to lognormlaparams.σ_e\n",
    "        e_shock = np.exp(params.μ_e + params.σ_e * e_shock)\n",
    "    else:\n",
    "        raise(\"Distribution unknown.\")\n",
    "        \n",
    "    #V2 For replicability reasons\n",
    "    #Gives exactly the  All-in-One for N =2\n",
    "    #Draw 2 series of shock and reorganize the order of shocks. \n",
    "    \"\"\"\n",
    "    if params.e_distribution == \"Normal\":\n",
    "        e1 = torch.normal(mean=0, std=params.σ_e, size=(params.T,)).unsqueeze(1)\n",
    "        # randomly drawing 2nd realization for shocks\n",
    "        e2 = torch.normal(mean=0, std=params.σ_e, size=(params.T,)).unsqueeze(1)\n",
    "    elif params.e_distribution == \"T\":\n",
    "        m = torch.distributions.studentT.StudentT(torch.tensor([3.0]))\n",
    "        e1 = m.sample([params.T])\n",
    "        e2 = m.sample([params.T])\n",
    "    elif params.e_distribution == \"Lognormal\":\n",
    "        # Standard Normal iid shocks\n",
    "        e1 = torch.normal(mean=0, std=params.σ_e, size=(params.T,)).unsqueeze(1)\n",
    "        # randomly drawing 2nd realization for shocks\n",
    "        e2 = torch.normal(mean=0, std=params.σ_e, size=(params.T,)).unsqueeze(1)\n",
    "        # Transform to lognormlaparams.σ_e\n",
    "        e1 = np.exp(params.μ_e + params.σ_e * e1)\n",
    "        e2 = np.exp(params.μ_e + params.σ_e * e2)\n",
    "    else:\n",
    "        raise(\"Distribution unknown.\")\n",
    "    \n",
    "    e_shock = torch.column_stack((e1, e2)).reshape(params.MN, 1) #recombine the 2 series of shocks\n",
    "    \"\"\"\n",
    "    \n",
    "    # residuals for n random grid points under 2 realizations of shocks\n",
    "    R1 = Residuals_torch(model, params, x_repeated, e_shock).squeeze(1)\n",
    "    R_squared = torch.mean((2/((params.M)*(params.N)*(params.N - 1)))*torch.matmul(R1.unsqueeze(1).t(), torch.matmul(params.W_expanded, R1.unsqueeze(1))))\n",
    "   \n",
    "    return R_squared \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62305d2b",
   "metadata": {},
   "source": [
    "## Optimal choice of training parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb084d7",
   "metadata": {},
   "source": [
    "### Monte Carlo estimator: optimal choice of M and N\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "743fc8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grad_norm(parameters, norm_type=2.0):\n",
    "    \"\"\"\n",
    "    Compute norm over gradients of model parameters.\n",
    "\n",
    "    :param parameters:\n",
    "        the model parameters for gradient norm calculation. Iterable of\n",
    "        Tensors or single Tensor\n",
    "    :param norm_type:\n",
    "        type of p-norm to use\n",
    "\n",
    "    :returns:\n",
    "        the computed gradient norm\n",
    "    \"\"\"\n",
    "    if isinstance(parameters, torch.Tensor):\n",
    "        parameters = [parameters]\n",
    "    parameters = [p for p in parameters if p is not None and p.grad is not None]\n",
    "    total_norm = 0\n",
    "    for p in parameters:\n",
    "        param_norm = p.grad.data.norm(norm_type)\n",
    "        total_norm += param_norm.item() ** norm_type\n",
    "    return total_norm ** (1.0 / norm_type) \n",
    "\n",
    "# Calculate the variance of the loss for several choices of M and N\n",
    "# Start with a fresh model. Then train it for some iterations.\n",
    "# Then calculate variance of loss\n",
    "def calculate_variance_loss(params, nb_epoch_opt_M_N, nb_rep, \n",
    "                            nb_draws_loss, grid_M, grid_N,\n",
    "                            norm_chosen = 2.0, freq_accuracy=1000, \n",
    "                            calculate_variance_gradient = False,\n",
    "                            initial_guess = [1.0, 1.0]):\n",
    "    # To store results\n",
    "    df_variance_loss = pd.DataFrame()\n",
    "    print(\"Norm chosen: {}\".format(norm_chosen))\n",
    "    \n",
    "    for k in range(0, nb_rep):\n",
    "        #-----------------------------------------\n",
    "        # A. Train the model for nb_epoch_opt_M_N\n",
    "        #-----------------------------------------\n",
    "        # Initialize a network\n",
    "        print(\"Rep {} / {}. Training a model for {} Iterations\".format(int(k), nb_rep, nb_epoch_opt_M_N))\n",
    "        model_opt_M_N = NeuralNetwork().to(device) # New model\n",
    "        \n",
    "        # Set initial value\n",
    "        set_initial_values(model_opt_M_N, initial_guess[0], initial_guess[1])\n",
    "        \n",
    "        # Training mode\n",
    "        model_opt_M_N.train()\n",
    "\n",
    "        if params.optimizer == \"Adam\":\n",
    "            optimizer = torch.optim.Adam(model_opt_M_N.parameters(), lr=params.lr, eps=1e-07, betas=(0.9, 0.999)) \n",
    "        elif params.optimizer == \"SGD\":\n",
    "            optimizer = torch.optim.SGD(model_opt_M_N.parameters(), params.lr)\n",
    "        elif params.optimizer == \"SWA\":\n",
    "            base_opt = torch.optim.Adam(model_opt_M_N.parameters(), lr=params.lr, eps=1e-07, betas=(0.9, 0.999)) \n",
    "            optimizer = SWA(base_opt, swa_start=params.swa_start, swa_freq=params.swa_freq, swa_lr=params.lr)\n",
    "        else:\n",
    "            raise(\"optimizer unknown\")\n",
    "\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=params.freq_gamma)\n",
    "        list_perc_abs_error_opt_M_N = [] #store abs value percentage error\n",
    "        list_perc_abs_error_opt_M_N_i = []\n",
    "        loss_epochs_opt_M_N = torch.zeros(nb_epoch_opt_M_N)\n",
    "\n",
    "        for i in range(0, nb_epoch_opt_M_N):\n",
    "            \n",
    "            optimizer.zero_grad() #clear gradient\n",
    "            \n",
    "            loss = Ξ_torch_MC(model_opt_M_N, params) #loss \n",
    "            loss_epochs_opt_M_N[[i]] = float(loss.item())\n",
    "\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if i % freq_accuracy == 0: #Monitor the predictive power\n",
    "                # Define the grid\n",
    "                with torch.no_grad():\n",
    "                    xvec = params.xvec_test_torch #add a dimension\n",
    "                    y = model_opt_M_N(xvec) \n",
    "                xvec = xvec.detach().numpy()\n",
    "                y = y.detach().numpy()\n",
    "                perc_abs_error = 100*(np.abs((y - params.true_function(xvec))/params.true_function(xvec)))\n",
    "                list_perc_abs_error_opt_M_N.append(np.median(perc_abs_error))\n",
    "                list_perc_abs_error_opt_M_N_i.append(i)\n",
    "            if i % 1000 == 0:\n",
    "                loss, current = float(loss.item()), i\n",
    "                print(f\"loss: {loss:>7f}, median percentage euler error {list_perc_abs_error_opt_M_N[-1]:>7f}, [{current:>5d}/{nb_epoch_opt_M_N:>5d}]\")\n",
    "            if (i % params.freq_scheduler == 0) & (i != 0) & (params.use_scheduler == True):\n",
    "                scheduler.step()\n",
    "                print(\"i : {}. Decreasing learning rate: {}\".format(i, scheduler.get_last_lr()))\n",
    "\n",
    "        if params.optimizer == \"SWA\":\n",
    "            optimizer.swap_swa_sgd()\n",
    "        #--------------------------------------\n",
    "        # B. Calculate the variance of the loss\n",
    "        #--------------------------------------\n",
    "        print(\"Rep {} / {}. Calculting variance loss for {} Iterations\".format(int(k), nb_rep, nb_draws_loss))\n",
    "    \n",
    "        model_opt_M_N.eval()\n",
    "        var_loss = torch.zeros(len(grid_N))\n",
    "        std_loss = torch.zeros(len(grid_N))\n",
    "        mean_loss = torch.zeros(len(grid_N))\n",
    "\n",
    "        # Loop over choice of N and M\n",
    "        for (ind, (N_chosen, M_chosen)) in enumerate(zip(grid_N.astype('int'), grid_M.astype('int'))):\n",
    "            # Change M and N\n",
    "            params_local = MyParams(N_chosen, M_chosen, params.lr, \n",
    "                                    params.pre_train_model, params.nb_epochs,\n",
    "                                    params.order_gauss, params.σ_e, params.use_Sobol, \n",
    "                                    params.optimizer)\n",
    "\n",
    "            with torch.no_grad():        \n",
    "                Xms = torch.zeros(nb_draws_loss)\n",
    "                # Loop over realizations of loss function\n",
    "                for (j_index, j) in enumerate(range(0, nb_draws_loss)):\n",
    "                    Xms[j] = Ξ_torch_MC(model_opt_M_N, params_local)\n",
    "                # Calculate mean and variance:\n",
    "                var_loss[ind] = torch.var(Xms)\n",
    "                std_loss[ind] = torch.sqrt(var_loss[ind])\n",
    "                mean_loss[ind] = torch.mean(Xms)\n",
    "        #-----------------------------------------------\n",
    "        # C. Calculate variance of the norm the gradient\n",
    "        #-----------------------------------------------\n",
    "        print(\"Rep {} / {}. Calculting variance norm gradient for {} Iterations\".format(int(k), nb_rep, nb_draws_loss))\n",
    "        var_gradient_loss = torch.zeros(len(grid_N )) #variance \n",
    "        std_gradient_loss = torch.zeros(len(grid_N )) #standard deviation\n",
    "        mean_gradient_loss = torch.zeros(len(grid_N ))\n",
    "        \n",
    "        if calculate_variance_gradient == True:\n",
    "            for (ind, (N_chosen, M_chosen)) in enumerate(zip(grid_N.astype('int'), grid_M.astype('int'))):\n",
    "                \n",
    "                params_local = MyParams(N_chosen, M_chosen, params.lr, \n",
    "                        params.pre_train_model, params.nb_epochs,\n",
    "                        params.order_gauss, params.σ_e, params.use_Sobol, \n",
    "                        params.optimizer)\n",
    "    \n",
    "                Xms = torch.zeros(nb_draws_loss)\n",
    "                # Loop over draws \n",
    "                for (j_index, j) in enumerate(range(0, nb_draws_loss)):\n",
    "                    optimizer.zero_grad() # clear gradient\n",
    "                    loss = Ξ_torch_MC(model_opt_M_N, params_local)\n",
    "                    loss.backward() #calculate gradient\n",
    "                    # Store the norm of the gradient\n",
    "                    total_norm = compute_grad_norm(model_opt_M_N.parameters(), norm_type=norm_chosen)\n",
    "                    Xms[j] = total_norm\n",
    "                var_gradient_loss[ind] = torch.var(Xms)\n",
    "                std_gradient_loss[ind] = torch.sqrt(var_gradient_loss[ind])\n",
    "                mean_gradient_loss[ind] = torch.mean(Xms)\n",
    "\n",
    "        # Save to dataframe\n",
    "        if df_variance_loss.empty == True:\n",
    "            df_variance_loss = pd.DataFrame({'N': grid_N,\n",
    "                                  'M': grid_M,\n",
    "                                  'var_loss': var_loss,\n",
    "                                  'std_loss': std_loss,\n",
    "                                  'mean_loss': mean_loss,\n",
    "                                  'var_gradient_loss': var_gradient_loss,\n",
    "                                  'std_gradient_loss': std_gradient_loss,\n",
    "                                  'mean_gradient_loss': mean_gradient_loss,\n",
    "                                  'nb_rep': k})\n",
    "        else:\n",
    "            df_variance_loss_bis = pd.DataFrame({'N': grid_N,\n",
    "                                  'M': grid_M,\n",
    "                                  'var_loss': var_loss,\n",
    "                                  'std_loss': std_loss,\n",
    "                                  'mean_loss': mean_loss,\n",
    "                                  'var_gradient_loss': var_gradient_loss,\n",
    "                                  'std_gradient_loss': std_gradient_loss,\n",
    "                                  'mean_gradient_loss': mean_gradient_loss,\n",
    "                                  'nb_rep': k})\n",
    "            df_variance_loss = pd.concat([df_variance_loss, df_variance_loss_bis])\n",
    "        \n",
    "    # Replace NAN by large values. Need to penalize non convergence of gradient descent.\n",
    "    list_cols = [\"var_loss\", \"std_loss\", \"mean_loss\", \n",
    "                \"std_gradient_loss\", \"var_gradient_loss\", \"mean_gradient_loss\"]\n",
    "    \n",
    "    for col in list_cols:\n",
    "        df_variance_loss[col] = df_variance_loss[col].fillna(np.nanmax(df_variance_loss[col]))\n",
    "    \n",
    "    # Stats on df_var\n",
    "    # Median values\n",
    "    df_variance_loss_median = df_variance_loss.groupby('M').median().reset_index()\n",
    "\n",
    "    for col in list_cols:\n",
    "        df_variance_loss_median[\"min_\" + col] = df_variance_loss.groupby('M')[col].min().reset_index()[col]                     \n",
    "        df_variance_loss_median[\"max_\" + col] = df_variance_loss.groupby('M')[col].max().reset_index()[col]\n",
    "        df_variance_loss_median[\"std_\" + col] = df_variance_loss.groupby('M')[col].std().reset_index()[col]\n",
    "        for qq in [10, 25, 50, 75, 90]:\n",
    "            df_variance_loss_median[\"P\" + str(qq) + \"_\" + col] = df_variance_loss.groupby('M')[col].quantile(qq/100).reset_index()[col]\n",
    "    \n",
    "\n",
    "    return df_variance_loss, df_variance_loss_median\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9859c496",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3925264104.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_7889/3925264104.py\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    def calculate_variance_loss_model(model, params, nb_draws_loss)\u001b[0m\n\u001b[0m                                                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#Given a model, calculate the current variance of the loss\n",
    "def calculate_variance_loss_model(model, params, nb_draws_loss):\n",
    "    model.eval() #eval mode\n",
    "    with torch.no_grad():        \n",
    "        Xms = torch.zeros(nb_draws_loss)\n",
    "        # Loop over realizations of loss function\n",
    "        for (j_index, j) in enumerate(range(0, nb_draws_loss)):\n",
    "            Xms[j] = Ξ_torch_MC(model, params)\n",
    "        # Calculate mean and variance:\n",
    "        var_loss = torch.var(Xms)\n",
    "        mean_loss = torch.mean(Xms)\n",
    "    train() #train mode\n",
    "    return var_loss, mean_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b0667f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimizer(model, optimizer_name, lr, momentum):\n",
    "    \"\"\"\n",
    "    Function to create an optimizer\n",
    "    \"\"\"\n",
    "    if optimizer_name == \"Adam\":\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, eps=1e-07, betas=(0.9, 0.999)) \n",
    "    elif optimizer_name == \"SGD\":\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr)\n",
    "    elif optimizer_name == \"SGD-momentum\":\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr, momentum)\n",
    "    elif optimizer_name == \"Adadelta\":\n",
    "        optimizer = torch.optim.Adadelta(model.parameters(), lr)\n",
    "    elif optimizer_name == \"RMSprop\":\n",
    "        optimizer = torch.optim.RMSprop(model.parameters(), lr)\n",
    "    else:\n",
    "        raise(\"optimizer unknown\")\n",
    "    return optimizer\n",
    "\n",
    "def set_initial_values(model, w, b):\n",
    "    \"\"\"\n",
    "    Function to analyse the weigth and bias to certain values\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'linear_relu_stack.0.weight' in name:\n",
    "                param.copy_(torch.tensor([w]))\n",
    "            elif 'linear_relu_stack.0.bias' in name:\n",
    "                param.copy_(torch.tensor([b]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "05821e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve a model several times, holding initial parameters constant\n",
    "def calculate_several_runs(params, nb_rep, freq_loss = 1, initial_guess = [1.0, 1.0]):\n",
    "    #initialization of lists\n",
    "    list_elapsed_time = []\n",
    "    list_M = []\n",
    "    list_N = []\n",
    "    list_list_losses = [] \n",
    "    list_list_perc_abs_error_MC = [] \n",
    "    list_list_perc_abs_error_MC_i = [] \n",
    "    list_list_perc_abs_error_MC_loss = [] \n",
    "    list_lr = []\n",
    "    list_index_rep = []\n",
    "    list_list_beta = [] #store coefficients\n",
    "    list_list_abs_perc_dev_bias = [] #absolute percentage deviation from true bias \n",
    "    list_list_abs_perc_dev_weight = [] #absolute percentage deviation from true weight\n",
    "            \n",
    "    # A.\n",
    "    # Run seral times\n",
    "    for k in range(0, nb_rep):\n",
    "            print(\"Training model : {} / {}\".format(int(k), nb_rep))\n",
    "            list_index_rep.append(k)\n",
    "            list_M.append(params.M)\n",
    "            list_N.append(params.N)\n",
    "            list_lr.append(params.lr)\n",
    "\n",
    "            #-----------------------------------------------------\n",
    "            # Train a model for some periods\n",
    "            # Then look at the expected variance of the gradient\n",
    "            #-----------------------------------------------------\n",
    "            # Initialize a network\n",
    "            model_MC = NeuralNetwork().to(device) \n",
    "\n",
    "            # Set initial value\n",
    "            set_initial_values(model_MC, initial_guess[0], initial_guess[1])\n",
    "        \n",
    "            if params.optimizer == \"Adam\":\n",
    "                optimizer_MC = torch.optim.Adam(model_MC.parameters(), lr=params.lr, eps=1e-07, betas=(0.9, 0.999)) \n",
    "            elif params.optimizer == \"SGD\":\n",
    "                optimizer_MC = torch.optim.SGD(model_MC.parameters(), params.lr)\n",
    "            elif params.optimizer == \"SWA\":\n",
    "                base_opt_MC = torch.optim.Adam(model_MC.parameters(), lr=params.lr, eps=1e-07, betas=(0.9, 0.999)) \n",
    "                optimizer_MC = SWA(base_opt_MC, swa_start=params.swa_start, swa_freq=params.swa_freq, swa_lr=params.lr)\n",
    "            else:\n",
    "                raise(\"optimizer unknown\")\n",
    "\n",
    "            scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer_MC, gamma=params.freq_gamma)\n",
    "            loss_epochs_MC = torch.zeros(params.nb_epochs)\n",
    "            list_perc_abs_error_MC = [] #store abs value percentage error\n",
    "            list_perc_abs_error_MC_i = [] #store index i\n",
    "            list_perc_abs_error_MC_loss = [] #store loss\n",
    "            list_beta = [] #store coefficients\n",
    "            list_abs_perc_dev_bias = [] #np.zeros(params.nb_epochs) #absolute percentage deviation from true bias \n",
    "            list_abs_perc_dev_weight = [] #np.zeros(params.nb_epochs) #absolute percentage deviation from true weight\n",
    "            \n",
    "            for i in range(0, params.nb_epochs):\n",
    "                \n",
    "                optimizer_MC.zero_grad() #clear gradient\n",
    "                \n",
    "                loss = Ξ_torch_MC(model_MC, params) #calculate loss\n",
    "                loss_epochs_MC[[i]] = float(loss.item())\n",
    "                \n",
    "                # Store coefficients\n",
    "                if i % freq_loss == 0:\n",
    "                    with torch.no_grad():\n",
    "                        # Extract weight and bias\n",
    "                        b_current = np.array([k.item() for k in model_MC.parameters()])\n",
    "                        b_current_ordered = np.array((b_current[1], b_current[0])) #reorder (bias, weight)\n",
    "                    list_beta.append(b_current_ordered)\n",
    "\n",
    "                    # Calculate the absolute percentage deviation from true value:\n",
    "                    ## Bias: b_current_ordered[0]\n",
    "                    with torch.no_grad():\n",
    "                        dev_b = 100*(np.abs((b_current_ordered[0] - params.true_bias)/params.true_bias))\n",
    "                        #list_abs_perc_dev_bias[i] = dev_b #absolute percentage deviation from true bias \n",
    "                        list_abs_perc_dev_bias.append(dev_b[0])\n",
    "                        ## Cannot divide by 0\n",
    "                        dev_w = 100*(np.abs((b_current_ordered[1] - params.true_weight)))\n",
    "                        #list_abs_perc_dev_weight[i] = dev_w #absolute percentage deviation from true weight\n",
    "                        list_abs_perc_dev_weight.append(dev_w)\n",
    "                        \n",
    "                # Backpropagation\n",
    "                loss.backward()\n",
    "                \n",
    "                # Gradient descent step\n",
    "                optimizer_MC.step()\n",
    "\n",
    "                if i % freq_loss == 0: #Monitor the predictive power\n",
    "                    # Define the grid\n",
    "                    with torch.no_grad():\n",
    "                        xvec = params.xvec_test_torch #add a dimension\n",
    "                        y_MC = model_MC(xvec)\n",
    "                    xvec = xvec.detach().numpy()\n",
    "                    y_MC = y_MC.detach().numpy()\n",
    "                    perc_abs_error_MC = 100*(np.abs((y_MC - params.true_function(xvec))/params.true_function(xvec)))\n",
    "                    list_perc_abs_error_MC.append(np.median(perc_abs_error_MC))\n",
    "                    list_perc_abs_error_MC_i.append(i)\n",
    "                    list_perc_abs_error_MC_loss.append(float(loss.item()))\n",
    "                if i % 1000 == 0:\n",
    "                    loss, current = float(loss.item()), i\n",
    "                    print(f\"loss: {loss:>7f} [{current:>5d}/{params.nb_epochs:>5d}]\")\n",
    "                if (i % params.freq_scheduler == 0) & (i != 0) & (params.use_scheduler == True):\n",
    "                    scheduler.step()\n",
    "                    print(\"i : {}. Decreasing learning rate: {}\".format(i, scheduler.get_last_lr()))\n",
    "                    print(f\"loss: {loss:>7f}, median percentage error {list_perc_abs_error_MC[-1]:>7f}, [{current:>5d}/{params.nb_epochs:>5d}]\")\n",
    "\n",
    "\n",
    "            list_list_losses.append(list_perc_abs_error_MC_loss) #list_list_losses.append(loss_epochs_MC)\n",
    "            list_list_perc_abs_error_MC.append(list_perc_abs_error_MC)\n",
    "            list_list_perc_abs_error_MC_i.append(list_perc_abs_error_MC_i)\n",
    "            list_list_perc_abs_error_MC_loss.append(list_perc_abs_error_MC_loss)\n",
    "            list_list_beta.append(list_beta)\n",
    "            list_list_abs_perc_dev_bias.append(list_abs_perc_dev_bias) #absolute percentage deviation from true bias \n",
    "            list_list_abs_perc_dev_weight.append(list_abs_perc_dev_weight) #absolute percentage deviation from true weight\n",
    "       \n",
    "            if params.optimizer == \"SWA\":\n",
    "                optimizer_MC.swap_swa_sgd()\n",
    "                \n",
    "    # B. Compile results\n",
    "    #-------------------\n",
    "    df_MC = pd.DataFrame({'M': list_M[0],\n",
    "                        'N': list_N[0],\n",
    "                        'repetition': list_index_rep[0],\n",
    "                        'iter': list_list_perc_abs_error_MC_i[0],\n",
    "                        'loss': np.sqrt(np.abs(list_list_losses[0])),\n",
    "                        'med_percentage_error': list_list_perc_abs_error_MC[0],\n",
    "                        'intercept': np.array(list_list_beta[0])[:,0],\n",
    "                        'slope': np.array(list_list_beta[0])[:,1],\n",
    "                        'abs_perc_dev_bias': list_list_abs_perc_dev_bias[0], #absolute percentage deviation from true bias \n",
    "                        'abs_perc_dev_weight': list_list_abs_perc_dev_weight[0] #absolute percentage deviation from true weight\n",
    "       })\n",
    "\n",
    "    for k in range(0, len(list_list_perc_abs_error_MC)):\n",
    "        df_MC_bis = pd.DataFrame({'M': list_M[k],\n",
    "                                'N': list_N[k],\n",
    "                                'repetition': list_index_rep[k],\n",
    "                                'iter': list_list_perc_abs_error_MC_i[0],\n",
    "                                'loss': np.sqrt(np.abs(list_list_losses[k])),\n",
    "                                'med_percentage_error': list_list_perc_abs_error_MC[k],\n",
    "                                'intercept': np.array(list_list_beta[k])[:,0],\n",
    "                                'slope': np.array(list_list_beta[k])[:,1],\n",
    "                                'abs_perc_dev_bias': list_list_abs_perc_dev_bias[k], #absolute percentage deviation from true bias \n",
    "                                'abs_perc_dev_weight': list_list_abs_perc_dev_weight[k]}) #absolute percentage deviation from true weight)\n",
    "        df_MC = pd.concat([df_MC, df_MC_bis])\n",
    "    \n",
    "    # Replace NAN by large values\n",
    "    for col in [\"loss\", \"med_percentage_error\", \"intercept\", \"slope\"]:\n",
    "        df_MC[col] = df_MC[col].fillna(np.nanmax(df_MC[col]))\n",
    "    \n",
    "    # C. Statistics on results\n",
    "    df_MC_average = df_MC.groupby('iter').mean().reset_index() #mean value by iteration\n",
    "\n",
    "\n",
    "    list_cols = [\"loss\", \"med_percentage_error\", \n",
    "                 \"intercept\", \"slope\",\n",
    "                 \"abs_perc_dev_bias\", \"abs_perc_dev_weight\"]\n",
    "    \n",
    "    for col in list_cols:\n",
    "        df_MC_average[\"min_\" + col] = df_MC.groupby('iter')[col].min().reset_index()[col]                     \n",
    "        df_MC_average[\"max_\" + col] = df_MC.groupby('iter')[col].max().reset_index()[col]\n",
    "        df_MC_average[\"std_\" + col] = df_MC.groupby('iter')[col].std().reset_index()[col]\n",
    "        for qq in [1, 5, 10, 25, 50, 75, 90, 95, 99]:\n",
    "            df_MC_average[\"P\" + str(qq) + \"_\" + col] = df_MC.groupby('iter')[col].quantile(qq/100).reset_index()[col]\n",
    "\n",
    "    # Add extra info\n",
    "    df_MC_average['optimizer'] = params.optimizer\n",
    "    df_MC_average['sigma_e'] = params.σ_e\n",
    "    df_MC_average['Sobol'] = params.use_Sobol\n",
    "    df_MC_average['user_scheduler'] = params.use_scheduler\n",
    "    df_MC_average['gamma_scheduler'] = params.freq_gamma\n",
    "    df_MC_average['lr'] = params.lr\n",
    "    \n",
    "    return df_MC, df_MC_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50bcd999",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_variance_loss_fast(params, model, nb_draws, grid_M, grid_N):\n",
    "    \"\"\"\n",
    "    Calculate variance of the loss using proposition 4\n",
    "    Use four independent shocks \n",
    "    \"\"\"\n",
    "    grid_T = torch.tensor(grid_M*grid_N/2)\n",
    "    grid_N = torch.tensor(grid_N)\n",
    "    \n",
    "    # Calculate variance and covariance\n",
    "    with torch.no_grad(): \n",
    "        # V1\n",
    "        if params.x_distribution == \"Uniform\":\n",
    "            if params.use_Sobol_T == False:\n",
    "                x = ((params.x_low - params.x_high) * torch.rand(nb_draws) + params.x_high).unsqueeze(1)\n",
    "            else:\n",
    "                #Very slow if T is large\n",
    "                x = ((params.x_low - params.x_high) * params.soboleng.draw(nb_draws) + params.x_high)\n",
    "        else:\n",
    "            x = torch.normal(mean=0, std=params.σ_x, size=(nb_draws,)).unsqueeze(1)\n",
    "\n",
    "        if params.e_distribution == \"Normal\":\n",
    "            e1 = torch.normal(mean=0, std=params.σ_e, size=(nb_draws,)).unsqueeze(1)\n",
    "            e2 = torch.normal(mean=0, std=params.σ_e, size=(nb_draws,)).unsqueeze(1)\n",
    "            e3 = torch.normal(mean=0, std=params.σ_e, size=(nb_draws,)).unsqueeze(1)\n",
    "            e4 = torch.normal(mean=0, std=params.σ_e, size=(nb_draws,)).unsqueeze(1)\n",
    "        elif params.e_distribution == \"T\":\n",
    "            m = torch.distributions.studentT.StudentT(torch.tensor([3.0]))\n",
    "            e1 = m.sample([nb_draws]).squeeze(1)\n",
    "            e2 = m.sample([nb_draws]).squeeze(1)\n",
    "            e3 = m.sample([nb_draws]).squeeze(1)\n",
    "            e4 = m.sample([nb_draws]).squeeze(1)\n",
    "        elif params.e_distribution == \"Lognormal\":\n",
    "            # Standard Normal iid shocks\n",
    "            e1 = torch.normal(mean=0, std=params.σ_e, size=(nb_draws,)).unsqueeze(1)\n",
    "            e2 = torch.normal(mean=0, std=params.σ_e, size=(nb_draws,)).unsqueeze(1)\n",
    "            e3 = torch.normal(mean=0, std=params.σ_e, size=(nb_draws,)).unsqueeze(1)\n",
    "            e4 = torch.normal(mean=0, std=params.σ_e, size=(nb_draws,)).unsqueeze(1)\n",
    "\n",
    "            # Transform to lognormlaparams.σ_e\n",
    "            e1 = np.exp(params.μ_e + params.σ_e * e1)\n",
    "            e2 = np.exp(params.μ_e + params.σ_e * e2)\n",
    "            e3 = np.exp(params.μ_e + params.σ_e * e4)\n",
    "            e4 = np.exp(params.μ_e + params.σ_e * e4)\n",
    "        else:\n",
    "            raise(\"Distribution unknown.\")\n",
    "\n",
    "        # residuals for n random grid points under 2 realizations of shocks\n",
    "        R1 = Residuals_torch(model, params, x, e1)\n",
    "        R2 = Residuals_torch(model, params, x, e2)\n",
    "        R3 = Residuals_torch(model, params, x, e3)\n",
    "        R4 = Residuals_torch(model, params, x, e4)\n",
    "\n",
    "        # Construct combinations\n",
    "        R1_R2 = R1*R2\n",
    "        R1_R3 = R1*R3\n",
    "        R1_R4 = R1*R4\n",
    "        R2_R3 = R2*R3\n",
    "        R2_R4 = R2*R4\n",
    "        R3_R4 = R3*R4\n",
    "\n",
    "        #var_R1 = torch.var(R1)\n",
    "\n",
    "        # Variance cross\n",
    "        var_R1_R2 = torch.var(R1_R2)\n",
    "\n",
    "        # Co-variances with one shared element\n",
    "        cov_R1R2_R1R3 = torch.cov(torch.column_stack((R1_R2, R1_R3)).T)[0,1]\n",
    "\n",
    "        # Covariances with four different terms\n",
    "        cov_R1R2_R3R4 = torch.cov(torch.column_stack((R1_R2, R3_R4)).T)[0,1]\n",
    "\n",
    "        var_L = (1/(grid_T*(grid_N - 1)))*(var_R1_R2 + 2*(grid_N - 2)*(cov_R1R2_R1R3) + 2*((grid_N*(grid_N - 1)/4) - grid_N + 3/2)*(cov_R1R2_R3R4))\n",
    "\n",
    "    return var_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387db96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_variance_loss_fast_2(params, model, nb_draws, grid_M, grid_N):\n",
    "    \"\"\"\n",
    "    Calculate variance of the loss using proposition 4\n",
    "    Use the four shocks to calculate more accurate values\n",
    "    \"\"\"\n",
    "    grid_T = torch.tensor(grid_M*grid_N/2)\n",
    "    grid_N = torch.tensor(grid_N)\n",
    "    \n",
    "    # Calculate variance and covariance\n",
    "    with torch.no_grad(): \n",
    "        # V1\n",
    "        if params.x_distribution == \"Uniform\":\n",
    "            if params.use_Sobol_T == False:\n",
    "                x = ((params.x_low - params.x_high) * torch.rand(nb_draws) + params.x_high).unsqueeze(1)\n",
    "            else:\n",
    "                #Very slow if T is large\n",
    "                x = ((params.x_low - params.x_high) * params.soboleng.draw(nb_draws) + params.x_high)\n",
    "        else:\n",
    "            x = torch.normal(mean=0, std=params.σ_x, size=(nb_draws,)).unsqueeze(1)\n",
    "\n",
    "        if params.e_distribution == \"Normal\":\n",
    "            e1 = torch.normal(mean=0, std=params.σ_e, size=(nb_draws,)).unsqueeze(1)\n",
    "            e2 = torch.normal(mean=0, std=params.σ_e, size=(nb_draws,)).unsqueeze(1)\n",
    "            e3 = torch.normal(mean=0, std=params.σ_e, size=(nb_draws,)).unsqueeze(1)\n",
    "            e4 = torch.normal(mean=0, std=params.σ_e, size=(nb_draws,)).unsqueeze(1)\n",
    "        elif params.e_distribution == \"T\":\n",
    "            m = torch.distributions.studentT.StudentT(torch.tensor([3.0]))\n",
    "            e1 = m.sample([nb_draws]).squeeze(1)\n",
    "            e2 = m.sample([nb_draws]).squeeze(1)\n",
    "            e3 = m.sample([nb_draws]).squeeze(1)\n",
    "            e4 = m.sample([nb_draws]).squeeze(1)\n",
    "        elif params.e_distribution == \"Lognormal\":\n",
    "            # Standard Normal iid shocks\n",
    "            e1 = torch.normal(mean=0, std=params.σ_e, size=(nb_draws,)).unsqueeze(1)\n",
    "            e2 = torch.normal(mean=0, std=params.σ_e, size=(nb_draws,)).unsqueeze(1)\n",
    "            e3 = torch.normal(mean=0, std=params.σ_e, size=(nb_draws,)).unsqueeze(1)\n",
    "            e4 = torch.normal(mean=0, std=params.σ_e, size=(nb_draws,)).unsqueeze(1)\n",
    "\n",
    "            # Transform to lognormlaparams.σ_e\n",
    "            e1 = np.exp(params.μ_e + params.σ_e * e1)\n",
    "            e2 = np.exp(params.μ_e + params.σ_e * e2)\n",
    "            e3 = np.exp(params.μ_e + params.σ_e * e4)\n",
    "            e4 = np.exp(params.μ_e + params.σ_e * e4)\n",
    "        else:\n",
    "            raise(\"Distribution unknown.\")\n",
    "\n",
    "        # residuals for n random grid points under 2 realizations of shocks\n",
    "        R1 = Residuals_torch(model, params, x, e1)\n",
    "        R2 = Residuals_torch(model, params, x, e2)\n",
    "        R3 = Residuals_torch(model, params, x, e3)\n",
    "        R4 = Residuals_torch(model, params, x, e4)\n",
    "\n",
    "        # Construct combinations\n",
    "        R1_R2 = R1*R2\n",
    "        R1_R3 = R1*R3\n",
    "        R1_R4 = R1*R4\n",
    "        R2_R3 = R2*R3\n",
    "        R2_R4 = R2*R4\n",
    "        R3_R4 = R3*R4\n",
    "\n",
    "        #var_R1 = torch.var(R1)\n",
    "\n",
    "        # Variance cross\n",
    "        var_R1_R2 = torch.var(R1_R2)\n",
    "        var_R1_R3 = torch.var(R1_R3)\n",
    "        var_R1_R4 = torch.var(R1_R4)\n",
    "        var_R2_R3 = torch.var(R2_R3)\n",
    "        var_R2_R4 = torch.var(R2_R4)\n",
    "        var_R3_R4 = torch.var(R3_R4)\n",
    "\n",
    "        mean_var_R1_R2 = (1/6)*(var_R1_R2 + var_R1_R3 + var_R1_R4 + var_R2_R3 + var_R2_R4 +  var_R3_R4)\n",
    "\n",
    "        # Co-variances with one shared element\n",
    "        cov_R1R2_R1R3 = torch.cov(torch.column_stack((R1_R2, R1_R3)).T)[0,1]\n",
    "        cov_R1R2_R1R4 = torch.cov(torch.column_stack((R1_R2, R1_R4)).T)[0,1]\n",
    "        cov_R1R2_R2R3 = torch.cov(torch.column_stack((R1_R2, R2_R3)).T)[0,1]\n",
    "        cov_R1R2_R2R4 = torch.cov(torch.column_stack((R1_R2, R2_R4)).T)[0,1]\n",
    "        cov_R1R3_R3R4 = torch.cov(torch.column_stack((R1_R3, R3_R4)).T)[0,1]\n",
    "\n",
    "        mean_cov_R1R2_R1R3 = (1/5)*(cov_R1R2_R1R3 + cov_R1R2_R1R4 + cov_R1R2_R2R3 + cov_R1R2_R2R4 + cov_R1R3_R3R4)\n",
    "\n",
    "        # Covariances with four different terms\n",
    "        cov_R1R2_R3R4 = torch.cov(torch.column_stack((R1_R2, R3_R4)).T)[0,1]\n",
    "\n",
    "        var_L = (1/(grid_T*(grid_N - 1)))*(mean_var_R1_R2 + 2*(grid_N - 2)*(mean_cov_R1R2_R1R3) + 2*((grid_N*(grid_N - 1)/4) - grid_N + 3/2)*(cov_R1R2_R3R4))\n",
    "\n",
    "    return var_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43d4ad88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_variance_loss_fast_3(params, model, nb_draws, grid_M, grid_N):\n",
    "    \"\"\"\n",
    "    Calculate variance of the loss using proposition 4\n",
    "    Use the 8 shocks to calculate more accurate values.\n",
    "    More costly, but potentially more accurate (?)\n",
    "    \n",
    "    \"\"\"\n",
    "    grid_T = torch.tensor(grid_M*grid_N/2)\n",
    "    grid_N = torch.tensor(grid_N)\n",
    "    \n",
    "    # Calculate variance and covariance\n",
    "    with torch.no_grad(): \n",
    "        # V1\n",
    "        if params.x_distribution == \"Uniform\":\n",
    "            if params.use_Sobol_T == False:\n",
    "                x = ((params.x_low - params.x_high) * torch.rand(nb_draws) + params.x_high).unsqueeze(1)\n",
    "            else:\n",
    "                #Very slow if T is large\n",
    "                x = ((params.x_low - params.x_high) * params.soboleng.draw(nb_draws) + params.x_high)\n",
    "        else:\n",
    "            x = torch.normal(mean=0, std=params.σ_x, size=(nb_draws,)).unsqueeze(1)\n",
    "\n",
    "        if params.e_distribution == \"Normal\":\n",
    "            e1 = torch.normal(mean=0, std=params.σ_e, size=(nb_draws,)).unsqueeze(1)\n",
    "            e2 = torch.normal(mean=0, std=params.σ_e, size=(nb_draws,)).unsqueeze(1)\n",
    "            e3 = torch.normal(mean=0, std=params.σ_e, size=(nb_draws,)).unsqueeze(1)\n",
    "            e4 = torch.normal(mean=0, std=params.σ_e, size=(nb_draws,)).unsqueeze(1)\n",
    "            e5 = torch.normal(mean=0, std=params.σ_e, size=(nb_draws,)).unsqueeze(1)\n",
    "            e6 = torch.normal(mean=0, std=params.σ_e, size=(nb_draws,)).unsqueeze(1)\n",
    "            e7 = torch.normal(mean=0, std=params.σ_e, size=(nb_draws,)).unsqueeze(1)\n",
    "            e8 = torch.normal(mean=0, std=params.σ_e, size=(nb_draws,)).unsqueeze(1)\n",
    "        elif params.e_distribution == \"T\":\n",
    "            m = torch.distributions.studentT.StudentT(torch.tensor([3.0]))\n",
    "            e1 = m.sample([nb_draws]).squeeze(1)\n",
    "            e2 = m.sample([nb_draws]).squeeze(1)\n",
    "            e3 = m.sample([nb_draws]).squeeze(1)\n",
    "            e4 = m.sample([nb_draws]).squeeze(1)\n",
    "            e5 = m.sample([nb_draws]).squeeze(1)\n",
    "            e6 = m.sample([nb_draws]).squeeze(1)\n",
    "            e7 = m.sample([nb_draws]).squeeze(1)\n",
    "            e8 = m.sample([nb_draws]).squeeze(1)\n",
    "        elif params.e_distribution == \"Lognormal\":\n",
    "            # Standard Normal iid shocks\n",
    "            e1 = torch.normal(mean=0, std=params.σ_e, size=(nb_draws,)).unsqueeze(1)\n",
    "            e2 = torch.normal(mean=0, std=params.σ_e, size=(nb_draws,)).unsqueeze(1)\n",
    "            e3 = torch.normal(mean=0, std=params.σ_e, size=(nb_draws,)).unsqueeze(1)\n",
    "            e4 = torch.normal(mean=0, std=params.σ_e, size=(nb_draws,)).unsqueeze(1)\n",
    "            e5 = torch.normal(mean=0, std=params.σ_e, size=(nb_draws,)).unsqueeze(1)\n",
    "            e6 = torch.normal(mean=0, std=params.σ_e, size=(nb_draws,)).unsqueeze(1)\n",
    "            e7 = torch.normal(mean=0, std=params.σ_e, size=(nb_draws,)).unsqueeze(1)\n",
    "            e8 = torch.normal(mean=0, std=params.σ_e, size=(nb_draws,)).unsqueeze(1)\n",
    "            # Transform to lognormlaparams.σ_e\n",
    "            e1 = np.exp(params.μ_e + params.σ_e * e1)\n",
    "            e2 = np.exp(params.μ_e + params.σ_e * e2)\n",
    "            e3 = np.exp(params.μ_e + params.σ_e * e4)\n",
    "            e4 = np.exp(params.μ_e + params.σ_e * e4)\n",
    "            e5 = np.exp(params.μ_e + params.σ_e * e5)\n",
    "            e6 = np.exp(params.μ_e + params.σ_e * e6)\n",
    "            e7 = np.exp(params.μ_e + params.σ_e * e7)\n",
    "            e8 = np.exp(params.μ_e + params.σ_e * e8)\n",
    "        else:\n",
    "            raise(\"Distribution unknown.\")\n",
    "\n",
    "        # residuals \n",
    "        R1 = Residuals_torch(model, params, x, e1)\n",
    "        R2 = Residuals_torch(model, params, x, e2)\n",
    "        R3 = Residuals_torch(model, params, x, e3)\n",
    "        R4 = Residuals_torch(model, params, x, e4)\n",
    "        R5 = Residuals_torch(model, params, x, e5)\n",
    "        R6 = Residuals_torch(model, params, x, e6)\n",
    "        R7 = Residuals_torch(model, params, x, e7)\n",
    "        R8 = Residuals_torch(model, params, x, e8)\n",
    "        \n",
    "        # Construct combinations\n",
    "        R1_R2 = R1*R2\n",
    "        R1_R3 = R1*R3\n",
    "        R1_R4 = R1*R4\n",
    "        R2_R3 = R2*R3\n",
    "        R2_R4 = R2*R4\n",
    "        R3_R4 = R3*R4\n",
    "        \n",
    "        R5_R6 = R5*R6\n",
    "        R7_R8 = R7*R8\n",
    "\n",
    "        # Variance cross\n",
    "        var_R1_R2 = torch.var(R1_R2)\n",
    "        var_R1_R3 = torch.var(R1_R3)\n",
    "        var_R1_R4 = torch.var(R1_R4)\n",
    "        var_R2_R3 = torch.var(R2_R3)\n",
    "        var_R2_R4 = torch.var(R2_R4)\n",
    "        var_R3_R4 = torch.var(R3_R4)\n",
    "\n",
    "        mean_var_R1_R2 = (1/6)*(var_R1_R2 + var_R1_R3 + var_R1_R4 + var_R2_R3 + var_R2_R4 +  var_R3_R4)\n",
    "\n",
    "        # Co-variances with one shared element\n",
    "        cov_R1R2_R1R3 = torch.cov(torch.column_stack((R1_R2, R1_R3)).T)[0,1]\n",
    "        cov_R1R2_R1R4 = torch.cov(torch.column_stack((R1_R2, R1_R4)).T)[0,1]\n",
    "        cov_R1R2_R2R3 = torch.cov(torch.column_stack((R1_R2, R2_R3)).T)[0,1]\n",
    "        cov_R1R2_R2R4 = torch.cov(torch.column_stack((R1_R2, R2_R4)).T)[0,1]\n",
    "        cov_R1R3_R3R4 = torch.cov(torch.column_stack((R1_R3, R3_R4)).T)[0,1]\n",
    "\n",
    "        mean_cov_R1R2_R1R3 = (1/5)*(cov_R1R2_R1R3 + cov_R1R2_R1R4 + cov_R1R2_R2R3 + cov_R1R2_R2R4 + cov_R1R3_R3R4)\n",
    "\n",
    "        # Covariances with four different terms\n",
    "        cov_R1R2_R3R4 = torch.cov(torch.column_stack((R1_R2, R3_R4)).T)[0,1]\n",
    "        cov_R5R6_R7R8 = torch.cov(torch.column_stack((R5_R6, R7_R8)).T)[0,1]\n",
    "        mean_cov_4_terms = 0.5*(cov_R1R2_R3R4 + cov_R5R6_R7R8)\n",
    "            \n",
    "        var_L = (1/(grid_T*(grid_N - 1)))*(mean_var_R1_R2 + 2*(grid_N - 2)*(mean_cov_R1R2_R1R3) + 2*((grid_N*(grid_N - 1)/4) - grid_N + 3/2)*(mean_cov_4_terms))\n",
    "\n",
    "    return var_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849eb940",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_flat(a):\n",
    "    \"\"\"\n",
    "    Function to flatten a list\n",
    "    \"\"\"\n",
    "    return list(np.array(a).flat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
